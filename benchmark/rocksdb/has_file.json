[
    {
        "hash": "1d6c33d2a59f6f9dd23e7db61618a0f0aa3c1600",
        "author": "Nicolas De Carli",
        "date": "2025-03-02T08:05:21-08:00",
        "message": "Enable hardware accelerated crc32c for ARM on Linux (#13432)\n\nSummary:\nPull Request resolved: https://github.com/facebook/rocksdb/pull/13432\n\nWe've noticed the default CRC32c function gets executed when running on aarch64 cpus within our servers\n\nIssue is that ROCKSDB_AUXV_GETAUXVAL_PRESENT evaluates to false\n\nThis fix allows the usage of hardware-accelerated crc32 within our fleet\n\nReviewed By: jaykorean\n\nDifferential Revision: D70423483\n\nfbshipit-source-id: 601da3fbf156e3e40695eb76ee5d37f67f83d427",
        "modified_files_count": 1,
        "modified_files": [
            "util/crc32c_arm64.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1d6c33d2a59f6f9dd23e7db61618a0f0aa3c1600",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e48ccc28f4eebcc05b6333b129ee5908214d3259",
        "author": "Peter Dillinger",
        "date": "2025-01-02T10:48:46-08:00",
        "message": "Reduce unnecessary manifest data when no file checksum (#13250)\n\nSummary:\nDon't write file checksum manifest entries when unused, to avoid using extra manifest file space.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/13250\n\nTest Plan: very minor performance improvement, existing tests\n\nReviewed By: cbi42\n\nDifferential Revision: D67653954\n\nPulled By: pdillinger\n\nfbshipit-source-id: 9156e093ed5e4a5152cc55354a4beea9a841b89f",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_edit.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e48ccc28f4eebcc05b6333b129ee5908214d3259",
        "contains_optimization_keyword": true
    },
    {
        "hash": "6ae3412244e487a63e9e0c09938230aca611d7a1",
        "author": "Andrew Chang",
        "date": "2024-12-17T15:25:04-08:00",
        "message": "Explain why RandomAccessFileReader* is not passed into FilePrefetchBuffer constructor (#13159)\n\nSummary:\nIn https://github.com/facebook/rocksdb/pull/13118#discussion_r1842848359, we decided to make a separate follow-up PR that refactors `FilePrefetchBuffer` to determine `use_fs_buffer` once at construction time.\n\nThe change would have involved passing in the `RandomAccessFileReader*` directly to the constructor, and using that to determine `use_fs_buffer`. This would avoid repeatedly calling `UseFSBuffer(RandomAccessFileReader* reader)` during the actual prefetch requests.\n\nI started working on this refactoring change but ran into issues with these 2 files, which used `GetOrCreatePrefetchBuffer`\n- https://github.com/facebook/rocksdb/blob/main/db/compaction/compaction_iterator.cc\n- https://github.com/facebook/rocksdb/blob/main/db/merge_helper.cc\n\nAs I explained in the added code comments, sometimes the `RandomAccessFileReader*` is not available when we construct the `FilePrefetchBuffer`, so although it is not the most elegant, I think right now it makes sense to pass in the `reader` into the `Prefetch` / `PrefetchAsync` / `TryReadFromCache` calls. Maybe there is a workaround but I don't think the refactor would be worth it.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/13159\n\nTest Plan: N/A (comments)\n\nReviewed By: anand1976\n\nDifferential Revision: D66473731\n\nPulled By: archang19\n\nfbshipit-source-id: ce3473694c2cd82513da1a76ad5995afa5bc9cfa",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/6ae3412244e487a63e9e0c09938230aca611d7a1",
        "contains_optimization_keyword": true
    },
    {
        "hash": "85d8ee78440e86f1dc678c215b56c090351f3b90",
        "author": "Peter Dillinger",
        "date": "2024-12-12T11:41:16-08:00",
        "message": "Improve paranoid_checks API comment (#13206)\n\nSummary:\nsee comment change\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/13206\n\nTest Plan: no functional change\n\nReviewed By: cbi42\n\nDifferential Revision: D67108123\n\nPulled By: pdillinger\n\nfbshipit-source-id: 669de1fff8df452c3e279f311452f02b40a03aaf",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/85d8ee78440e86f1dc678c215b56c090351f3b90",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d386385e0bb7a17564493b3a2f5eed0f2356e05e",
        "author": "Andrew Chang",
        "date": "2024-12-09T13:24:16-08:00",
        "message": "Temporarily disable file system buffer reuse optimization for compaction prefetches (#13177)\n\nSummary:\nhttps://github.com/facebook/rocksdb/issues/13182 successfully fixed the heap `use-after-free` issue.\n\nHowever, there was one additional error I found while looking through the warm storage crash test logs. There are repeated (though infrequent) unsigned pointer arithmetic overflow errors that look like this:\n```cpp\nfile_prefetch_buffer.cc:860:46: runtime error: addition of unsigned offset to 0x7f282001880f overflowed to 0x7f2820017667\n```\n\nIt took me a while to figure it out, but I was finally able to reproduce the issue locally. It turns out the issue is when we call `TryReadFromCache` with `for_compaction` set to `true`. The default value for `for_compaction` is `false`, and this was not covered in the unit tests written for https://github.com/facebook/rocksdb/issues/13118.\n\nWhen I run the same unit tests with `for_compaction` set to `true`, I am able to break this assertion that I added at the end of `TryReadFromCacheUntracked`:\n```cpp\nassert(buf->offset_ <= offset);\n```\n\nIf `buf->offset_` is greater than `offset`, then that explains the overflow we get in the following lines:\n```cpp\nuint64_t offset_in_buffer = offset - buf->offset_;\n*result = Slice(buf->buffer_.BufferStart() + offset_in_buffer, n);\n```\n\nI will have another PR out that fixes the issue and enables the optimization when `for_compaction` is set to `true`. I will need to add some overlap buffer logic, similar to what I have inside `PrefetchInternal`. For now, since I have confirmed that there is indeed a bug, we should disable the optimization where needed. It will take me some time to implement the fix and write new test cases.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/13177\n\nTest Plan: I kept the existing unit tests which test the file system buffer reuse code when `for_compaction` is `false`. I expect that the warm storage crash test logs will no longer show the integer overflow issue once we merge this PR.\n\nReviewed By: anand1976\n\nDifferential Revision: D66721857\n\nPulled By: archang19\n\nfbshipit-source-id: 22d523646f969a7a0ccbbea73f63c32601f1179a",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d386385e0bb7a17564493b3a2f5eed0f2356e05e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "55de26580af57abaa1c398e405dbc05cf8228c7f",
        "author": "Levi Tamasi",
        "date": "2024-10-15T17:32:07-07:00",
        "message": "Small improvement to MultiCFIteratorImpl (#13075)\n\nSummary:\nPull Request resolved: https://github.com/facebook/rocksdb/pull/13075\n\nThe patch simplifies the iteration logic in `MultiCFIteratorImpl::{Advance,Populate}Iterator` a bit and adds some assertions to uniformly enforce the invariant that any iterators currently on the heap should be valid and have an OK status.\n\nReviewed By: jaykorean\n\nDifferential Revision: D64429566\n\nfbshipit-source-id: 36bc22465285b670f859692a048e10f21df7da7a",
        "modified_files_count": 1,
        "modified_files": [
            "db/multi_cf_iterator_impl.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/55de26580af57abaa1c398e405dbc05cf8228c7f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "632746bb5b8d9d817b0075b295e1a085e1e543a4",
        "author": "Hui Xiao",
        "date": "2024-10-09T12:51:19-07:00",
        "message": "Improve DBTest.DynamicLevelCompressionPerLevel (#13044)\n\nSummary:\n**Context/Summary:**\n\nA part of this test is to verify compression conditionally happens depending on the shape of the LSM when `options.level_compaction_dynamic_level_bytes = true;`. It uses the total file size to determine whether compression has happened or not. This involves some hard-coded math hard to understand. This PR replaces those with statistics that directly shows whether compression has happened or not.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/13044\n\nTest Plan: Existing test\n\nReviewed By: jaykorean\n\nDifferential Revision: D63666361\n\nPulled By: hx235\n\nfbshipit-source-id: 8c9b1bea9b06ff1e3ed95c576aec6705159af137",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/632746bb5b8d9d817b0075b295e1a085e1e543a4",
        "contains_optimization_keyword": true
    },
    {
        "hash": "92ad4a88f3199b013532b37d6598c442319355a5",
        "author": "Changyu Bi",
        "date": "2024-08-27T13:57:40-07:00",
        "message": "Small CPU optimization in InlineSkipList::Insert() (#12975)\n\nSummary:\nreuse decode key in more places to avoid decoding length prefixed key x->Key().\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12975\n\nTest Plan:\nran benchmarks simultaneously for \"before\" and \"after\"\n* fillseq:\n```\n(for I in $(seq 1 50); do ./db_bench --benchmarks=fillseq --disable_auto_compactions=1 --min_write_buffer_number_to_merge=100 --max_write_buffer_number=1000  --write_buffer_size=268435456 --num=5000000 --seed=1723056275 --disable_wal=1 2>&1 | grep \"fillseq\"\ndone;) | awk '{ t += $5; c++; print } END { printf (\"%9.3f\\n\", 1.0 * t / c) }';\n\nbefore: 1483191\nafter: 1490555 (+0.5%)\n```\n\n* fillrandom:\n```\n(for I in $(seq 1 2); do ./db_bench_imain --benchmarks=fillrandom --disable_auto_compactions=1 --min_write_buffer_number_to_merge=100 --max_write_buffer_number=1000  --write_buffer_size=268435456 --num=2500000 --seed=1723056275 --disable_wal=1 2>&1 | grep \"fillrandom\"\n\nbefore: 255463\nafter: 256128 (+0.26%)\n```\n\nReviewed By: anand1976\n\nDifferential Revision: D61835340\n\nPulled By: cbi42\n\nfbshipit-source-id: 70345510720e348bacd51269acb5d2dd5a62bf0a",
        "modified_files_count": 1,
        "modified_files": [
            "memtable/inlineskiplist.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/92ad4a88f3199b013532b37d6598c442319355a5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "5c456c4c08ac046429c38792d242dd095c50b049",
        "author": "SGZW",
        "date": "2024-08-09T15:05:02-07:00",
        "message": "fix compaction speedup for marked files ut (#12912)\n\nSummary: Pull Request resolved: https://github.com/facebook/rocksdb/pull/12912\n\nReviewed By: hx235\n\nDifferential Revision: D60973460\n\nPulled By: cbi42\n\nfbshipit-source-id: ebaa343757f09f7281884a512ebe3a7d6845c8b3",
        "modified_files_count": 1,
        "modified_files": [
            "db/column_family_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/5c456c4c08ac046429c38792d242dd095c50b049",
        "contains_optimization_keyword": true
    },
    {
        "hash": "16c21afc061bffba8ec1a518273080e1d59e3d96",
        "author": "Hui Xiao",
        "date": "2024-08-08T15:37:19-07:00",
        "message": "Fix failure to clean the temporary directory due to NotFound in crash test checkpoint creation (#12919)\n\nSummary:\n**Context/Summary:**\nhttps://github.com/facebook/rocksdb/commit/b26b395e0a15255d322be08110db551976188745 propagates `CleanStagingDirectory()` status to `CreateCheckpoint()`.  However, we didn't return early when `Status s = db_->GetEnv()->FileExists(full_private_path);` return non-NotFound non-ok stratus in `CleanStagingDirectory()`. Therefore we can proceed to the next step when `full_private_path` doesn't exist.\n```\nVerification failed: Checkpoint failed: Operation aborted: Failed to clean the temporary directory /dev/shm/rocksdb.J4Su/rocksdb_crashtest_blackbox/.checkpoint28.tmp needed before checkpoint creation : NotFound:\n\ndb_stress: db_stress_tool/db_stress_test_base.cc:549: void rocksdb::StressTest::ProcessStatus(rocksdb::SharedState*, std::string, const rocksdb::Status&, bool) const: Assertion `false' failed.\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12919\n\nTest Plan:\nBelow failed before the fix and passes after\n\n```\n./db_stress --WAL_size_limit_MB=1 --WAL_ttl_seconds=0 --acquire_snapshot_one_in=100 --adaptive_readahead=1 --adm_policy=1 --advise_random_on_open=0 --allow_data_in_errors=True --allow_fallocate=1 --async_io=1 --auto_readahead_size=0 --avoid_flush_during_recovery=1 --avoid_flush_during_shutdown=0 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=8 --bgerror_resume_retry_interval=1000000 --block_align=0 --block_protection_bytes_per_key=4 --block_size=16384 --bloom_before_level=2 --bloom_bits=4 --bottommost_compression_type=snappy --bottommost_file_compaction_delay=0 --bytes_per_sync=0 --cache_index_and_filter_blocks=1 --cache_index_and_filter_blocks_with_high_priority=0 --cache_size=8388608 --cache_type=auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=1 --charge_filter_construction=1 --charge_table_reader=1 --check_multiget_consistency=0 --check_multiget_entity_consistency=0 --checkpoint_one_in=10000 --checksum_type=kxxHash64 --clear_column_family_one_in=0 --compact_files_one_in=1000 --compact_range_one_in=1000000 --compaction_pri=3 --compaction_readahead_size=1048576 --compaction_ttl=0 --compress_format_version=2 --compressed_secondary_cache_ratio=0.0 --compressed_secondary_cache_size=0 --compression_checksum=0 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=none --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --daily_offpeak_time_utc= --data_block_index_type=0 --db=/dev/shm/rocksdb.J4Su/rocksdb_crashtest_blackbox --db_write_buffer_size=134217728 --default_temperature=kUnknown --default_write_temperature=kHot --delete_obsolete_files_period_micros=21600000000 --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --detect_filter_construct_corruption=0 --disable_file_deletions_one_in=1000000 --disable_manual_compaction_one_in=10000 --disable_wal=0 --dump_malloc_stats=0 --enable_checksum_handoff=0 --enable_compaction_filter=0 --enable_custom_split_merge=0 --enable_do_not_compress_roles=1 --enable_index_compression=1 --enable_memtable_insert_with_hint_prefix_extractor=0 --enable_pipelined_write=0 --enable_sst_partitioner_factory=0 --enable_thread_tracking=0 --enable_write_thread_adaptive_yield=1 --error_recovery_with_no_fault_injection=1 --exclude_wal_from_write_fault_injection=0 --expected_values_dir=/dev/shm/rocksdb.J4Su/rocksdb_crashtest_expected --fail_if_options_file_error=1 --fifo_allow_compaction=1 --file_checksum_impl=xxh64 --fill_cache=1 --flush_one_in=1000000 --format_version=6 --get_all_column_family_metadata_one_in=1000000 --get_current_wal_file_one_in=0 --get_live_files_apis_one_in=1000000 --get_properties_of_all_tables_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=274877906944 --high_pri_pool_ratio=0.5 --index_block_restart_interval=13 --index_shortening=0 --index_type=3 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --inplace_update_support=0 --iterpercent=10 --key_len_percent_dist=1,30,69 --key_may_exist_one_in=100000 --last_level_temperature=kWarm --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=1000000 --log_file_time_to_roll=0 --log_readahead_size=0 --long_running_snapshots=0 --low_pri_pool_ratio=0 --lowest_used_cache_tier=0 --manifest_preallocation_size=5120 --manual_wal_flush_one_in=1000 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=16384 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=2500000 --max_key_len=3 --max_log_file_size=0 --max_manifest_file_size=1073741824 --max_sequential_skip_in_iterations=8 --max_total_wal_size=0 --max_write_batch_group_size_bytes=16 --max_write_buffer_number=10 --max_write_buffer_size_to_maintain=0 --memtable_insert_hint_per_batch=0 --memtable_max_range_deletions=100 --memtable_prefix_bloom_size_ratio=0.1 --memtable_protection_bytes_per_key=4 --memtable_whole_key_filtering=0 --memtablerep=skip_list --metadata_charge_policy=1 --metadata_read_fault_one_in=32 --metadata_write_fault_one_in=128 --min_write_buffer_number_to_merge=2 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=2 --open_files=500000 --open_metadata_read_fault_one_in=8 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=16 --ops_per_thread=100000000 --optimize_filters_for_hits=0 --optimize_filters_for_memory=1 --optimize_multiget_for_io=0 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=0 --pause_background_one_in=10000 --periodic_compaction_seconds=0 --prefix_size=7 --prefixpercent=5 --prepopulate_block_cache=1 --preserve_internal_time_seconds=36000 --progress_reports=0 --promote_l0_one_in=0 --read_amp_bytes_per_bit=0 --read_fault_one_in=0 --readahead_size=0 --readpercent=45 --recycle_log_file_num=1 --reopen=0 --report_bg_io_stats=0 --reset_stats_one_in=10000 --sample_for_compression=5 --secondary_cache_fault_one_in=32 --set_options_one_in=10000 --skip_stats_update_on_db_open=1 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=68719476736 --sqfc_name=foo --sqfc_version=0 --sst_file_manager_bytes_per_sec=104857600 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=600 --stats_history_buffer_size=1048576 --strict_bytes_per_sync=1 --subcompactions=3 --sync=0 --sync_fault_injection=0 --table_cache_numshardbits=6 --target_file_size_base=524288 --target_file_size_multiplier=2 --test_batches_snapshots=0 --test_cf_consistency=1 --top_level_index_pinning=1 --uncache_aggressiveness=0 --universal_max_read_amp=0 --unpartitioned_pinning=0 --use_adaptive_mutex=0 --use_adaptive_mutex_lru=1 --use_attribute_group=0 --use_delta_encoding=1 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=1 --use_merge=0 --use_multi_cf_iterator=0 --use_multi_get_entity=0 --use_multiget=0 --use_put_entity_one_in=0 --use_sqfc_for_range_queries=1 --use_timed_put_one_in=1 --use_write_buffer_manager=1 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000 --verify_compression=1 --verify_db_one_in=100000 --verify_file_checksums_one_in=1000 --verify_iterator_with_expected_state_one_in=0 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=1048576 --write_dbid_to_manifest=0 --write_fault_one_in=128 --writepercent=35\n```\n\nReviewed By: cbi42\n\nDifferential Revision: D60938952\n\nPulled By: hx235\n\nfbshipit-source-id: 5696cd6b00f33c9f9a256944fecb4e2f4d52a2e6",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/checkpoint/checkpoint_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/16c21afc061bffba8ec1a518273080e1d59e3d96",
        "contains_optimization_keyword": true
    },
    {
        "hash": "5e203c76a2b3be439705770e5ebbc8415b1dadf6",
        "author": "Hui Xiao",
        "date": "2024-08-02T10:45:34-07:00",
        "message": "SyncWAL() before Close() when FLAGS_avoid_flush_during_shutdown=true in crash test (#12900)\n\nSummary:\n**Context/Summary:**\nWhen we use WAL and don't flush data during shutdown `FLAGS_avoid_flush_during_shutdown=true`, then we rely on WAL to recover data in next Open() so will need to sync WAL in crash test. Currently the condition is flipped.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12900\n\nTest Plan:\nBelow fails with data loss `Verification failed. Expected state has key 000000000000015D000000000000012B0000000000000147, iterator is at key 000000000000015D000000000000012B0000000000000152` before the fix but not after the fix\n```\n./db_stress --WAL_size_limit_MB=0 --WAL_ttl_seconds=0 --acquire_snapshot_one_in=10000 --adaptive_readahead=1 --adm_policy=3 --advise_random_on_open=1 --allow_concurrent_memtable_write=0 --allow_data_in_errors=True --allow_fallocate=1 --async_io=1 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_flush_during_shutdown=1 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=1000 --batch_protection_bytes_per_key=8 --bgerror_resume_retry_interval=100 --block_align=0 --block_protection_bytes_per_key=4 --block_size=16384 --bloom_before_level=0 --bloom_bits=10 --bottommost_compression_type=disable --bottommost_file_compaction_delay=3600 --bytes_per_sync=262144 --cache_index_and_filter_blocks=1 --cache_index_and_filter_blocks_with_high_priority=0 --cache_size=33554432 --cache_type=tiered_auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=0 --charge_filter_construction=0 --charge_table_reader=0 --check_multiget_consistency=0 --check_multiget_entity_consistency=0 --checkpoint_one_in=1000000 --checksum_type=kxxHash64 --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=3 --compaction_readahead_size=0 --compaction_style=1 --compaction_ttl=0 --compress_format_version=1 --compressed_secondary_cache_ratio=0.3333333333333333 --compressed_secondary_cache_size=0 --compression_checksum=1 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=8 --compression_type=zlib --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --daily_offpeak_time_utc= --data_block_index_type=1 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_whitebox_2 --db_write_buffer_size=0 --default_temperature=kUnknown --default_write_temperature=kWarm --delete_obsolete_files_period_micros=30000000 --delpercent=4 --delrangepercent=1 --destroy_db_initially=1 --detect_filter_construct_corruption=0 --disable_file_deletions_one_in=1000000 --disable_manual_compaction_one_in=10000 --disable_wal=0 --dump_malloc_stats=0 --enable_checksum_handoff=0 --enable_compaction_filter=0 --enable_custom_split_merge=0 --enable_do_not_compress_roles=0 --enable_index_compression=1 --enable_memtable_insert_with_hint_prefix_extractor=0 --enable_pipelined_write=0 --enable_sst_partitioner_factory=0 --enable_thread_tracking=1 --enable_write_thread_adaptive_yield=1 --error_recovery_with_no_fault_injection=0 --exclude_wal_from_write_fault_injection=1 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected_2 --fail_if_options_file_error=1 --fifo_allow_compaction=1 --file_checksum_impl=none --fill_cache=0 --flush_one_in=1000000 --format_version=5 --get_all_column_family_metadata_one_in=1000000 --get_current_wal_file_one_in=0 --get_live_files_apis_one_in=10000 --get_properties_of_all_tables_one_in=100000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=274877906944 --high_pri_pool_ratio=0.5 --index_block_restart_interval=13 --index_shortening=1 --index_type=2 --ingest_external_file_one_in=0 --initial_auto_readahead_size=524288 --inplace_update_support=0 --iterpercent=10 --key_len_percent_dist=1,30,69 --key_may_exist_one_in=100000 --last_level_temperature=kHot --level_compaction_dynamic_level_bytes=0 --lock_wal_one_in=10000 --log2_keys_per_lock=10 --log_file_time_to_roll=60 --log_readahead_size=16777216 --long_running_snapshots=0 --low_pri_pool_ratio=0.5 --lowest_used_cache_tier=0 --manifest_preallocation_size=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=0 --max_auto_readahead_size=0 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=100000 --max_key_len=3 --max_log_file_size=1048576 --max_manifest_file_size=1073741824 --max_sequential_skip_in_iterations=16 --max_total_wal_size=0 --max_write_batch_group_size_bytes=16 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=8388608 --memtable_insert_hint_per_batch=0 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.01 --memtable_protection_bytes_per_key=4 --memtable_whole_key_filtering=1 --memtablerep=skip_list --metadata_charge_policy=1 --metadata_read_fault_one_in=0 --metadata_write_fault_one_in=0 --min_write_buffer_number_to_merge=1 --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --num_file_reads_for_auto_readahead=2 --open_files=100 --open_metadata_read_fault_one_in=0 --open_metadata_write_fault_one_in=8 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=200000 --optimize_filters_for_hits=1 --optimize_filters_for_memory=1 --optimize_multiget_for_io=0 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=1 --pause_background_one_in=1000000 --periodic_compaction_seconds=0 --prefix_size=1 --prefixpercent=5 --prepopulate_block_cache=0 --preserve_internal_time_seconds=0 --progress_reports=0 --promote_l0_one_in=0 --read_amp_bytes_per_bit=0 --read_fault_one_in=0 --readahead_size=16384 --readpercent=45 --recycle_log_file_num=1 --reopen=20 --report_bg_io_stats=1 --reset_stats_one_in=1000000 --sample_for_compression=0 --secondary_cache_fault_one_in=32 --secondary_cache_uri= --skip_stats_update_on_db_open=1 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=68719476736 --sqfc_name=foo --sqfc_version=2 --sst_file_manager_bytes_per_sec=104857600 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=0 --stats_history_buffer_size=0 --strict_bytes_per_sync=1 --subcompactions=3 --sync=0 --sync_fault_injection=1 --table_cache_numshardbits=6 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=0 --uncache_aggressiveness=4404 --universal_max_read_amp=-1 --unpartitioned_pinning=2 --use_adaptive_mutex=0 --use_adaptive_mutex_lru=1 --use_attribute_group=0 --use_delta_encoding=1 --use_direct_io_for_flush_and_compaction=1 --use_direct_reads=1 --use_full_merge_v1=1 --use_get_entity=0 --use_merge=0 --use_multi_cf_iterator=1 --use_multi_get_entity=0 --use_multiget=1 --use_put_entity_one_in=0 --use_sqfc_for_range_queries=1 --use_timed_put_one_in=0 --use_write_buffer_manager=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000 --verify_compression=1 --verify_db_one_in=10000 --verify_file_checksums_one_in=0 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=33554432 --write_dbid_to_manifest=1 --write_fault_one_in=0 --writepercent=35\n\n```\n\nReviewed By: anand1976, ltamasi\n\nDifferential Revision: D60489038\n\nPulled By: hx235\n\nfbshipit-source-id: fb35889ae1509eb1bac27b015bb24a07d3b95268",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_test_base.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/5e203c76a2b3be439705770e5ebbc8415b1dadf6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9883b5f497a6c451065595c8c668728cfa5b8f59",
        "author": "Yu Zhang",
        "date": "2024-07-24T17:50:08-07:00",
        "message": "Fix manifest_number_ point to invalid file (#12882)\n\nSummary:\nThis PR fix `VersionSet`'s `manifest_number_` could be pointing to an invalid number intermediately. This happens when a new manifest roll is attempted but fast failed after loading table handlers and before the new manifest file creation/writing is actually attempted.\n\nIn theory, a later manifest roll effort will overthrow this intermediate invalid in memory state. There is on harm when the DB crashes in this invalid state either. But efforts that takes a file snapshot of the DB like backup will incorrectly try to copy a non existing manifest file.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12882\n\nReviewed By: cbi42\n\nDifferential Revision: D60204956\n\nPulled By: jowlyzhang\n\nfbshipit-source-id: effbdb124b582f879d114988af06ac63867fc549",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9883b5f497a6c451065595c8c668728cfa5b8f59",
        "contains_optimization_keyword": true
    },
    {
        "hash": "21db55f8164d2a6519dcc993f74bf7f49c700854",
        "author": "Hui Xiao",
        "date": "2024-07-17T13:39:14-07:00",
        "message": "Move WAL sync before memtable insertion (#12869)\n\nSummary:\n**Context/Summary:**\nWAL sync currently happens after memtable write. This causes inconvenience in stress test as we can't simply rollback the ExpectedState when write fails due to injected WAL sync error so something complicated like https://github.com/facebook/rocksdb/pull/12838 might be needed. After moving WAL sync before memtable insertion, there should not be injected IO error after memtable insertion so we can keep the current simple way of handling failed write in stress test with ExpectedState rollback.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12869\n\nTest Plan:\n1. Below command failed with `iterator has key 0000000000000207000000000000012B0000000000000013, but expected state does not.` before this PR and passes after\n```\n./db_stress  --WAL_size_limit_MB=0 --WAL_ttl_seconds=0 --acquire_snapshot_one_in=10000 --adaptive_readahead=1 --adm_policy=1 --advise_random_on_open=0 --allow_concurrent_memtable_write=0 --allow_data_in_errors=True --allow_fallocate=0 --async_io=0 --auto_readahead_size=0 --avoid_flush_during_recovery=0 --avoid_flush_during_shutdown=0 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=0 --batch_protection_bytes_per_key=0 --bgerror_resume_retry_interval=1000000 --block_align=1 --block_protection_bytes_per_key=4 --block_size=16384 --bloom_before_level=4 --bloom_bits=56.810257702625165 --bottommost_compression_type=none --bottommost_file_compaction_delay=0 --bytes_per_sync=262144 --cache_index_and_filter_blocks=1 --cache_index_and_filter_blocks_with_high_priority=1 --cache_size=8388608 --cache_type=auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=1 --charge_filter_construction=1 --charge_table_reader=0 --check_multiget_consistency=0 --check_multiget_entity_consistency=1 --checkpoint_one_in=10000 --checksum_type=kxxHash --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000 --compact_range_one_in=1000 --compaction_pri=4 --compaction_readahead_size=1048576 --compaction_ttl=10 --compress_format_version=1 --compressed_secondary_cache_ratio=0.0 --compressed_secondary_cache_size=0 --compression_checksum=0 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=none --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --daily_offpeak_time_utc=04:00-08:00 --data_block_index_type=1 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_blackbox --db_write_buffer_size=0 --default_temperature=kWarm --default_write_temperature=kCold --delete_obsolete_files_period_micros=30000000 --delpercent=0 --delrangepercent=0 --destroy_db_initially=0 --detect_filter_construct_corruption=0 --disable_file_deletions_one_in=10000 --disable_manual_compaction_one_in=1000000 --disable_wal=0 --dump_malloc_stats=0 --enable_checksum_handoff=1 --enable_compaction_filter=0 --enable_custom_split_merge=0 --enable_do_not_compress_roles=0 --enable_index_compression=1 --enable_memtable_insert_with_hint_prefix_extractor=0 --enable_pipelined_write=0 --enable_sst_partitioner_factory=0 --enable_thread_tracking=0 --enable_write_thread_adaptive_yield=0 --error_recovery_with_no_fault_injection=1 --exclude_wal_from_write_fault_injection=1 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=1 --fifo_allow_compaction=0 --file_checksum_impl=crc32c --fill_cache=1 --flush_one_in=1000000 --format_version=3 --get_all_column_family_metadata_one_in=1000000 --get_current_wal_file_one_in=0 --get_live_files_apis_one_in=1000000 --get_properties_of_all_tables_one_in=1000000 --get_property_one_in=100000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=274877906944 --high_pri_pool_ratio=0.5 --index_block_restart_interval=4 --index_shortening=2 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --inplace_update_support=0 --iterpercent=50 --key_len_percent_dist=1,30,69 --key_may_exist_one_in=100 --last_level_temperature=kWarm --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=10000 --log_file_time_to_roll=60 --log_readahead_size=16777216 --long_running_snapshots=1 --low_pri_pool_ratio=0 --lowest_used_cache_tier=0 --manifest_preallocation_size=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=16384 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=100000 --max_key_len=3 --max_log_file_size=1048576 --max_manifest_file_size=32768 --max_sequential_skip_in_iterations=1 --max_total_wal_size=0 --max_write_batch_group_size_bytes=16 --max_write_buffer_number=10 --max_write_buffer_size_to_maintain=8388608 --memtable_insert_hint_per_batch=1 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.01 --memtable_protection_bytes_per_key=1 --memtable_whole_key_filtering=1 --memtablerep=skip_list --metadata_charge_policy=1 --metadata_read_fault_one_in=32 --metadata_write_fault_one_in=0 --min_write_buffer_number_to_merge=1 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=1 --open_files=-1 --open_metadata_read_fault_one_in=0 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=100000000 --optimize_filters_for_hits=1 --optimize_filters_for_memory=1 --optimize_multiget_for_io=1 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=3 --pause_background_one_in=1000000 --periodic_compaction_seconds=2 --prefix_size=7 --prefixpercent=0 --prepopulate_block_cache=0 --preserve_internal_time_seconds=0 --progress_reports=0 --promote_l0_one_in=0 --read_amp_bytes_per_bit=0 --read_fault_one_in=1000 --readahead_size=524288 --readpercent=0 --recycle_log_file_num=1 --reopen=0 --report_bg_io_stats=0 --reset_stats_one_in=1000000 --sample_for_compression=0 --secondary_cache_fault_one_in=0 --set_options_one_in=0 --skip_stats_update_on_db_open=1 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=68719476736 --sqfc_name=foo --sqfc_version=0 --sst_file_manager_bytes_per_sec=104857600 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=10 --stats_history_buffer_size=0 --strict_bytes_per_sync=1 --subcompactions=4 --sync=1 --sync_fault_injection=0 --table_cache_numshardbits=6 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=2 --uncache_aggressiveness=239 --universal_max_read_amp=-1 --unpartitioned_pinning=1 --use_adaptive_mutex=1 --use_adaptive_mutex_lru=1 --use_attribute_group=0 --use_delta_encoding=0 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=0 --use_multi_cf_iterator=0 --use_multi_get_entity=0 --use_multiget=0 --use_put_entity_one_in=0 --use_sqfc_for_range_queries=1 --use_timed_put_one_in=0 --use_write_buffer_manager=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_compression=0 --verify_db_one_in=100000 --verify_file_checksums_one_in=1000000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=33554432 --write_dbid_to_manifest=0 --write_fault_one_in=128 --writepercent=50\n\nReviewed By: jowlyzhang\n\nDifferential Revision: D59825730\n\nPulled By: hx235\n\nfbshipit-source-id: 7d77aaf177ded2f99bf1ce19f5a4bd0783b9ca92",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl/db_impl_write.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/21db55f8164d2a6519dcc993f74bf7f49c700854",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ebe2116240a4efac4226975b975917f889010aa2",
        "author": "Hui Xiao",
        "date": "2024-07-09T15:35:54-07:00",
        "message": "Remove false-postive assertion in `FaultInjectionTestFS::RenameFile` (#12828)\n\nSummary:\n**Context/Summary:**\nThe assertion `tlist.find(tdn.second) == tlist.end()` https://github.com/facebook/rocksdb/blame/9eebaf11cbd875435b572f05f0378ecdb761cc74/utilities/fault_injection_fs.cc#L1003 can catch us false positive.\n\nSome context\n(1) When fault injection is enabled and db open fails because of that, crash test will retry open without injected error in order to proceed with a clean open:\nhttps://github.com/facebook/rocksdb/blob/9eebaf11cbd875435b572f05f0378ecdb761cc74/db_stress_tool/db_stress_test_base.cc#L3559\nhttps://github.com/facebook/rocksdb/blob/9eebaf11cbd875435b572f05f0378ecdb761cc74/db_stress_tool/db_stress_test_base.cc#L3586-L3639\n(2)\na. `FaultInjectionTestFS::dir_to_new_files_since_last_sync` records files that are created but not yet synced.\nb. When we create CURRENT, we will first create a temp file and rename it as \"CURRENT\". As part of the renaming, we will [assert](https://github.com/facebook/rocksdb/blame/9eebaf11cbd875435b572f05f0378ecdb761cc74/utilities/fault_injection_fs.cc#L1003) `FaultInjectionTestFS::dir_to_new_files_since_last_sync ` doesn't already have a file named `CURRENT`.\n\nSuppose the following sequence of events happened:\n\n(1) 1st open, with metadata write error\n1. As part of creating CURRENT file, added \"CURRENT\" to `FaultInjectionTestFS::dir_to_new_files_since_last_sync_`\nhttps://github.com/facebook/rocksdb/blob/9eebaf11cbd875435b572f05f0378ecdb761cc74/utilities/fault_injection_fs.cc#L735\n2.  `SyncDir()` here https://github.com/facebook/rocksdb/blob/9eebaf11cbd875435b572f05f0378ecdb761cc74/file/filename.cc#L412 failed with injected metadata write error. Therefore, \"CURRENT\" file didn't get removed from `FaultInjectionTestFS::dir_to_new_files_since_last_sync_` as it would if `SyncDir()` succeeded https://github.com/facebook/rocksdb/blob/9eebaf11cbd875435b572f05f0378ecdb761cc74/utilities/fault_injection_fs.h#L344\n\n(2) 2st open\n1. Attempted to create a CURRENT file and failed during renaming since `FaultInjectionTestFS::dir_to_new_files_since_last_sync_` already had a file called CURRENT. So  will fail\n```\nassertion failed - tlist.find(tdn.second) == tlist.end()\n```\n\nThis PR fixed this by removing the assertion. It used to catch us some missing sync of some directory (e.,g https://github.com/facebook/rocksdb/pull/10573) so we will keep thinking about a better way to catch that.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12828\n\nTest Plan:\nCommand constantly failed before the fix but passed after the PR running for 10 minutes\n```\npython3 tools/db_crashtest.py --simple blackbox --interval=10 --WAL_size_limit_MB=1 --WAL_ttl_seconds=60 --acquire_snapshot_one_in=100 --adaptive_readahead=1 --adm_policy=2 --advise_random_on_open=1 --allow_concurrent_memtable_write=1 --allow_data_in_errors=True --allow_fallocate=1 --async_io=0 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_flush_during_shutdown=0 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=0 --bgerror_resume_retry_interval=100 --block_align=0 --block_protection_bytes_per_key=8 --block_size=16384 --bloom_before_level=1 --bloom_bits=10 --bottommost_compression_type=lz4hc --bottommost_file_compaction_delay=86400 --bytes_per_sync=0 --cache_index_and_filter_blocks=1 --cache_index_and_filter_blocks_with_high_priority=0 --cache_size=8388608 --cache_type=tiered_auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=0 --charge_filter_construction=0 --charge_table_reader=0 --check_multiget_consistency=0 --check_multiget_entity_consistency=0 --checkpoint_one_in=10000 --checksum_type=kCRC32c --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000 --compact_range_one_in=1000000 --compaction_pri=3 --compaction_readahead_size=0 --compaction_ttl=1 --compress_format_version=1 --compressed_secondary_cache_ratio=0.5 --compressed_secondary_cache_size=0 --compression_checksum=0 --compression_max_dict_buffer_bytes=15 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=zstd --compression_use_zstd_dict_trainer=1 --compression_zstd_max_train_bytes=65536 --continuous_verification_interval=0 --daily_offpeak_time_utc= --data_block_index_type=1 --db_write_buffer_size=0 --default_temperature=kHot --default_write_temperature=kUnknown --delete_obsolete_files_period_micros=30000000 --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_file_deletions_one_in=10000 --disable_manual_compaction_one_in=10000 --disable_wal=0 --dump_malloc_stats=0 --enable_checksum_handoff=1 --enable_compaction_filter=0 --enable_custom_split_merge=0 --enable_do_not_compress_roles=0 --enable_index_compression=1 --enable_memtable_insert_with_hint_prefix_extractor=0 --enable_pipelined_write=0 --enable_sst_partitioner_factory=1 --enable_thread_tracking=1 --enable_write_thread_adaptive_yield=0 --error_recovery_with_no_fault_injection=1 --exclude_wal_from_write_fault_injection=1 --fail_if_options_file_error=1 --fifo_allow_compaction=0 --file_checksum_impl=crc32c --fill_cache=1 --flush_one_in=1000000 --format_version=3 --get_all_column_family_metadata_one_in=1000000 --get_current_wal_file_one_in=0 --get_live_files_apis_one_in=1000000 --get_properties_of_all_tables_one_in=100000 --get_property_one_in=100000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=2097152 --high_pri_pool_ratio=0 --index_block_restart_interval=2 --index_shortening=0 --index_type=2 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --inplace_update_support=0 --iterpercent=10 --key_len_percent_dist=1,30,69 --key_may_exist_one_in=100000 --last_level_temperature=kWarm --level_compaction_dynamic_level_bytes=0 --lock_wal_one_in=10000 --log_file_time_to_roll=60 --log_readahead_size=16777216 --long_running_snapshots=1 --low_pri_pool_ratio=0.5 --lowest_used_cache_tier=1 --manifest_preallocation_size=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=16384 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=1000000 --max_key_len=3 --max_log_file_size=0 --max_manifest_file_size=1073741824 --max_sequential_skip_in_iterations=1 --max_total_wal_size=0 --max_write_batch_group_size_bytes=16 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=2097152 --memtable_insert_hint_per_batch=0 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.1 --memtable_protection_bytes_per_key=8 --memtable_whole_key_filtering=0 --memtablerep=skip_list --metadata_charge_policy=1 --metadata_read_fault_one_in=32 --metadata_write_fault_one_in=0 --min_write_buffer_number_to_merge=2 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=1 --open_files=-1 --open_metadata_read_fault_one_in=0 --open_metadata_write_fault_one_in=8 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=100000000 --optimize_filters_for_hits=0 --optimize_filters_for_memory=0 --optimize_multiget_for_io=1 --paranoid_file_checks=1 --partition_filters=1 --partition_pinning=3 --pause_background_one_in=1000000 --periodic_compaction_seconds=1000 --prefix_size=5 --prefixpercent=5 --prepopulate_block_cache=1 --preserve_internal_time_seconds=0 --progress_reports=0 --promote_l0_one_in=0 --read_amp_bytes_per_bit=32 --read_fault_one_in=0 --readahead_size=524288 --readpercent=45 --recycle_log_file_num=0 --reopen=0 --report_bg_io_stats=0 --reset_stats_one_in=1000000 --sample_for_compression=0 --secondary_cache_fault_one_in=32 --secondary_cache_uri= --set_options_one_in=0 --skip_stats_update_on_db_open=0 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=68719476736 --sqfc_name=foo --sqfc_version=1 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=10 --stats_history_buffer_size=1048576 --strict_bytes_per_sync=1 --subcompactions=2 --sync=0 --sync_fault_injection=1 --table_cache_numshardbits=6 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=2 --uncache_aggressiveness=1582 --universal_max_read_amp=4 --unpartitioned_pinning=0 --use_adaptive_mutex=0 --use_adaptive_mutex_lru=1 --use_attribute_group=1 --use_delta_encoding=0 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=0 --use_multi_cf_iterator=1 --use_multi_get_entity=1 --use_multiget=0 --use_put_entity_one_in=1 --use_sqfc_for_range_queries=1 --use_timed_put_one_in=0 --use_write_buffer_manager=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000 --verify_compression=1 --verify_db_one_in=10000 --verify_file_checksums_one_in=1000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=33554432 --write_dbid_to_manifest=1 --write_fault_one_in=8 --writepercent=35\n```\n\nReviewed By: cbi42\n\nDifferential Revision: D59241548\n\nPulled By: hx235\n\nfbshipit-source-id: 5bb49e6a94943273f47578a2caf3d08ca5b67e5f",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/fault_injection_fs.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ebe2116240a4efac4226975b975917f889010aa2",
        "contains_optimization_keyword": true
    },
    {
        "hash": "093f4ef82cdd599896517f5414ee9ae7f6af3b35",
        "author": "Jeffery",
        "date": "2024-07-01T16:14:19-07:00",
        "message": "Fix db_rate_limiter_test for win (#12816)\n\nSummary:\nWe didn't implement file system prefetch for OS Win. During table open, it uses `FilePrefetchBuffer` instead and only do 1 read instead of 4 in BufferedIO.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12816\n\nReviewed By: jaykorean\n\nDifferential Revision: D59181835\n\nPulled By: ajkr\n\nfbshipit-source-id: 18b8f0247408cd1a80f289357ede5232ae5a3c66",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_rate_limiter_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/093f4ef82cdd599896517f5414ee9ae7f6af3b35",
        "contains_optimization_keyword": true
    },
    {
        "hash": "aec15eebec08429142fde04a4006303412def90c",
        "author": "Hui Xiao",
        "date": "2024-06-26T23:02:28-07:00",
        "message": "Ignore non-critical IO error in `BlockCacheLookupForReadAheadSize()` in crash test (#12814)\n\nSummary:\n**Context/Summary:**\n\nError in `BlockCacheLookupForReadAheadSize()` is not critical enough to return such error in read path. That's because the worst case is to not have any read ahead. See below comment. https://github.com/facebook/rocksdb/blob/a31fe521732c6150003ea43f1e30f27f13be597c/table/block_based/block_based_table_iterator.cc#L867-L871\n\nTherefore we should allow the read to return ok() even when we inject read error there.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12814\n\nTest Plan:\nBelow command failed with ` Didn't get expected error from PrefixScan` before the fix but passes after\n\n```\n./db_stress --WAL_size_limit_MB=0 --WAL_ttl_seconds=60 --acquire_snapshot_one_in=100 --adaptive_readahead=0 --adm_policy=3 --advise_random_on_open=0 --allow_concurrent_memtable_write=0 --allow_data_in_errors=True --allow_fallocate=1 --async_io=0 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_flush_during_shutdown=1 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=1000 --batch_protection_bytes_per_key=8 --bgerror_resume_retry_interval=1000000 --block_align=0 --block_protection_bytes_per_key=1 --block_size=16384 --bloom_before_level=5 --bloom_bits=29.31310447925055 --bottommost_compression_type=lz4hc --bottommost_file_compaction_delay=0 --bytes_per_sync=262144 --cache_index_and_filter_blocks=1 --cache_index_and_filter_blocks_with_high_priority=0 --cache_size=8388608 --cache_type=tiered_auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=1 --charge_filter_construction=0 --charge_table_reader=1 --check_multiget_consistency=0 --check_multiget_entity_consistency=0 --checkpoint_one_in=1000000 --checksum_type=kxxHash64 --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=0 --compaction_readahead_size=0 --compaction_ttl=0 --compress_format_version=2 --compressed_secondary_cache_ratio=0.6666666666666666 --compressed_secondary_cache_size=0 --compression_checksum=1 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=lz4 --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --daily_offpeak_time_utc= --data_block_index_type=0 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_whitebox --db_write_buffer_size=8388608 --default_temperature=kHot --default_write_temperature=kHot --delete_obsolete_files_period_micros=30000000 --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_file_deletions_one_in=10000 --disable_manual_compaction_one_in=10000 --disable_wal=0 --dump_malloc_stats=0 --enable_checksum_handoff=1 --enable_compaction_filter=0 --enable_custom_split_merge=0 --enable_do_not_compress_roles=1 --enable_index_compression=0 --enable_memtable_insert_with_hint_prefix_extractor=0 --enable_pipelined_write=1 --enable_sst_partitioner_factory=0 --enable_thread_tracking=1 --enable_write_thread_adaptive_yield=1 --error_recovery_with_no_fault_injection=0 --exclude_wal_from_write_fault_injection=0 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=0 --fifo_allow_compaction=0 --file_checksum_impl=big --fill_cache=1 --flush_one_in=1000 --format_version=3 --get_all_column_family_metadata_one_in=1000000 --get_current_wal_file_one_in=0 --get_live_files_apis_one_in=10000 --get_properties_of_all_tables_one_in=100000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=274877906944 --high_pri_pool_ratio=0 --index_block_restart_interval=2 --index_shortening=0 --index_type=2 --ingest_external_file_one_in=1000 --initial_auto_readahead_size=0 --inplace_update_support=0 --iterpercent=10 --key_len_percent_dist=1,30,69 --key_may_exist_one_in=100 --last_level_temperature=kHot --level_compaction_dynamic_level_bytes=0 --lock_wal_one_in=1000000 --log2_keys_per_lock=10 --log_file_time_to_roll=60 --log_readahead_size=0 --long_running_snapshots=0 --low_pri_pool_ratio=0 --lowest_used_cache_tier=2 --manifest_preallocation_size=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=0 --max_auto_readahead_size=16384 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=100000 --max_key_len=3 --max_log_file_size=0 --max_manifest_file_size=1073741824 --max_sequential_skip_in_iterations=1 --max_total_wal_size=0 --max_write_batch_group_size_bytes=16 --max_write_buffer_number=10 --max_write_buffer_size_to_maintain=2097152 --memtable_insert_hint_per_batch=0 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.1 --memtable_protection_bytes_per_key=8 --memtable_whole_key_filtering=1 --memtablerep=skip_list --metadata_charge_policy=0 --metadata_read_fault_one_in=32 --metadata_write_fault_one_in=128 --min_write_buffer_number_to_merge=2 --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --num_file_reads_for_auto_readahead=1 --open_files=100 --open_metadata_read_fault_one_in=0 --open_metadata_write_fault_one_in=8 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=20000000 --optimize_filters_for_hits=0 --optimize_filters_for_memory=1 --optimize_multiget_for_io=0 --paranoid_file_checks=1 --partition_filters=1 --partition_pinning=2 --pause_background_one_in=10000 --periodic_compaction_seconds=0 --prefix_size=5 --prefixpercent=5 --prepopulate_block_cache=1 --preserve_internal_time_seconds=0 --progress_reports=0 --promote_l0_one_in=0 --read_amp_bytes_per_bit=32 --read_fault_one_in=1000 --readahead_size=524288 --readpercent=45 --recycle_log_file_num=0 --reopen=20 --report_bg_io_stats=1 --reset_stats_one_in=1000000 --sample_for_compression=5 --secondary_cache_fault_one_in=32 --secondary_cache_uri= --skip_stats_update_on_db_open=1 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=1048576 --sqfc_name=foo --sqfc_version=1 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=600 --stats_history_buffer_size=0 --strict_bytes_per_sync=1 --subcompactions=2 --sync=0 --sync_fault_injection=0 --table_cache_numshardbits=6 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=1 --uncache_aggressiveness=203 --universal_max_read_amp=10 --unpartitioned_pinning=0 --use_adaptive_mutex=1 --use_adaptive_mutex_lru=1 --use_attribute_group=0 --use_delta_encoding=0 --use_direct_io_for_flush_and_compaction=1 --use_direct_reads=0 --use_full_merge_v1=1 --use_get_entity=0 --use_merge=0 --use_multi_cf_iterator=0 --use_multi_get_entity=0 --use_multiget=1 --use_put_entity_one_in=0 --use_sqfc_for_range_queries=1 --use_timed_put_one_in=0 --use_write_buffer_manager=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_compression=1 --verify_db_one_in=10000 --verify_file_checksums_one_in=1000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=33554432 --write_dbid_to_manifest=0 --write_fault_one_in=1000 --writepercent=35\n```\n\nReviewed By: jaykorean\n\nDifferential Revision: D59092430\n\nPulled By: hx235\n\nfbshipit-source-id: 39558c34461ce92275cae706c33dfd00e6f0ecce",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_iterator.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/aec15eebec08429142fde04a4006303412def90c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "cce51f06640dc693796d87507b89a315a2a52718",
        "author": "Jay Huh",
        "date": "2024-06-21T11:56:10-07:00",
        "message": "Fix heap-use-after-free in MultiCfIteratorImpl (#12784)\n\nSummary:\n# Summary\n\nWhen changing the direction of the multi-cf-iter, we do this by `Seek(current_key)` (if changing from backward to forward) or `SeekForPrev(current_key)` (if forward -> backward) in the child iters and rebuild the heap.\n\n`Slice target` is just a pointer and contents are not guaranteed to be the same after re-init the heap.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12784\n\nTest Plan:\nI was able to steadily repro by building with `COMPILE_WITH_ASAN=1` running db_stress.\n```\nCOMPILE_WITH_ASAN=1 make -j64 dbg\n```\n```\n./db_stress --WAL_size_limit_MB=1 --WAL_ttl_seconds=60 --acquire_snapshot_one_in=10000 --adaptive_readahead=0 --adm_policy=2 --advise_random_on_open=1 --allow_data_in_errors=True --allow_fallocate=0 --async_io=0 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_flush_during_shutdown=1 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=1000 --batch_protection_bytes_per_key=0 --bgerror_resume_retry_interval=100 --block_align=1 --block_protection_bytes_per_key=8 --block_size=16384 --bloom_before_level=2147483646 --bloom_bits=62.9095874568401 --bottommost_compression_type=none --bottommost_file_compaction_delay=600 --bytes_per_sync=0 --cache_index_and_filter_blocks=0 --cache_index_and_filter_blocks_with_high_priority=0 --cache_size=33554432 --cache_type=lru_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=1 --charge_filter_construction=1 --charge_table_reader=0 --check_multiget_consistency=0 --check_multiget_entity_consistency=0 --checkpoint_one_in=10000 --checksum_type=kxxHash64 --clear_column_family_one_in=0 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=1 --compaction_readahead_size=0 --compaction_ttl=100 --compress_format_version=2 --compressed_secondary_cache_size=8388608 --compression_checksum=1 --compression_max_dict_buffer_bytes=1099511627775 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=none --compression_use_zstd_dict_trainer=1 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --daily_offpeak_time_utc= --data_block_index_type=1 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_whitebox --db_write_buffer_size=0 --default_temperature=kUnknown --default_write_temperature=kWarm --delete_obsolete_files_period_micros=21600000000 --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_file_deletions_one_in=1000000 --disable_manual_compaction_one_in=10000 --disable_wal=0 --dump_malloc_stats=1 --enable_checksum_handoff=0 --enable_compaction_filter=0 --enable_custom_split_merge=1 --enable_do_not_compress_roles=0 --enable_index_compression=0 --enable_memtable_insert_with_hint_prefix_extractor=0 --enable_pipelined_write=1 --enable_sst_partitioner_factory=1 --enable_thread_tracking=1 --enable_write_thread_adaptive_yield=1 --error_recovery_with_no_fault_injection=0 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=0 --fifo_allow_compaction=1 --file_checksum_impl=crc32c --fill_cache=0 --flush_one_in=1000000 --format_version=4 --get_all_column_family_metadata_one_in=1000000 --get_current_wal_file_one_in=0 --get_live_files_apis_one_in=10000 --get_properties_of_all_tables_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=274877906944 --high_pri_pool_ratio=0 --index_block_restart_interval=4 --index_shortening=1 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=524288 --inplace_update_support=0 --iterpercent=10 --key_len_percent_dist=1,30,69 --key_may_exist_one_in=100000 --kill_random_test=888887 --last_level_temperature=kHot --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=10000 --log2_keys_per_lock=10 --log_file_time_to_roll=60 --log_readahead_size=0 --long_running_snapshots=1 --low_pri_pool_ratio=0 --lowest_used_cache_tier=0 --manifest_preallocation_size=5120 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=0 --max_auto_readahead_size=16384 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=100000 --max_key_len=3 --max_log_file_size=0 --max_manifest_file_size=1073741824 --max_sequential_skip_in_iterations=1 --max_total_wal_size=0 --max_write_batch_group_size_bytes=64 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=0 --memtable_insert_hint_per_batch=0 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0 --memtable_protection_bytes_per_key=8 --memtable_whole_key_filtering=0 --memtablerep=skip_list --metadata_charge_policy=0 --metadata_read_fault_one_in=1000 --metadata_write_fault_one_in=128 --min_write_buffer_number_to_merge=1 --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --num_file_reads_for_auto_readahead=1 --open_files=-1 --open_metadata_read_fault_one_in=0 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=16 --ops_per_thread=20000000 --optimize_filters_for_hits=0 --optimize_filters_for_memory=0 --optimize_multiget_for_io=0 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=3 --pause_background_one_in=1000000 --periodic_compaction_seconds=0 --persist_user_defined_timestamps=1 --prefix_size=-1 --prefixpercent=0 --prepopulate_block_cache=1 --preserve_internal_time_seconds=36000 --progress_reports=0 --promote_l0_one_in=0 --read_amp_bytes_per_bit=0 --read_fault_one_in=0 --readahead_size=0 --readpercent=50 --recycle_log_file_num=0 --reopen=20 --report_bg_io_stats=1 --reset_stats_one_in=10000 --sample_for_compression=0 --secondary_cache_fault_one_in=0 --secondary_cache_uri= --skip_stats_update_on_db_open=0 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=68719476736 --sqfc_name=bar --sqfc_version=0 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=10 --stats_history_buffer_size=1048576 --strict_bytes_per_sync=0 --subcompactions=1 --sync=0 --sync_fault_injection=1 --table_cache_numshardbits=6 --target_file_size_base=2097152 --target_file_size_multiplier=2 --test_batches_snapshots=0 --test_cf_consistency=0 --top_level_index_pinning=0 --uncache_aggressiveness=14 --universal_max_read_amp=-1 --unpartitioned_pinning=2 --use_adaptive_mutex=1 --use_adaptive_mutex_lru=0 --use_attribute_group=0 --use_delta_encoding=1 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=1 --use_full_merge_v1=0 --use_get_entity=1 --use_merge=0 --use_multi_cf_iterator=1 --use_multi_get_entity=1 --use_multiget=1 --use_put_entity_one_in=0 --use_sqfc_for_range_queries=1 --use_timed_put_one_in=0 --use_txn=0 --use_write_buffer_manager=0 --user_timestamp_size=8 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_compression=1 --verify_db_one_in=10000 --verify_file_checksums_one_in=1000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=zstd --write_buffer_size=4194304 --write_dbid_to_manifest=1 --write_fault_one_in=0 --writepercent=35\n```\n```\n==1606272==ERROR: AddressSanitizer: heap-use-after-free on address 0x6060000b0cc0 at pc 0x7f733469c7de bp 0x7f7311bfcfe0 sp 0x7f7311bfc790\nREAD of size 40 at 0x6060000b0cc0 thread T57\n    #0 0x7f733469c7dd in __interceptor_memcpy /home/engshare/third-party2/gcc/11.x/src/gcc-11.x/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:827\n    https://github.com/facebook/rocksdb/issues/1 0x7f7331f65f7e in rocksdb::IterKey::SetInternalKey(rocksdb::Slice const&, rocksdb::Slice const&, unsigned long, rocksdb::ValueType, rocksdb::Slice const*) db/dbformat.h:761\n    https://github.com/facebook/rocksdb/issues/2 0x7f7331f661ee in rocksdb::IterKey::SetInternalKey(rocksdb::Slice const&, unsigned long, rocksdb::ValueType, rocksdb::Slice const*) db/dbformat.h:776\n    https://github.com/facebook/rocksdb/issues/3 0x7f73323039ff in rocksdb::DBIter::SetSavedKeyToSeekTarget(rocksdb::Slice const&) db/db_iter.cc:1462\n    https://github.com/facebook/rocksdb/issues/4 0x7f7332304eb8 in rocksdb::DBIter::Seek(rocksdb::Slice const&) db/db_iter.cc:1540\n    https://github.com/facebook/rocksdb/issues/5 0x7f7331d94abd in rocksdb::ArenaWrappedDBIter::Seek(rocksdb::Slice const&) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x1394abd)\n    https://github.com/facebook/rocksdb/issues/6 0x7f73320f1a52 in rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}::operator()(rocksdb::Iterator*) const db/multi_cf_iterator_impl.h:73\n    https://github.com/facebook/rocksdb/issues/7 0x7f73320fccf0 in void rocksdb::MultiCfIteratorImpl::SeekCommon<rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >, rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}>(rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >&, rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x16fccf0)\n    https://github.com/facebook/rocksdb/issues/8 0x7f73320f1a93 in rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&) db/multi_cf_iterator_impl.h:73\n    https://github.com/facebook/rocksdb/issues/9 0x7f73320f1dbe in rocksdb::MultiCfIteratorImpl::Next()::{lambda()https://github.com/facebook/rocksdb/issues/1}::operator()() const db/multi_cf_iterator_impl.h:90\n    https://github.com/facebook/rocksdb/issues/10 0x7f73320fe159 in rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >& rocksdb::MultiCfIteratorImpl::GetHeap<rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >, rocksdb::MultiCfIteratorImpl::Next()::{lambda()https://github.com/facebook/rocksdb/issues/1}>(rocksdb::MultiCfIteratorImpl::Next()::{lambda()https://github.com/facebook/rocksdb/issues/1}) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x16fe159)\n    https://github.com/facebook/rocksdb/issues/11 0x7f73320f1ec9 in rocksdb::MultiCfIteratorImpl::Next() db/multi_cf_iterator_impl.h:87\n    https://github.com/facebook/rocksdb/issues/12 0x7f73320f3255 in rocksdb::CoalescingIterator::Next() db/coalescing_iterator.h:34\n    https://github.com/facebook/rocksdb/issues/13 0x66f28a in TestIterateImpl<rocksdb::Iterator, rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, const rocksdb::ReadOptions&, const std::vector<int>&, const std::vector<long int>&)::<lambda(const rocksdb::ReadOptions&)>, rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, const rocksdb::ReadOptions&, const std::vector<int>&, const std::vector<long int>&)::<lambda(rocksdb::Iterator*)> > db_stress_tool/db_stress_test_base.cc:1718\n    https://github.com/facebook/rocksdb/issues/14 0x6440b4 in rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, rocksdb::ReadOptions const&, std::vector<int, std::allocator<int> > const&, std::vector<long, std::allocator<long> > const&) db_stress_tool/db_stress_test_base.cc:1504\n    https://github.com/facebook/rocksdb/issues/15 0x640cb0 in rocksdb::StressTest::OperateDb(rocksdb::ThreadState*) db_stress_tool/db_stress_test_base.cc:1376\n    https://github.com/facebook/rocksdb/issues/16 0x6004f6 in rocksdb::ThreadBody(void*) db_stress_tool/db_stress_driver.cc:39\n    https://github.com/facebook/rocksdb/issues/17 0x7f73327caed4 in StartThreadWrapper env/env_posix.cc:469\n    https://github.com/facebook/rocksdb/issues/18 0x7f733029abc8 in start_thread /home/engshare/third-party2/glibc/2.34/src/glibc-2.34/nptl/pthread_create.c:434\n    https://github.com/facebook/rocksdb/issues/19 0x7f733032cf5b in __GI___clone3 (/usr/local/fbcode/platform010/lib/libc.so.6+0x12cf5b)\n\n0x6060000b0cc0 is located 0 bytes inside of 55-byte region [0x6060000b0cc0,0x6060000b0cf7)\nfreed by thread T57 here:\n    #0 0x7f73346d1d77 in operator delete[](void*) /home/engshare/third-party2/gcc/11.x/src/gcc-11.x/libsanitizer/asan/asan_new_delete.cpp:163\n    https://github.com/facebook/rocksdb/issues/1 0x7f7331d9274b in rocksdb::IterKey::ResetBuffer() db/dbformat.h:830\n    https://github.com/facebook/rocksdb/issues/2 0x7f73323146b9 in rocksdb::IterKey::EnlargeBuffer(unsigned long) db/dbformat.cc:278\n    https://github.com/facebook/rocksdb/issues/3 0x7f7331f33031 in rocksdb::IterKey::EnlargeBufferIfNeeded(unsigned long) db/dbformat.h:846\n    https://github.com/facebook/rocksdb/issues/4 0x7f7331f65ee0 in rocksdb::IterKey::SetInternalKey(rocksdb::Slice const&, rocksdb::Slice const&, unsigned long, rocksdb::ValueType, rocksdb::Slice const*) db/dbformat.h:757\n    https://github.com/facebook/rocksdb/issues/5 0x7f7331f661ee in rocksdb::IterKey::SetInternalKey(rocksdb::Slice const&, unsigned long, rocksdb::ValueType, rocksdb::Slice const*) db/dbformat.h:776\n    https://github.com/facebook/rocksdb/issues/6 0x7f73323039ff in rocksdb::DBIter::SetSavedKeyToSeekTarget(rocksdb::Slice const&) db/db_iter.cc:1462\n    https://github.com/facebook/rocksdb/issues/7 0x7f7332304eb8 in rocksdb::DBIter::Seek(rocksdb::Slice const&) db/db_iter.cc:1540\n    https://github.com/facebook/rocksdb/issues/8 0x7f7331d94abd in rocksdb::ArenaWrappedDBIter::Seek(rocksdb::Slice const&) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x1394abd)\n    https://github.com/facebook/rocksdb/issues/9 0x7f73320f1a52 in rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}::operator()(rocksdb::Iterator*) const db/multi_cf_iterator_impl.h:73\n    https://github.com/facebook/rocksdb/issues/10 0x7f73320fccf0 in void rocksdb::MultiCfIteratorImpl::SeekCommon<rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >, rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}>(rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >&, rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x16fccf0)\n    https://github.com/facebook/rocksdb/issues/11 0x7f73320f1a93 in rocksdb::MultiCfIteratorImpl::Seek(rocksdb::Slice const&) db/multi_cf_iterator_impl.h:73\n    https://github.com/facebook/rocksdb/issues/12 0x7f73320f1dbe in rocksdb::MultiCfIteratorImpl::Next()::{lambda()https://github.com/facebook/rocksdb/issues/1}::operator()() const db/multi_cf_iterator_impl.h:90\n    https://github.com/facebook/rocksdb/issues/13 0x7f73320fe159 in rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >& rocksdb::MultiCfIteratorImpl::GetHeap<rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::greater<int> > >, rocksdb::MultiCfIteratorImpl::Next()::{lambda()https://github.com/facebook/rocksdb/issues/1}>(rocksdb::MultiCfIteratorImpl::Next()::{lambda()https://github.com/facebook/rocksdb/issues/1}) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x16fe159)\n    https://github.com/facebook/rocksdb/issues/14 0x7f73320f1ec9 in rocksdb::MultiCfIteratorImpl::Next() db/multi_cf_iterator_impl.h:87\n    https://github.com/facebook/rocksdb/issues/15 0x7f73320f3255 in rocksdb::CoalescingIterator::Next() db/coalescing_iterator.h:34\n    https://github.com/facebook/rocksdb/issues/16 0x66f28a in TestIterateImpl<rocksdb::Iterator, rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, const rocksdb::ReadOptions&, const std::vector<int>&, const std::vector<long int>&)::<lambda(const rocksdb::ReadOptions&)>, rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, const rocksdb::ReadOptions&, const std::vector<int>&, const std::vector<long int>&)::<lambda(rocksdb::Iterator*)> > db_stress_tool/db_stress_test_base.cc:1718\n    https://github.com/facebook/rocksdb/issues/17 0x6440b4 in rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, rocksdb::ReadOptions const&, std::vector<int, std::allocator<int> > const&, std::vector<long, std::allocator<long> > const&) db_stress_tool/db_stress_test_base.cc:1504\n    https://github.com/facebook/rocksdb/issues/18 0x640cb0 in rocksdb::StressTest::OperateDb(rocksdb::ThreadState*) db_stress_tool/db_stress_test_base.cc:1376\n    https://github.com/facebook/rocksdb/issues/19 0x6004f6 in rocksdb::ThreadBody(void*) db_stress_tool/db_stress_driver.cc:39\n    https://github.com/facebook/rocksdb/issues/20 0x7f73327caed4 in StartThreadWrapper env/env_posix.cc:469\n    https://github.com/facebook/rocksdb/issues/21 0x7f733029abc8 in start_thread /home/engshare/third-party2/glibc/2.34/src/glibc-2.34/nptl/pthread_create.c:434\n\npreviously allocated by thread T57 here:\n    #0 0x7f73346d13b7 in operator new[](unsigned long) /home/engshare/third-party2/gcc/11.x/src/gcc-11.x/libsanitizer/asan/asan_new_delete.cpp:102\n    https://github.com/facebook/rocksdb/issues/1 0x7f73323146c5 in rocksdb::IterKey::EnlargeBuffer(unsigned long) db/dbformat.cc:279\n    https://github.com/facebook/rocksdb/issues/2 0x7f7331f33031 in rocksdb::IterKey::EnlargeBufferIfNeeded(unsigned long) db/dbformat.h:846\n    https://github.com/facebook/rocksdb/issues/3 0x7f7331f65ee0 in rocksdb::IterKey::SetInternalKey(rocksdb::Slice const&, rocksdb::Slice const&, unsigned long, rocksdb::ValueType, rocksdb::Slice const*) db/dbformat.h:757\n    https://github.com/facebook/rocksdb/issues/4 0x7f7331f661ee in rocksdb::IterKey::SetInternalKey(rocksdb::Slice const&, unsigned long, rocksdb::ValueType, rocksdb::Slice const*) db/dbformat.h:776\n    https://github.com/facebook/rocksdb/issues/5 0x7f7332303e1e in rocksdb::DBIter::SetSavedKeyToSeekForPrevTarget(rocksdb::Slice const&) db/db_iter.cc:1479\n    https://github.com/facebook/rocksdb/issues/6 0x7f7332306302 in rocksdb::DBIter::SeekForPrev(rocksdb::Slice const&) db/db_iter.cc:1615\n    https://github.com/facebook/rocksdb/issues/7 0x7f7331d94b0f in rocksdb::ArenaWrappedDBIter::SeekForPrev(rocksdb::Slice const&) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x1394b0f)\n    https://github.com/facebook/rocksdb/issues/8 0x7f73320f1c5a in rocksdb::MultiCfIteratorImpl::SeekForPrev(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}::operator()(rocksdb::Iterator*) const db/multi_cf_iterator_impl.h:82\n    https://github.com/facebook/rocksdb/issues/9 0x7f73320fdc1e in void rocksdb::MultiCfIteratorImpl::SeekCommon<rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::less<int> > >, rocksdb::MultiCfIteratorImpl::SeekForPrev(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}>(rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::less<int> > >&, rocksdb::MultiCfIteratorImpl::SeekForPrev(rocksdb::Slice const&)::{lambda(rocksdb::Iterator*)https://github.com/facebook/rocksdb/issues/2}) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x16fdc1e)\n    https://github.com/facebook/rocksdb/issues/10 0x7f73320f1c9b in rocksdb::MultiCfIteratorImpl::SeekForPrev(rocksdb::Slice const&) db/multi_cf_iterator_impl.h:81\n    https://github.com/facebook/rocksdb/issues/11 0x7f73320f2002 in rocksdb::MultiCfIteratorImpl::Prev()::{lambda()https://github.com/facebook/rocksdb/issues/1}::operator()() const db/multi_cf_iterator_impl.h:99\n    https://github.com/facebook/rocksdb/issues/12 0x7f73320ff223 in rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::less<int> > >& rocksdb::MultiCfIteratorImpl::GetHeap<rocksdb::BinaryHeap<rocksdb::MultiCfIteratorInfo, rocksdb::MultiCfIteratorImpl::MultiCfHeapItemComparator<std::less<int> > >, rocksdb::MultiCfIteratorImpl::Prev()::{lambda()https://github.com/facebook/rocksdb/issues/1}>(rocksdb::MultiCfIteratorImpl::Prev()::{lambda()https://github.com/facebook/rocksdb/issues/1}) (/data/users/jewoongh/rocksdb/librocksdb.so.9.4+0x16ff223)\n    https://github.com/facebook/rocksdb/issues/13 0x7f73320f210d in rocksdb::MultiCfIteratorImpl::Prev() db/multi_cf_iterator_impl.h:96\n    https://github.com/facebook/rocksdb/issues/14 0x7f73320f3275 in rocksdb::CoalescingIterator::Prev() db/coalescing_iterator.h:35\n    https://github.com/facebook/rocksdb/issues/15 0x66f440 in TestIterateImpl<rocksdb::Iterator, rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, const rocksdb::ReadOptions&, const std::vector<int>&, const std::vector<long int>&)::<lambda(const rocksdb::ReadOptions&)>, rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, const rocksdb::ReadOptions&, const std::vector<int>&, const std::vector<long int>&)::<lambda(rocksdb::Iterator*)> > db_stress_tool/db_stress_test_base.cc:1725\n    https://github.com/facebook/rocksdb/issues/16 0x6440b4 in rocksdb::StressTest::TestIterate(rocksdb::ThreadState*, rocksdb::ReadOptions const&, std::vector<int, std::allocator<int> > const&, std::vector<long, std::allocator<long> > const&) db_stress_tool/db_stress_test_base.cc:1504\n    https://github.com/facebook/rocksdb/issues/17 0x640cb0 in rocksdb::StressTest::OperateDb(rocksdb::ThreadState*) db_stress_tool/db_stress_test_base.cc:1376\n    https://github.com/facebook/rocksdb/issues/18 0x6004f6 in rocksdb::ThreadBody(void*) db_stress_tool/db_stress_driver.cc:39\n    https://github.com/facebook/rocksdb/issues/19 0x7f73327caed4 in StartThreadWrapper env/env_posix.cc:469\n    https://github.com/facebook/rocksdb/issues/20 0x7f733029abc8 in start_thread /home/engshare/third-party2/glibc/2.34/src/glibc-2.34/nptl/pthread_create.c:434\n\nThread T57 created by T0 here:\n    #0 0x7f7334642136 in __interceptor_pthread_create /home/engshare/third-party2/gcc/11.x/src/gcc-11.x/libsanitizer/asan/asan_interceptors.cpp:216\n    https://github.com/facebook/rocksdb/issues/1 0x7f73327cb008 in StartThread env/env_posix.cc:479\n    https://github.com/facebook/rocksdb/issues/2 0x7f733276b406 in rocksdb::CompositeEnvWrapper::StartThread(void (*)(void*), void*) env/composite_env_wrapper.h:316\n    https://github.com/facebook/rocksdb/issues/3 0x7f733276b406 in rocksdb::CompositeEnvWrapper::StartThread(void (*)(void*), void*) env/composite_env_wrapper.h:316\n    https://github.com/facebook/rocksdb/issues/4 0x6013d9 in rocksdb::RunStressTestImpl(rocksdb::SharedState*) db_stress_tool/db_stress_driver.cc:108\n    https://github.com/facebook/rocksdb/issues/5 0x603083 in rocksdb::RunStressTest(rocksdb::SharedState*) db_stress_tool/db_stress_driver.cc:248\n    https://github.com/facebook/rocksdb/issues/6 0x4e6ab3 in rocksdb::db_stress_tool(int, char**) db_stress_tool/db_stress_tool.cc:365\n    https://github.com/facebook/rocksdb/issues/7 0x4e260a in main db_stress_tool/db_stress.cc:23\n    https://github.com/facebook/rocksdb/issues/8 0x7f733022c656 in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\n    https://github.com/facebook/rocksdb/issues/9 0x7f733022c717 in __libc_start_main_impl ../csu/libc-start.c:409\n    https://github.com/facebook/rocksdb/issues/10 0x4e2530 in _start (/data/users/jewoongh/rocksdb/db_stress+0x4e2530)\n```\n\n`heap-use-after-free` was no longer happening with the same command after making the change.\n\nReviewed By: pdillinger\n\nDifferential Revision: D58871081\n\nPulled By: jaykorean\n\nfbshipit-source-id: 0194c34ffec5f16a6556c6bf3941a27253a4ecb4",
        "modified_files_count": 1,
        "modified_files": [
            "db/multi_cf_iterator_impl.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/cce51f06640dc693796d87507b89a315a2a52718",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a2f772910ed80a3ba357351bf5e835bb73f4fd6d",
        "author": "Hui Xiao",
        "date": "2024-06-12T12:17:45-07:00",
        "message": "Fix manual WAL flush causing false-positive inconsistent values in TestBackupRestore() (#12758)\n\nSummary:\n**Context/Summary:**\nWhen manual WAL flush is used, the following can happen:\n\nt1: Issued Put(k1) to original DB. It entered WAL buffer since manual_wal_flush_one_in > 0. It never made it to WAL file without FlushWAL()\nt2: The same WAL got back-up and restored to restore DB. So the restore DB's WAL does not contain this Put()\nt3: The same WAL in the original DB got FlushWAL() so it got the Put() entry\n\nQuerying k1 in original and restored DB will give different result and fail our consistency check in stress test.\n\n```\nFailure in a backup/restore operation with: Corruption: 0x000000000000000178 exists in original db but not in restore\n```\n\nThis PR fixed it.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12758\n\nTest Plan:\n```\n\n./db_stress --WAL_size_limit_MB=0 --WAL_ttl_seconds=0 --acquire_snapshot_one_in=10000 --adaptive_readahead=1 --adm_policy=1 --advise_random_on_open=1 --allow_concurrent_memtable_write=0 --allow_data_in_errors=True --allow_fallocate=1 --async_io=1 --auto_readahead_size=0 --avoid_flush_during_recovery=1 --avoid_flush_during_shutdown=1 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=100 --batch_protection_bytes_per_key=8 --bgerror_resume_retry_interval=1000000 --block_align=1 --block_protection_bytes_per_key=0 --block_size=16384 --bloom_before_level=2147483646 --bloom_bits=13 --bottommost_compression_type=none --bottommost_file_compaction_delay=600 --bytes_per_sync=262144 --cache_index_and_filter_blocks=0 --cache_index_and_filter_blocks_with_high_priority=0 --cache_size=33554432 --cache_type=auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=0 --charge_filter_construction=1 --charge_table_reader=0 --check_multiget_consistency=0 --check_multiget_entity_consistency=1 --checkpoint_one_in=1000000 --checksum_type=kxxHash --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000 --compact_range_one_in=1000000 --compaction_pri=3 --compaction_readahead_size=0 --compaction_ttl=0 --compress_format_version=1 --compressed_secondary_cache_size=8388608 --compression_checksum=0 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=4 --compression_type=none --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --daily_offpeak_time_utc= --data_block_index_type=0 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_blackbox_1 --db_write_buffer_size=0 --default_temperature=kCold --default_write_temperature=kHot --delete_obsolete_files_period_micros=21600000000 --delpercent=40 --delrangepercent=0 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_file_deletions_one_in=1000000 --disable_manual_compaction_one_in=10000 --disable_wal=0 --dump_malloc_stats=0 --enable_checksum_handoff=0 --enable_compaction_filter=0 --enable_custom_split_merge=1 --enable_do_not_compress_roles=1 --enable_index_compression=0 --enable_memtable_insert_with_hint_prefix_extractor=0 --enable_pipelined_write=0 --enable_sst_partitioner_factory=1 --enable_thread_tracking=0 --enable_write_thread_adaptive_yield=1 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected_1 --fail_if_options_file_error=1 --fifo_allow_compaction=1 --file_checksum_impl=none --fill_cache=0 --flush_one_in=1000000 --format_version=2 --get_all_column_family_metadata_one_in=1000000 --get_current_wal_file_one_in=0 --get_live_files_apis_one_in=1000000 --get_properties_of_all_tables_one_in=1000000 --get_property_one_in=100000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=274877906944 --high_pri_pool_ratio=0.5 --index_block_restart_interval=5 --index_shortening=2 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --inplace_update_support=0 --iterpercent=10 --key_len_percent_dist=1,30,69 --key_may_exist_one_in=100 --last_level_temperature=kUnknown --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=0 --log_file_time_to_roll=60 --log_readahead_size=16777216 --long_running_snapshots=0 --low_pri_pool_ratio=0.5 --lowest_used_cache_tier=1 --manifest_preallocation_size=5120 --manual_wal_flush_one_in=100 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=524288 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=10 --max_key_len=3 --max_log_file_size=0 --max_manifest_file_size=1073741824 --max_sequential_skip_in_iterations=16 --max_total_wal_size=0 --max_write_batch_group_size_bytes=64 --max_write_buffer_number=10 --max_write_buffer_size_to_maintain=2097152 --memtable_insert_hint_per_batch=1 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.1 --memtable_protection_bytes_per_key=1 --memtable_whole_key_filtering=0 --memtablerep=skip_list --metadata_charge_policy=1 --min_write_buffer_number_to_merge=1 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=0 --open_files=-1 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=16 --ops_per_thread=100000000 --optimize_filters_for_hits=0 --optimize_filters_for_memory=0 --optimize_multiget_for_io=1 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=0 --pause_background_one_in=1000000 --periodic_compaction_seconds=2 --prefix_size=7 --prefixpercent=5 --prepopulate_block_cache=0 --preserve_internal_time_seconds=0 --progress_reports=0 --promote_l0_one_in=0 --read_amp_bytes_per_bit=0 --read_fault_one_in=1000 --readahead_size=16384 --readpercent=0 --recycle_log_file_num=0 --reopen=0 --report_bg_io_stats=0 --reset_stats_one_in=10000 --sample_for_compression=0 --secondary_cache_fault_one_in=0 --secondary_cache_uri= --set_options_one_in=0 --skip_stats_update_on_db_open=1 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=68719476736 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=600 --stats_history_buffer_size=0 --strict_bytes_per_sync=1 --subcompactions=2 --sync=0 --sync_fault_injection=1 --table_cache_numshardbits=-1 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=2 --uncache_aggressiveness=709 --universal_max_read_amp=0 --unpartitioned_pinning=0 --use_adaptive_mutex=0 --use_adaptive_mutex_lru=1 --use_attribute_group=1 --use_delta_encoding=1 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=1 --use_multi_cf_iterator=0 --use_multi_get_entity=0 --use_multiget=0 --use_put_entity_one_in=0 --use_timed_put_one_in=0 --use_write_buffer_manager=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000 --verify_compression=0 --verify_db_one_in=100000 --verify_file_checksums_one_in=0 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=335544 --write_dbid_to_manifest=1 --write_fault_one_in=128 --writepercent=45\n```\nRepro-ed quickly before the fix and stably run after the fix.\n\nReviewed By: jowlyzhang\n\nDifferential Revision: D58426535\n\nPulled By: hx235\n\nfbshipit-source-id: 611e56086e76f8c06d292624e60fd96e511ce723",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_test_base.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a2f772910ed80a3ba357351bf5e835bb73f4fd6d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d64eac28d32a025770cba641ea04e697f475cdd6",
        "author": "Peter Dillinger",
        "date": "2024-06-11T21:41:21-07:00",
        "message": "Fix a failure to propagate ReadOptions (#12757)\n\nSummary:\nThe crash test revealed a case in which the uncache functionality in ~BlockBasedTableReader could initiate an block read (IO), despite setting ReadOptions::read_tier = kBlockCacheTier.\n\nThe root cause is a place in the code where many people have over time decided to opt-in propagating ReadOptions and no one took the initiative to propagate ReadOptions by default (opt out / override only as needed). The fix is in partitioned_index_reader.cc. Here,\nReadOptions::readahead_size is opted-out to avoid churn in prefetch_test that is not clearly an improvement or regression. It's hard to tell given the poor state of relevant documentation https://github.com/facebook/rocksdb/issues/12756. The affected unit test was added in https://github.com/facebook/rocksdb/issues/10602.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12757\n\nTest Plan: (Now postponed to a follow-up diff) I have added some new infrastructure to DEBUG builds to catch this specific kind of violation in unit tests and in the stress/crash test. `EnforceReadOpts` establishes a thread-local context under which we assert no IOs are performed if ReadOptions said it should be forbidden. With this new checking, the Uncache unit test would catch the critical step toward a violation (inner ReadOptions allowing IO, even if no IO is actually performed), which is fixed with the production code change.\n\nReviewed By: hx235\n\nDifferential Revision: D58421526\n\nPulled By: pdillinger\n\nfbshipit-source-id: 9e9917a0e320c78967e751bd887926a2ed231d37",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/partitioned_index_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d64eac28d32a025770cba641ea04e697f475cdd6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a8a52e5b4d938c1aea28d7a96ec70a459ffaa1a4",
        "author": "Valery Mironov",
        "date": "2024-06-04T09:41:53-07:00",
        "message": "Fix AddressSanitizer container-overflow (#12722)\n\nSummary:\n```\nERROR: AddressSanitizer: container-overflow on address 0x506000682221 at pc 0x5583da569f76 bp 0x7f0ec8a9ffb0 sp 0x7f0ec8a9f780\nWRITE of size 53 at 0x506000682221 thread T29\n    #0 0x5583da569f75 in pread\n    https://github.com/facebook/rocksdb/issues/1 0x5583e334fde4 in rocksdb::PosixRandomAccessFile::Read(unsigned long, unsigned long, rocksdb::IOOptions const&, rocksdb::Slice*, char*, rocksdb::IODebugContext*) const /rocksdb/env/io_posix.cc:580:9\n    https://github.com/facebook/rocksdb/issues/2 0x5583e2cac42b in rocksdb::(anonymous namespace)::CompositeRandomAccessFileWrapper::Read(unsigned long, unsigned long, rocksdb::Slice*, char*) const /rocksdb/env/composite_env.cc:61:21\n    https://github.com/facebook/rocksdb/issues/3 0x5583e2c8a8e4 in rocksdb::(anonymous namespace)::LegacyRandomAccessFileWrapper::Read(unsigned long, unsigned long, rocksdb::IOOptions const&, rocksdb::Slice*, char*, rocksdb::IODebugContext*) const /rocksdb/env/env.cc:152:41\n    https://github.com/facebook/rocksdb/issues/4 0x5583e2d6cbfb in rocksdb::RandomAccessFileReader::Read(rocksdb::IOOptions const&, unsigned long, unsigned long, rocksdb::Slice*, char*, std::__2::unique_ptr<char [], std::__2::default_delete<char []>>*, rocksdb::Env::IOPriority) const /rocksdb/file/random_access_file_reader.cc:204:25\n    https://github.com/facebook/rocksdb/issues/5 0x5583e307c614 in rocksdb::ReadFooterFromFile(rocksdb::IOOptions const&, rocksdb::RandomAccessFileReader*, rocksdb::FilePrefetchBuffer*, unsigned long, rocksdb::Footer*, unsigned long) /rocksdb/table/format.cc:383:17\n    https://github.com/facebook/rocksdb/issues/6 0x5583e2f88456 in rocksdb::BlockBasedTable::Open(rocksdb::ReadOptions const&, rocksdb::ImmutableOptions const&, rocksdb::EnvOptions const&, rocksdb::BlockBasedTableOptions const&, rocksdb::InternalKeyComparator const&, std::__2::unique_ptr<rocksdb::RandomAccessFileReader, std::__2::default_delete<rocksdb::RandomAccessFileReader>>&&, unsigned long, std::__2::unique_ptr<rocksdb::TableReader, std::__2::default_delete<rocksdb::TableReader>>*, std::__2::shared_ptr<rocksdb::CacheReservationManager>, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, bool, bool, int, bool, unsigned long, bool, rocksdb::TailPrefetchStats*, rocksdb::BlockCacheTracer*, unsigned long, std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>> const&, unsigned long) /rocksdb/table/block_based/block_based_table_reader.cc:610:9\n    https://github.com/facebook/rocksdb/issues/7 0x5583e2ef7837 in rocksdb::BlockBasedTableFactory::NewTableReader(rocksdb::ReadOptions const&, rocksdb::TableReaderOptions const&, std::__2::unique_ptr<rocksdb::RandomAccessFileReader, std::__2::default_delete<rocksdb::RandomAccessFileReader>>&&, unsigned long, std::__2::unique_ptr<rocksdb::TableReader, std::__2::default_delete<rocksdb::TableReader>>*, bool) const /rocksdb/table/block_based/block_based_table_factory.cc:599:10\n    https://github.com/facebook/rocksdb/issues/8 0x5583e2ab873c in rocksdb::TableCache::GetTableReader(rocksdb::ReadOptions const&, rocksdb::FileOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, bool, bool, rocksdb::HistogramImpl*, std::__2::unique_ptr<rocksdb::TableReader, std::__2::default_delete<rocksdb::TableReader>>*, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, bool, int, bool, unsigned long, rocksdb::Temperature) /rocksdb/db/table_cache.cc:142:34\n    https://github.com/facebook/rocksdb/issues/9 0x5583e2aba5f6 in rocksdb::TableCache::FindTable(rocksdb::ReadOptions const&, rocksdb::FileOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, rocksdb::Cache::Handle**, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, bool, bool, rocksdb::HistogramImpl*, bool, int, bool, unsigned long, rocksdb::Temperature) /rocksdb/db/table_cache.cc:190:16\n    https://github.com/facebook/rocksdb/issues/10 0x5583e2abb7e1 in rocksdb::TableCache::NewIterator(rocksdb::ReadOptions const&, rocksdb::FileOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileMetaData const&, rocksdb::RangeDelAggregator*, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, rocksdb::TableReader**, rocksdb::HistogramImpl*, rocksdb::TableReaderCaller, rocksdb::Arena*, bool, int, unsigned long, rocksdb::InternalKey const*, rocksdb::InternalKey const*, bool) /rocksdb/db/table_cache.cc:235:9\n    https://github.com/facebook/rocksdb/issues/11 0x5583e28d14cf in rocksdb::BuildTable(std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>> const&, rocksdb::VersionSet*, rocksdb::ImmutableDBOptions const&, rocksdb::TableBuilderOptions const&, rocksdb::FileOptions const&, rocksdb::TableCache*, rocksdb::InternalIteratorBase<rocksdb::Slice>*, std::__2::vector<std::__2::unique_ptr<rocksdb::FragmentedRangeTombstoneIterator, std::__2::default_delete<rocksdb::FragmentedRangeTombstoneIterator>>, std::__2::allocator<std::__2::unique_ptr<rocksdb::FragmentedRangeTombstoneIterator, std::__2::default_delete<rocksdb::FragmentedRangeTombstoneIterator>>>>, rocksdb::FileMetaData*, std::__2::vector<rocksdb::BlobFileAddition, std::__2::allocator<rocksdb::BlobFileAddition>>*, std::__2::vector<unsigned long, std::__2::allocator<unsigned long>>, unsigned long, unsigned long, rocksdb::SnapshotChecker*, bool, rocksdb::InternalStats*, rocksdb::IOStatus*, std::__2::shared_ptr<rocksdb::IOTracer> const&, rocksdb::BlobFileCreationReason, rocksdb::EventLogger*, int, rocksdb::Env::IOPriority, rocksdb::TableProperties*, rocksdb::Env::WriteLifeTimeHint, std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>> const*, rocksdb::BlobFileCompletionCallback*, unsigned long*, unsigned long*, unsigned long*) /rocksdb/db/builder.cc:335:57\n    https://github.com/facebook/rocksdb/issues/12 0x5583e29bf29d in rocksdb::FlushJob::WriteLevel0Table() /rocksdb/db/flush_job.cc:919:11\n    https://github.com/facebook/rocksdb/issues/13 0x5583e29b33ac in rocksdb::FlushJob::Run(rocksdb::LogsWithPrepTracker*, rocksdb::FileMetaData*, bool*) /rocksdb/db/flush_job.cc:276:9\n    https://github.com/facebook/rocksdb/issues/14 0x5583e27a4781 in rocksdb::DBImpl::FlushMemTableToOutputFile(rocksdb::ColumnFamilyData*, rocksdb::MutableCFOptions const&, bool*, rocksdb::JobContext*, rocksdb::SuperVersionContext*, std::__2::vector<unsigned long, std::__2::allocator<unsigned long>>&, unsigned long, rocksdb::SnapshotChecker*, rocksdb::LogBuffer*, rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:258:19\n    https://github.com/facebook/rocksdb/issues/15 0x5583e27a7a96 in rocksdb::DBImpl::FlushMemTablesToOutputFiles(rocksdb::autovector<rocksdb::DBImpl::BGFlushArg, 8ul> const&, bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:377:14\n    https://github.com/facebook/rocksdb/issues/16 0x5583e27d6777 in rocksdb::DBImpl::BackgroundFlush(bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::FlushReason*, rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:2778:14\n    https://github.com/facebook/rocksdb/issues/17 0x5583e27d14e2 in rocksdb::DBImpl::BackgroundCallFlush(rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:2817:16\n    https://github.com/facebook/rocksdb/issues/18 0x5583e323d353 in std::__2::__function::__policy_func<void ()>::operator()[abi:ne180100]() const /root/build/3rdParty/llvm/runtimes/include/c++/v1/__functional/function.h:714:12\n    https://github.com/facebook/rocksdb/issues/19 0x5583e323d353 in std::__2::function<void ()>::operator()() const /root/build/3rdParty/llvm/runtimes/include/c++/v1/__functional/function.h:981:10\n    https://github.com/facebook/rocksdb/issues/20 0x5583e323d353 in rocksdb::ThreadPoolImpl::Impl::BGThread(unsigned long) /rocksdb/util/threadpool_imp.cc:266:5\n    https://github.com/facebook/rocksdb/issues/21 0x5583e3243d18 in decltype(std::declval<void (*)(void*)>()(std::declval<rocksdb::BGThreadMetadata*>())) std::__2::__invoke[abi:ne180100]<void (*)(void*), rocksdb::BGThreadMetadata*>(void (*&&)(void*), rocksdb::BGThreadMetadata*&&) /root/build/3rdParty/llvm/runtimes/include/c++/v1/__type_traits/invoke.h:344:25\n    https://github.com/facebook/rocksdb/issues/22 0x5583e3243d18 in void std::__2::__thread_execute[abi:ne180100]<std::__2::unique_ptr<std::__2::__thread_struct, std::__2::default_delete<std::__2::__thread_struct>>, void (*)(void*), rocksdb::BGThreadMetadata*, 2ul>(std::__2::tuple<std::__2::unique_ptr<std::__2::__thread_struct, std::__2::default_delete<std::__2::__thread_struct>>, void (*)(void*), rocksdb::BGThreadMetadata*>&, std::__2::__tuple_indices<2ul>) /root/build/3rdParty/llvm/runtimes/include/c++/v1/__thread/thread.h:193:3\n    https://github.com/facebook/rocksdb/issues/23 0x5583e3243d18 in void* std::__2::__thread_proxy[abi:ne180100]<std::__2::tuple<std::__2::unique_ptr<std::__2::__thread_struct, std::__2::default_delete<std::__2::__thread_struct>>, void (*)(void*), rocksdb::BGThreadMetadata*>>(void*) /root/build/3rdParty/llvm/runtimes/include/c++/v1/__thread/thread.h:202:3\n    https://github.com/facebook/rocksdb/issues/24 0x5583da5e819e in asan_thread_start(void*) crtstuff.c\n    https://github.com/facebook/rocksdb/issues/25 0x7f0eda362a93 in start_thread nptl/pthread_create.c:447:8\n    https://github.com/facebook/rocksdb/issues/26 0x7f0eda3efc3b in clone3 misc/../sysdeps/unix/sysv/linux/x86_64/clone3.S:78\n\n0x506000682221 is located 1 bytes inside of 56-byte region [0x506000682220,0x506000682258)\nallocated by thread T29 here:\n    #0 0x5583da6281d1 in operator new(unsigned long)\n    https://github.com/facebook/rocksdb/issues/1 0x5583da6c987d in __libcpp_operator_new<unsigned long> /root/build/3rdParty/llvm/runtimes/include/c++/v1/new:271:10\n    https://github.com/facebook/rocksdb/issues/2 0x5583da6c987d in __libcpp_allocate /root/build/3rdParty/llvm/runtimes/include/c++/v1/new:295:10\n    https://github.com/facebook/rocksdb/issues/3 0x5583da6c987d in allocate /root/build/3rdParty/llvm/runtimes/include/c++/v1/__memory/allocator.h:125:32\n    https://github.com/facebook/rocksdb/issues/4 0x5583da6c987d in allocate_at_least /root/build/3rdParty/llvm/runtimes/include/c++/v1/__memory/allocator.h:131:13\n    https://github.com/facebook/rocksdb/issues/5 0x5583da6c987d in allocate_at_least<std::__2::allocator<char> > /root/build/3rdParty/llvm/runtimes/include/c++/v1/__memory/allocate_at_least.h:34:20\n    https://github.com/facebook/rocksdb/issues/6 0x5583da6c987d in __allocate_at_least<std::__2::allocator<char> > /root/build/3rdParty/llvm/runtimes/include/c++/v1/__memory/allocate_at_least.h:42:10\n    https://github.com/facebook/rocksdb/issues/7 0x5583da6c987d in std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>>::__shrink_or_extend[abi:ne180100](unsigned long) /root/build/3rdParty/llvm/runtimes/include/c++/v1/string:3236:27\n    https://github.com/facebook/rocksdb/issues/8 0x5583e307c5aa in std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>>::reserve(unsigned long) /root/build/3rdParty/llvm/runtimes/include/c++/v1/string:3207:3\n    https://github.com/facebook/rocksdb/issues/9 0x5583e307c5aa in rocksdb::ReadFooterFromFile(rocksdb::IOOptions const&, rocksdb::RandomAccessFileReader*, rocksdb::FilePrefetchBuffer*, unsigned long, rocksdb::Footer*, unsigned long) /rocksdb/table/format.cc:382:18\n    https://github.com/facebook/rocksdb/issues/10 0x5583e2f88456 in rocksdb::BlockBasedTable::Open(rocksdb::ReadOptions const&, rocksdb::ImmutableOptions const&, rocksdb::EnvOptions const&, rocksdb::BlockBasedTableOptions const&, rocksdb::InternalKeyComparator const&, std::__2::unique_ptr<rocksdb::RandomAccessFileReader, std::__2::default_delete<rocksdb::RandomAccessFileReader>>&&, unsigned long, std::__2::unique_ptr<rocksdb::TableReader, std::__2::default_delete<rocksdb::TableReader>>*, std::__2::shared_ptr<rocksdb::CacheReservationManager>, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, bool, bool, int, bool, unsigned long, bool, rocksdb::TailPrefetchStats*, rocksdb::BlockCacheTracer*, unsigned long, std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>> const&, unsigned long) /rocksdb/table/block_based/block_based_table_reader.cc:610:9\n    https://github.com/facebook/rocksdb/issues/11 0x5583e2ef7837 in rocksdb::BlockBasedTableFactory::NewTableReader(rocksdb::ReadOptions const&, rocksdb::TableReaderOptions const&, std::__2::unique_ptr<rocksdb::RandomAccessFileReader, std::__2::default_delete<rocksdb::RandomAccessFileReader>>&&, unsigned long, std::__2::unique_ptr<rocksdb::TableReader, std::__2::default_delete<rocksdb::TableReader>>*, bool) const /rocksdb/table/block_based/block_based_table_factory.cc:599:10\n    https://github.com/facebook/rocksdb/issues/12 0x5583e2ab873c in rocksdb::TableCache::GetTableReader(rocksdb::ReadOptions const&, rocksdb::FileOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, bool, bool, rocksdb::HistogramImpl*, std::__2::unique_ptr<rocksdb::TableReader, std::__2::default_delete<rocksdb::TableReader>>*, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, bool, int, bool, unsigned long, rocksdb::Temperature) /rocksdb/db/table_cache.cc:142:34\n    https://github.com/facebook/rocksdb/issues/13 0x5583e2aba5f6 in rocksdb::TableCache::FindTable(rocksdb::ReadOptions const&, rocksdb::FileOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, rocksdb::Cache::Handle**, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, bool, bool, rocksdb::HistogramImpl*, bool, int, bool, unsigned long, rocksdb::Temperature) /rocksdb/db/table_cache.cc:190:16\n    https://github.com/facebook/rocksdb/issues/14 0x5583e2abb7e1 in rocksdb::TableCache::NewIterator(rocksdb::ReadOptions const&, rocksdb::FileOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileMetaData const&, rocksdb::RangeDelAggregator*, std::__2::shared_ptr<rocksdb::SliceTransform const> const&, rocksdb::TableReader**, rocksdb::HistogramImpl*, rocksdb::TableReaderCaller, rocksdb::Arena*, bool, int, unsigned long, rocksdb::InternalKey const*, rocksdb::InternalKey const*, bool) /rocksdb/db/table_cache.cc:235:9\n    https://github.com/facebook/rocksdb/issues/15 0x5583e28d14cf in rocksdb::BuildTable(std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>> const&, rocksdb::VersionSet*, rocksdb::ImmutableDBOptions const&, rocksdb::TableBuilderOptions const&, rocksdb::FileOptions const&, rocksdb::TableCache*, rocksdb::InternalIteratorBase<rocksdb::Slice>*, std::__2::vector<std::__2::unique_ptr<rocksdb::FragmentedRangeTombstoneIterator, std::__2::default_delete<rocksdb::FragmentedRangeTombstoneIterator>>, std::__2::allocator<std::__2::unique_ptr<rocksdb::FragmentedRangeTombstoneIterator, std::__2::default_delete<rocksdb::FragmentedRangeTombstoneIterator>>>>, rocksdb::FileMetaData*, std::__2::vector<rocksdb::BlobFileAddition, std::__2::allocator<rocksdb::BlobFileAddition>>*, std::__2::vector<unsigned long, std::__2::allocator<unsigned long>>, unsigned long, unsigned long, rocksdb::SnapshotChecker*, bool, rocksdb::InternalStats*, rocksdb::IOStatus*, std::__2::shared_ptr<rocksdb::IOTracer> const&, rocksdb::BlobFileCreationReason, rocksdb::EventLogger*, int, rocksdb::Env::IOPriority, rocksdb::TableProperties*, rocksdb::Env::WriteLifeTimeHint, std::__2::basic_string<char, std::__2::char_traits<char>, std::__2::allocator<char>> const*, rocksdb::BlobFileCompletionCallback*, unsigned long*, unsigned long*, unsigned long*) /rocksdb/db/builder.cc:335:57\n    https://github.com/facebook/rocksdb/issues/16 0x5583e29bf29d in rocksdb::FlushJob::WriteLevel0Table() /rocksdb/db/flush_job.cc:919:11\n    https://github.com/facebook/rocksdb/issues/17 0x5583e29b33ac in rocksdb::FlushJob::Run(rocksdb::LogsWithPrepTracker*, rocksdb::FileMetaData*, bool*) /rocksdb/db/flush_job.cc:276:9\n    https://github.com/facebook/rocksdb/issues/18 0x5583e27a4781 in rocksdb::DBImpl::FlushMemTableToOutputFile(rocksdb::ColumnFamilyData*, rocksdb::MutableCFOptions const&, bool*, rocksdb::JobContext*, rocksdb::SuperVersionContext*, std::__2::vector<unsigned long, std::__2::allocator<unsigned long>>&, unsigned long, rocksdb::SnapshotChecker*, rocksdb::LogBuffer*, rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:258:19\n    https://github.com/facebook/rocksdb/issues/19 0x5583e27a7a96 in rocksdb::DBImpl::FlushMemTablesToOutputFiles(rocksdb::autovector<rocksdb::DBImpl::BGFlushArg, 8ul> const&, bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:377:14\n    https://github.com/facebook/rocksdb/issues/20 0x5583e27d6777 in rocksdb::DBImpl::BackgroundFlush(bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::FlushReason*, rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:2778:14\n    https://github.com/facebook/rocksdb/issues/21 0x5583e27d14e2 in rocksdb::DBImpl::BackgroundCallFlush(rocksdb::Env::Priority) /rocksdb/db/db_impl/db_impl_compaction_flush.cc:2817:16\n    https://github.com/facebook/rocksdb/issues/22 0x5583e323d353 in std::__2::__function::__policy_func<void ()>::operator()[abi:ne180100]() const /root/build/3rdParty/llvm/runtimes/include/c++/v1/__functional/function.h:714:12\n    https://github.com/facebook/rocksdb/issues/23 0x5583e323d353 in std::__2::function<void ()>::operator()() const /root/build/3rdParty/llvm/runtimes/include/c++/v1/__functional/function.h:981:10\n    https://github.com/facebook/rocksdb/issues/24 0x5583e323d353 in rocksdb::ThreadPoolImpl::Impl::BGThread(unsigned long) /rocksdb/util/threadpool_imp.cc:266:5\n    https://github.com/facebook/rocksdb/issues/25 0x5583e3243d18 in decltype(std::declval<void (*)(void*)>()(std::declval<rocksdb::BGThreadMetadata*>())) std::__2::__invoke[abi:ne180100]<void (*)(void*), rocksdb::BGThreadMetadata*>(void (*&&)(void*), rocksdb::BGThreadMetadata*&&) /root/build/3rdParty/llvm/runtimes/include/c++/v1/__type_traits/invoke.h:344:25\n    https://github.com/facebook/rocksdb/issues/26 0x5583e3243d18 in void std::__2::__thread_execute[abi:ne180100]<std::__2::unique_ptr<std::__2::__thread_struct, std::__2::default_delete<std::__2::__thread_struct>>, void (*)(void*), rocksdb::BGThreadMetadata*, 2ul>(std::__2::tuple<std::__2::unique_ptr<std::__2::__thread_struct, std::__2::default_delete<std::__2::__thread_struct>>, void (*)(void*), rocksdb::BGThreadMetadata*>&, std::__2::__tuple_indices<2ul>) /root/build/3rdParty/llvm/runtimes/include/c++/v1/__thread/thread.h:193:3\n    https://github.com/facebook/rocksdb/issues/27 0x5583e3243d18 in void* std::__2::__thread_proxy[abi:ne180100]<std::__2::tuple<std::__2::unique_ptr<std::__2::__thread_struct, std::__2::default_delete<std::__2::__thread_struct>>, void (*)(void*), rocksdb::BGThreadMetadata*>>(void*) /root/build/3rdParty/llvm/runtimes/include/c++/v1/__thread/thread.h:202:3\n    https://github.com/facebook/rocksdb/issues/28 0x5583da5e819e in asan_thread_start(void*) crtstuff.c\n\nHINT: if you don't care about these errors you may set ASAN_OPTIONS=detect_container_overflow=0.\nIf you suspect a false positive see also: https://github.com/google/sanitizers/wiki/AddressSanitizerContainerOverflow.\n AddressSanitizer:container-overflow in pread\nShadow bytes around the buggy address:\n  0x506000681f80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x506000682000: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x506000682080: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x506000682100: fa fa fa fa fa fa fa fa fa fa fa fa 00 00 00 00\n  0x506000682180: 00 00 00 fa fa fa fa fa fa fa fa fa fa fa fa fa\n=>0x506000682200: fa fa fa fa[01]fc fc fc fc fc fc fa fa fa fa fa\n  0x506000682280: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x506000682300: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 00 01\n  0x506000682380: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa fa\n  0x506000682400: fd fd fd fd fd fd fd fa fa fa fa fa fd fd fd fd\n  0x506000682480: fd fd fd fd fa fa fa fa fd fd fd fd fd fd fd fd\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12722\n\nReviewed By: hx235\n\nDifferential Revision: D58118264\n\nPulled By: ajkr\n\nfbshipit-source-id: 0dd914c886c022d82697b769d664ba52de0770de",
        "modified_files_count": 1,
        "modified_files": [
            "table/format.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a8a52e5b4d938c1aea28d7a96ec70a459ffaa1a4",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e4428b7eb9b32d85495186757a13cc782f505535",
        "author": "Andrii Lysenko",
        "date": "2024-06-03T11:37:35-07:00",
        "message": "More details for 'tail prefetch size is calculated based on' (#12667)\n\nSummary:\nThese messages indicate that SST file was created by a pre-9.0.0 RocksDB. Eventually, `TailPrefetchStats` might be removed, so it would be more informative if log message also included name of the affected SST file.\n\nIssue: https://github.com/facebook/rocksdb/issues/12664\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12667\n\nReviewed By: ajkr\n\nDifferential Revision: D57464025\n\nPulled By: hx235\n\nfbshipit-source-id: 12f2f2635e3092f8c29362aa132462492b5c1417",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e4428b7eb9b32d85495186757a13cc782f505535",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3fdc7243f3c72ac191a56cfc16b4262b86c3fcb7",
        "author": "Patrik Valo",
        "date": "2024-05-06T08:53:06-07:00",
        "message": "Fix truncating last character in the StderrLogger (#12620)\n\nSummary:\nThis PR fixes a bug in the StderrLogger that truncated the last character in the logline. The problem was that we provided an incorrect max size parameter into the vsnprintf function. The size didn't take into account the null byte that the function automatically adds.\n\nBefore fix\n```\n** File Read Latency Histogram By Level [default] **\n2024/05/04-18:50:24.209304 4788 [/db_impl/db_impl.cc:498] Shutdown: canceling all background wor\n2024/05/04-18:50:24.209598 4788 [/db_impl/db_impl.cc:692] Shutdown complet\n```\n\nAfter fix\n```\n** File Read Latency Histogram By Level [default] **\n\n2024/05/04-18:51:19.814584 4d4d [/db_impl/db_impl.cc:498] Shutdown: canceling all background work\n2024/05/04-18:51:19.815528 4d4d [/db_impl/db_impl.cc:692] Shutdown complete\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12620\n\nTest Plan:\ntested on examples/simple_example.cc with StderrLogger\nFixes: https://github.com/facebook/rocksdb/issues/12576\n\nReviewed By: jaykorean\n\nDifferential Revision: D56972332\n\nPulled By: ajkr\n\nfbshipit-source-id: 70405e8231ae6e90d24fe0b351bc8e749176bd15",
        "modified_files_count": 1,
        "modified_files": [
            "util/stderr_logger.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3fdc7243f3c72ac191a56cfc16b4262b86c3fcb7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ef38d99edcf6cbcdb05cc265ee5bc58404986ba5",
        "author": "Levi Tamasi",
        "date": "2024-04-18T14:26:58-07:00",
        "message": "Sanity check the keys parameter in MultiGetEntityFromBatchAndDB (#12564)\n\nSummary:\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12564\n\nSimilarly to how `db`, `column_family`, and `results` are handled, bail out early from `WriteBatchWithIndex::MultiGetEntityFromBatchAndDB` if `keys` is `nullptr`. Note that these checks are best effort in the sense that with the current method signature, the callee has no way of reporting an error if `statuses` is `nullptr` or catching other types of invalid pointers (e.g. when `keys` and/or `results` is non-`nullptr` but do not point to a contiguous range of `num_keys` objects). We can improve this (and many similar RocksDB APIs) using `std::span` in a major release once we move to C++20.\n\nReviewed By: jaykorean\n\nDifferential Revision: D56318179\n\nfbshipit-source-id: bc7a258eda82b5f6c839f212ab824130e773a4f0",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/write_batch_with_index/write_batch_with_index.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ef38d99edcf6cbcdb05cc265ee5bc58404986ba5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2207a66fe5f96ed2eb7f3579f225dab92407713c",
        "author": "Yu Zhang",
        "date": "2024-04-04T12:33:05-07:00",
        "message": "Make autovector call default constructor explicitly before move/copy (#12499)\n\nSummary:\nMake `autovector` constructs the stack based element in place before move or copy another `autovector`'s stack based elements. This is already done in  the move/copy version of `autovector::push_back` when adding item to the stack based memory\nhttps://github.com/facebook/rocksdb/blob/8e6e8957fbb90992d1ac0c9996c70998751bd621/util/autovector.h#L269-L285\n\nThe ` values_ = reinterpret_cast<pointer>(buf_);` statement is not sufficient to ensure the class's member variables are properly constructed. I'm able to reproduce this consistently in a unit test in this change: https://github.com/facebook/rocksdb/compare/main...jowlyzhang:fix_sv_install with unit test:\n`./tiered_compaction_test --gtest_filter=\"\\*FastTrack\\*\"\n\nWith below stack trace P1203997597 showing the `std::string` copy destination is invalid, which indicates the object in the destination `autovector` is not constructed properly.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12499\n\nTest Plan: Existing unit tests.\n\nReviewed By: anand1976\n\nDifferential Revision: D55662354\n\nPulled By: jowlyzhang\n\nfbshipit-source-id: 581ceb11155d3dd711998607ec6950c0e327556a",
        "modified_files_count": 1,
        "modified_files": [
            "util/autovector.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2207a66fe5f96ed2eb7f3579f225dab92407713c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2443ebf8106f8e29218c7260958431f21299961a",
        "author": "Hui Xiao",
        "date": "2024-03-18T12:27:49-07:00",
        "message": "Don't write to WAL after previous WAL write error (#12448)\n\nSummary:\n**Context/Summary:**\nWAL write can continue onto the the WAL file that has encountered error and thus crash at https://github.com/facebook/rocksdb/blob/3f5bd46a07843e2117deb373008e63c38a393648/file/writable_file_writer.cc#L67 in below scenario:\n<img width=\"698\" alt=\"Screenshot 2024-03-15 at 1 52 45 PM\" src=\"https://github.com/facebook/rocksdb/assets/83968999/cd631ef2-c87c-4926-91ab-a0dc10f1eb14\">\n\nNote that GetLiveFilesStorageInfo() can happen concurrently with PUT() for the non-WAL-write part where db lock isn't held\n\nThis PR added an error check in LogWriter layer to prevent thread 2 from starting to write WAL after thread 1's write error.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12448\n\nTest Plan:\nStep 1 Apply the patch below to simulate frequent WAL write error for the purpose of repro\n```\n diff --git a/db_stress_tool/db_stress_driver.cc b/db_stress_tool/db_stress_driver.cc\nindex b47fa89e6..31930e976 100644\n --- a/db_stress_tool/db_stress_driver.cc\n+++ b/db_stress_tool/db_stress_driver.cc\n@@ -98,7 +98,7 @@ bool RunStressTestImpl(SharedState* shared) {\n     //  MANIFEST, CURRENT, and WAL files.\n     fault_fs_guard->SetRandomWriteError(\n         shared->GetSeed(), FLAGS_write_fault_one_in, error_msg,\n-        /*inject_for_all_file_types=*/false, {FileType::kTableFile});\n+        /*inject_for_all_file_types=*/false, {FileType::kWalFile});\n     fault_fs_guard->SetFilesystemDirectWritable(false);\n     fault_fs_guard->EnableWriteErrorInjection();\n   }\n\n diff --git a/utilities/fault_injection_fs.cc b/utilities/fault_injection_fs.cc\nindex 0ffb43ea6..589912cf4 100644\n --- a/utilities/fault_injection_fs.cc\n+++ b/utilities/fault_injection_fs.cc\n@@ -1042,7 +1042,7 @@ IOStatus FaultInjectionTestFS::InjectWriteError(const std::string& file_name) {\n   }\n\n   if (allowed_type) {\n-    if (write_error_rand_.OneIn(write_error_one_in_)) {\n+    if (write_error_rand_.OneIn(1)) {\n       return GetError();\n     }\n   }\n```\nStep 2 Run below\n```\n./db_stress --WAL_size_limit_MB=1 --WAL_ttl_seconds=60 --acquire_snapshot_one_in=100 --adaptive_readahead=1 --advise_random_on_open=1 --allow_concurrent_memtable_write=1 --allow_data_in_errors=True --allow_fallocate=1 --async_io=1 --auto_readahead_size=0 --avoid_flush_during_recovery=0 --avoid_flush_during_shutdown=0 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=1000 --batch_protection_bytes_per_key=8 --bgerror_resume_retry_interval=1000000 --block_protection_bytes_per_key=8 --block_size=16384 --bloom_before_level=2147483646 --bloom_bits=41.19540459544058 --bottommost_compression_type=disable --bottommost_file_compaction_delay=3600 --bytes_per_sync=0 --cache_index_and_filter_blocks=1 --cache_index_and_filter_blocks_with_high_priority=1 --cache_size=33554432 --cache_type=fixed_hyper_clock_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=0 --charge_filter_construction=0 --charge_table_reader=1 --checkpoint_one_in=1000000 --checksum_type=kCRC32c --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000000 --compact_range_one_in=1000 --compaction_pri=0 --compaction_readahead_size=1048576 --compaction_ttl=0 --compress_format_version=1 --compressed_secondary_cache_size=8388608 --compression_checksum=1 --compression_max_dict_buffer_bytes=68719476735 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=zlib --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=0 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_whitebox --db_write_buffer_size=1048576 --delete_obsolete_files_period_micros=30000000 --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_wal=0 --dump_malloc_stats=0 --enable_checksum_handoff=1 --enable_compaction_filter=0 --enable_index_compression=1 --enable_pipelined_write=1 --enable_thread_tracking=1 --enable_write_thread_adaptive_yield=0 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=0 --fifo_allow_compaction=1 --file_checksum_impl=big --fill_cache=1 --flush_one_in=1000 --format_version=6 --get_current_wal_file_one_in=0 --get_live_files_one_in=10000 --get_property_one_in=100000 --get_sorted_wal_files_one_in=0 --hard_pending_compaction_bytes_limit=274877906944 --high_pri_pool_ratio=0.5 --index_block_restart_interval=15 --index_shortening=2 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=524288 --iterpercent=10 --key_len_percent_dist=1,30,69 --kill_random_test=888887 --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=10000 --log2_keys_per_lock=10 --log_file_time_to_roll=0 --log_readahead_size=16777216 --long_running_snapshots=0 --low_pri_pool_ratio=0.5 --manifest_preallocation_size=5120 --manual_wal_flush_one_in=1000 --mark_for_compaction_one_file_in=0 --max_auto_readahead_size=524288 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=100000 --max_key_len=3 --max_log_file_size=1048576 --max_manifest_file_size=1073741824 --max_total_wal_size=0 --max_write_batch_group_size_bytes=64 --max_write_buffer_number=10 --max_write_buffer_size_to_maintain=1048576 --memtable_insert_hint_per_batch=1 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.5 --memtable_protection_bytes_per_key=8 --memtable_whole_key_filtering=0 --memtablerep=skip_list --metadata_charge_policy=0 --min_write_buffer_number_to_merge=2 --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --num_file_reads_for_auto_readahead=2 --open_files=500000 --open_metadata_write_fault_one_in=8 --open_read_fault_one_in=32 --open_write_fault_one_in=0 --ops_per_thread=20000000 --optimize_filters_for_hits=1 --optimize_filters_for_memory=1 --optimize_multiget_for_io=0 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=3 --pause_background_one_in=10000 --periodic_compaction_seconds=0 --prefix_size=5 --prefixpercent=5 --prepopulate_block_cache=0 --preserve_internal_time_seconds=3600 --progress_reports=0 --read_amp_bytes_per_bit=0 --read_fault_one_in=1000 --readahead_size=16384 --readpercent=45 --recycle_log_file_num=0 --reopen=20 --report_bg_io_stats=0 --sample_for_compression=5 --secondary_cache_fault_one_in=32 --secondary_cache_uri= --skip_stats_update_on_db_open=1 --snapshot_hold_ops=100000 --soft_pending_compaction_bytes_limit=68719476736 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=600 --stats_history_buffer_size=0 --strict_bytes_per_sync=0 --subcompactions=4 --sync=0 --sync_fault_injection=1 --table_cache_numshardbits=-1 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=1 --unpartitioned_pinning=1 --use_adaptive_mutex=1 --use_adaptive_mutex_lru=1 --use_delta_encoding=1 --use_direct_io_for_flush_and_compaction=1 --use_direct_reads=1 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=1 --use_multi_get_entity=0 --use_multiget=0 --use_put_entity_one_in=1 --use_write_buffer_manager=1 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_compression=1 --verify_db_one_in=100000 --verify_file_checksums_one_in=1000000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=33554432 --write_dbid_to_manifest=1 --write_fault_one_in=1000 --writepercent=35\n```\nPre-PR:\n```\ndb_stress: ./file/writable_file_writer.h:309: rocksdb::IOStatus rocksdb::WritableFileWriter::AssertFalseAndGetStatusForPrevError(): Assertion `sync_without_flush_called_' failed.\n```\nPost-PR\n```\n2024/03/15-13:44:08  Starting database operations\nput or merge error: IO error: Retryable injected write error\n```\n\nNote: The patch is NOT included in the PR as we first need to figure out how to handle this type of failed write in stress test (planned for the near future). It's sufficient to show the stress test does not crash as pre-PR for the purpose of this PR.\n\nReviewed By: ajkr\n\nDifferential Revision: D54969287\n\nPulled By: hx235\n\nfbshipit-source-id: 0ba4eabfec44ea7656d4d7117836f388897562f2",
        "modified_files_count": 1,
        "modified_files": [
            "db/log_writer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2443ebf8106f8e29218c7260958431f21299961a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fa4978c5660fecd9a29aea9edee4e89769ea2118",
        "author": "Hui Xiao",
        "date": "2024-03-14T14:50:56-07:00",
        "message": "Re-suppress tolerable manual compaction stress test failures (#12437)\n\nSummary:\n**Context/Summary:**\nPreviously manual compaction stress test failures won't terminate stress test. https://github.com/facebook/rocksdb/pull/12414 made more manual compaction failures terminate the stress test for signal boosting. A downside to that PR: some tolerable manual compaction stress test failures also unnecessarily terminate stress test.\n\nIdeally we should exclude exactly those tolerable errors (left as a TODO) from being able to terminate. For now we approximate those errors by Aborted(), InvalidArgument(), NotSupported() etc. It's still an improvement to pre-https://github.com/facebook/rocksdb/pull/12414 situation.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12437\n\nTest Plan: - No more tolerable manual compaction stress test failures terminating stress test.\n\nReviewed By: cbi42\n\nDifferential Revision: D54913010\n\nPulled By: hx235\n\nfbshipit-source-id: c43fa79d8f9c1c8b4f8786f8f46508b0ad619a9e",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_test_base.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fa4978c5660fecd9a29aea9edee4e89769ea2118",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1fa5dff7d1ce7be64555e7bdc8371be562b3eac6",
        "author": "\u594f\u4e4b\u7ae0",
        "date": "2024-02-27T15:23:54-08:00",
        "message": "WriteThread::EnterAsBatchGroupLeader reorder writers (#12138)\n\nSummary:\nReorder writers list to allow a leader can take as more commits as possible to maximize the throughput of the system and reduce IOPS.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12138\n\nReviewed By: hx235\n\nDifferential Revision: D53955592\n\nPulled By: ajkr\n\nfbshipit-source-id: 4d899d038faef691b63801d9d85f5cc079b7bbb5",
        "modified_files_count": 1,
        "modified_files": [
            "db/write_thread.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1fa5dff7d1ce7be64555e7bdc8371be562b3eac6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2233a2f4c0161d13ed4831a7a0a45e3a0fd0f740",
        "author": "Changyu Bi",
        "date": "2024-01-26T09:12:07-08:00",
        "message": "Enhance corruption status message for record mismatch in compaction (#12297)\n\nSummary:\n... to include the actual numbers of processed and expected records, and the file number for input files. The purpose is to be able to find the offending files even when the relevant LOG file is gone.\n\nAnother change is to check the record count even when `compaction_verify_record_count` is false, and log a warning message without setting corruption status if there is a mismatch. This is consistent with how we check the record count for flush.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12297\n\nTest Plan:\nprint the status message in `DBCompactionTest.VerifyRecordCount`\n```\nbefore\nCorruption: Compaction number of input keys does not match number of keys processed.\nafter\nCompaction number of input keys does not match number of keys processed. Expected 20 but processed 10. Compaction summary: Base version 4 Base level 0, inputs: [11(2156B) 9(2156B)]\n```\n\nReviewed By: ajkr\n\nDifferential Revision: D53110130\n\nPulled By: cbi42\n\nfbshipit-source-id: 6325cbfb8f71f25ce37f23f8277ebe9264863c3b",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction/compaction_job.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2233a2f4c0161d13ed4831a7a0a45e3a0fd0f740",
        "contains_optimization_keyword": true
    },
    {
        "hash": "438fc3d9b7fda27dc6099c5263a6b3cf35e611af",
        "author": "Hui Xiao",
        "date": "2024-01-25T09:58:25-08:00",
        "message": "No consistency check when compaction filter is enabled in stress test (#12291)\n\nSummary:\n**Context/Summary:**\n[Consistency check between Multiget and Get](https://github.com/facebook/rocksdb/blob/d82d179a5edc57e7de395e5db6f224d53e87c0cd/db_stress_tool/no_batched_ops_stress.cc#L585-L591) requires snapshot to be repeatable, that is, no keys being protected by this snapshot is deleted. However compaction filter can remove keys under snapshot, e,g,[DBStressCompactionFilter](https://github.com/facebook/rocksdb/blob/d82d179a5edc57e7de395e5db6f224d53e87c0cd/db_stress_tool/db_stress_compaction_filter.h#L59), which makes consistency check fail meaninglessly. This is noted in https://github.com/facebook/rocksdb/wiki/Compaction-Filter - \"Since release 6.0, with compaction filter enabled, RocksDB always invoke filtering for any key, even if it knows it will make a snapshot not repeatable.\"\n\nThis PR makes consistency check happens only when compaction filter is not enabled in stress test\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12291\n\nTest Plan:\n- Make the stress test command that fails on consistency check pass\n```\n ./db_stress --preserve_unverified_changes=1 --acquire_snapshot_one_in=0 --adaptive_readahead=0 --allow_concurrent_memtable_write=0 --allow_data_in_errors=True --async_io=0 --auto_readahead_size=0 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=0 --batch_protection_bytes_per_key=0 --block_protection_bytes_per_key=0 --block_size=16384 --bloom_before_level=2147483647 --bloom_bits=30.729729833325962 --bottommost_compression_type=disable --bottommost_file_compaction_delay=0 --bytes_per_sync=0 --cache_index_and_filter_blocks=1 --cache_size=33554432 --cache_type=lru_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=1 --charge_filter_construction=0 --charge_table_reader=1 --checkpoint_one_in=0 --checksum_type=kCRC32c --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=0 --compact_range_one_in=0 --compaction_pri=4 --compaction_readahead_size=0 --compaction_ttl=0 --compressed_secondary_cache_size=8388608 --compression_checksum=0 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=zlib --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=0 --db=$db --db_write_buffer_size=0 --delpercent=0 --delrangepercent=50 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_wal=0 --enable_compaction_filter=1 --disable_auto_compactions=1 --enable_pipelined_write=0 --enable_thread_tracking=0 --expected_values_dir=$expected --fail_if_options_file_error=1 --fifo_allow_compaction=0 --file_checksum_impl=xxh64 --flush_one_in=0 --format_version=5 --get_current_wal_file_one_in=0 --get_live_files_one_in=0 --get_property_one_in=0 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=3 --index_type=0 --ingest_external_file_one_in=100 --initial_auto_readahead_size=0 --iterpercent=0 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=0 --long_running_snapshots=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=0 --max_auto_readahead_size=0 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=1000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=64 --max_write_buffer_number=10 --max_write_buffer_size_to_maintain=2097152 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.01 --memtable_protection_bytes_per_key=8 --memtable_whole_key_filtering=0 --memtablerep=skip_list --min_write_buffer_number_to_merge=1 --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --num_file_reads_for_auto_readahead=1 --open_files=-1 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=100000 --optimize_filters_for_memory=0 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=2 --pause_background_one_in=0 --periodic_compaction_seconds=0 --prefix_size=8 --prefixpercent=0 --prepopulate_block_cache=0 --preserve_internal_time_seconds=0 --progress_reports=0 --read_fault_one_in=0 --readahead_size=0 --readpercent=45 --recycle_log_file_num=0 --reopen=0 --secondary_cache_fault_one_in=0 --secondary_cache_uri= --set_options_one_in=0 --snapshot_hold_ops=0 --sst_file_manager_bytes_per_sec=104857600 --sst_file_manager_bytes_per_truncate=1048576 --stats_dump_period_sec=0 --subcompactions=1 --sync=0 --sync_fault_injection=0 --target_file_size_base=167772 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=0 --unpartitioned_pinning=3 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=0 --use_multi_get_entity=0 --use_multiget=1 --use_put_entity_one_in=0 --use_write_buffer_manager=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=0 --verify_checksum_one_in=0 --verify_db_one_in=0 --verify_file_checksums_one_in=0 --verify_iterator_with_expected_state_one_in=0 --verify_sst_unique_id_in_manifest=0 --wal_bytes_per_sync=0 --wal_compression=none --write_buffer_size=335544 --write_dbid_to_manifest=1 --write_fault_one_in=0 --writepercent=5\n ```\n\nReviewed By: jaykorean, ajkr\n\nDifferential Revision: D53075223\n\nPulled By: hx235\n\nfbshipit-source-id: 61aa4a79de5d123a55eb5ac08b449a8362cc91ae",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/no_batched_ops_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/438fc3d9b7fda27dc6099c5263a6b3cf35e611af",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a29db3048fbc020f16700681d9d4742ba5540d79",
        "author": "Changyu Bi",
        "date": "2024-01-22T12:15:17-08:00",
        "message": "Fix TestGetEntity failure with UDT (#12264)\n\nSummary:\nUse the read option with right timestamp and skip verification when using old timestamps.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12264\n\nTest Plan:\nI can repro with small keyspace:\n```\n./db_stress --acquire_snapshot_one_in=10000 --adaptive_readahead=0 --allow_data_in_errors=True --async_io=0 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=1000 --batch_protection_bytes_per_key=0 --block_protection_bytes_per_key=4 --block_size=16384 --bloom_before_level=7 --bloom_bits=15 --bottommost_compression_type=xpress --bottommost_file_compaction_delay=0 --bytes_per_sync=262144 --cache_index_and_filter_blocks=1 --cache_size=33554432 --cache_type=fixed_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=0 --charge_filter_construction=1 --charge_table_reader=1 --checkpoint_one_in=10000 --checksum_type=kXXH3 --clear_column_family_one_in=0 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=4 --compaction_readahead_size=0 --compaction_style=1 --compaction_ttl=0 --compressed_secondary_cache_size=16777216 --compression_checksum=1 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=snappy --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=1 --db_write_buffer_size=0 --delpercent=4 --delrangepercent=1 --destroy_db_initially=1 --detect_filter_construct_corruption=1 --disable_wal=0 --enable_compaction_filter=0 --enable_pipelined_write=1 --enable_thread_tracking=0 --fail_if_options_file_error=1 --fifo_allow_compaction=1 --file_checksum_impl=big --flush_one_in=1000 --format_version=2 --get_current_wal_file_one_in=0 --get_live_files_one_in=10000 --get_property_one_in=100000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=13 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --iterpercent=10 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=0 --lock_wal_one_in=10000 --log2_keys_per_lock=10 --long_running_snapshots=0 --manual_wal_flush_one_in=1000 --mark_for_compaction_one_file_in=0 --max_auto_readahead_size=0 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=1000 --max_key_len=3 --max_manifest_file_size=16384 --max_write_batch_group_size_bytes=1048576 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=2097152 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.01 --memtable_protection_bytes_per_key=0 --memtable_whole_key_filtering=0 --memtablerep=skip_list --min_write_buffer_number_to_merge=2 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=1 --open_files=100 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=32 --open_write_fault_one_in=16 --ops_per_thread=200000 --optimize_filters_for_memory=0 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=2 --pause_background_one_in=1000000 --periodic_compaction_seconds=0 --persist_user_defined_timestamps=1 --prefix_size=8 --prefixpercent=5 --prepopulate_block_cache=1 --preserve_internal_time_seconds=3600 --progress_reports=0 --read_fault_one_in=0 --readahead_size=16384 --readpercent=45 --recycle_log_file_num=1 --reopen=20 --secondary_cache_fault_one_in=32 --secondary_cache_uri= --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=0 --subcompactions=3 --sync=0 --sync_fault_injection=0 --target_file_size_base=524288 --target_file_size_multiplier=2 --test_batches_snapshots=0 --test_cf_consistency=0 --top_level_index_pinning=1 --unpartitioned_pinning=1 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=1 --use_merge=0 --use_multi_get_entity=0 --use_multiget=1 --use_put_entity_one_in=0 --use_txn=0 --use_write_buffer_manager=0 --user_timestamp_size=8 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --verify_file_checksums_one_in=100000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=524288 --wal_compression=zstd --write_buffer_size=4194304 --write_dbid_to_manifest=0 --write_fault_one_in=0 --writepercent=35 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_whitebox --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected\n\nErrors when run with main:\nerror : inconsistent values for key 0x00000000000000E5000000000000012B000000000000014D: expected state has the key, GetEntity returns NotFound.\n\nerror : inconsistent values for key 0x0000000000000009000000000000012B0000000000000254: GetEntity returns :0x010000000504070609080B0A0D0C0F0E111013121514171619181B1A1D1C1F1E212023222524272629282B2A2D2C2F2E313033323534373639383B3A3D3C3F3E, expected state does not have the key.\n```\n\nReviewed By: jaykorean\n\nDifferential Revision: D52966251\n\nPulled By: cbi42\n\nfbshipit-source-id: 09436a1b747f1ac545140fc83a2fa4555fef51c1",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/no_batched_ops_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a29db3048fbc020f16700681d9d4742ba5540d79",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ec5b1be18dabb99acfc52872db36d9d163baea84",
        "author": "Changyu Bi",
        "date": "2024-01-19T10:13:52-08:00",
        "message": "Deflake `PerfContextTest.CPUTimer` (#12252)\n\nSummary:\nWe saw failures like\n```\ndb/perf_context_test.cc:952: Failure\nExpected: (next_count) > (count), actual: 26699 vs 26699\n```\nI can repro by running the test repeatedly and the test fails with different seek keys. So\nthe cause is likely not with Seek() implementation. I found that\n`clock_gettime(CLOCK_THREAD_CPUTIME_ID, &ts);` can return the same time when\ncalled repeatedly. However, I don't know if Seek() is fast enough that this happened during\ncontinuous test.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12252\n\nTest Plan: `gtest_parallel.py --repeat=10000 --workers=1 ./perf_context_test --gtest_filter=\"PerfContextTest.CPUTimer\"`\n\nReviewed By: ajkr\n\nDifferential Revision: D52912751\n\nPulled By: cbi42\n\nfbshipit-source-id: 8985ae93baa99cdf4b9136ea38addd2e41f4b202",
        "modified_files_count": 1,
        "modified_files": [
            "db/perf_context_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ec5b1be18dabb99acfc52872db36d9d163baea84",
        "contains_optimization_keyword": true
    },
    {
        "hash": "59ba1d200d657d328e91b18cc2a71cf7062a3745",
        "author": "Richard Barnes",
        "date": "2024-01-17T14:08:07-08:00",
        "message": "Remove unused variables in internal_repo_rocksdb/repo/env/env_posix.cc (#12243)\n\nSummary:\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12243\n\nLLVM-15 has a warning `-Wunused-but-set-variable` which we treat as an error because it's so often diagnostic of a code issue. Unused variables can compromise readability or, worse, performance.\n\nThis diff either (a) removes an unused variable and, possibly, it's associated code, or (b) qualifies the variable with `[[maybe_unused]]`, mostly in cases where the variable _is_ used, but, eg, in an `assert` statement that isn't present in production code.\n\n - If you approve of this diff, please use the \"Accept & Ship\" button :-)\n\nReviewed By: jowlyzhang\n\nDifferential Revision: D52847993\n\nfbshipit-source-id: 221da13c6ca9967e3b934f98f318a832a144df39",
        "modified_files_count": 1,
        "modified_files": [
            "env/env_posix.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/59ba1d200d657d328e91b18cc2a71cf7062a3745",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1de69409805e196dae4daad4975f6f83080f8a7c",
        "author": "akankshamahajan",
        "date": "2024-01-05T18:10:58-08:00",
        "message": "Fix heap use after free error in FilePrefetchBuffer (#12211)\n\nSummary:\nFix heap use after free error in FilePrefetchBuffer\nFix heap use after free error in FilePrefetchBuffer\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12211\n\nTest Plan:\nRan db_stress in ASAN mode\n```\n==652957==ERROR: AddressSanitizer: heap-use-after-free on address 0x6150006d8578 at pc 0x7f91f74ae85b bp 0x7f91c25f90c0 sp 0x7f91c25f90b8\nREAD of size 8 at 0x6150006d8578 thread T48\n    #0 0x7f91f74ae85a in void __gnu_cxx::new_allocator<rocksdb::BufferInfo*>::construct<rocksdb::BufferInfo*, rocksdb::BufferInfo*&>(rocksdb::BufferInfo**, rocksdb::BufferInfo*&) /mnt/gvfs/third-party2/libgcc/c00dcc6a3e4125c7e8b248e9a79c14b78ac9e0ca/11.x/platform010/5684a5a/include/c++/trunk/ext/new_allocator.h:163\n    https://github.com/facebook/rocksdb/issues/1 0x7f91f74ae85a in void std::allocator_traits<std::allocator<rocksdb::BufferInfo*> >::construct<rocksdb::BufferInfo*, rocksdb::BufferInfo*&>(std::allocator<rocksdb::BufferInfo*>&, rocksdb::BufferInfo**, rocksdb::BufferInfo*&) /mnt/gvfs/third-party2/libgcc/c00dcc6a3e4125c7e8b248e9a79c14b78ac9e0ca/11.x/platform010/5684a5a/include/c++/trunk/bits/alloc_traits.h:512\n    https://github.com/facebook/rocksdb/issues/2 0x7f91f74ae85a in rocksdb::BufferInfo*& std::deque<rocksdb::BufferInfo*, std::allocator<rocksdb::BufferInfo*> >::emplace_back<rocksdb::BufferInfo*&>(rocksdb::BufferInfo*&) /mnt/gvfs/third-party2/libgcc/c00dcc6a3e4125c7e8b248e9a79c14b78ac9e0ca/11.x/platform010/5684a5a/include/c++/trunk/bits/deque.tcc:170\n    https://github.com/facebook/rocksdb/issues/3 0x7f91f74b93d8 in rocksdb::FilePrefetchBuffer::FreeAllBuffers() file/file_prefetch_buffer.h:557\n```\n\nReviewed By: ajkr\n\nDifferential Revision: D52575217\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: 6811ec10a393f5a62fedaff0fab5fd6e823c2687",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1de69409805e196dae4daad4975f6f83080f8a7c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "81b6296c7e5a6a4a0241559d2de878677f160b38",
        "author": "Hui Xiao",
        "date": "2024-01-02T17:33:00-08:00",
        "message": "Pass flush IO activity enum in FlushJob::MaybeIncreaseFullHistoryTsLowToAboveCutoffUDT...() (#12197)\n\nSummary:\n**Context/Summary:** as titled\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12197\n\nTest Plan:\n```\n./db_stress --acquire_snapshot_one_in=100 --adaptive_readahead=0 --allow_concurrent_memtable_write=0 --allow_data_in_errors=True --async_io=1 --atomic_flush=0 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=0 --block_protection_bytes_per_key=0 --block_size=16384 --bloom_before_level=2147483647 --bloom_bits=4.393039399748979 --bottommost_compression_type=disable --bottommost_file_compaction_delay=86400 --bytes_per_sync=262144 --cache_index_and_filter_blocks=0 --cache_size=33554432 --cache_type=fixed_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=0 --charge_filter_construction=0 --charge_table_reader=1 --checkpoint_one_in=1000000 --checksum_type=kxxHash64 --clear_column_family_one_in=0 --compact_files_one_in=1000 --compact_range_one_in=1000 --compaction_pri=3 --compaction_readahead_size=1048576 --compaction_ttl=0 --compressed_secondary_cache_ratio=0.0 --compressed_secondary_cache_size=0 --compression_checksum=0 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=lz4hc --compression_use_zstd_dict_trainer=1 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=1 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_blackbox --db_write_buffer_size=0 --delpercent=5 --delrangepercent=0 --destroy_db_initially=0 --detect_filter_construct_corruption=0 --disable_wal=0 --enable_blob_files=0 --enable_compaction_filter=0 --enable_pipelined_write=0 --enable_thread_tracking=1 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=1 --fifo_allow_compaction=0 --file_checksum_impl=none --flush_one_in=1000 --format_version=6 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=100000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=13 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --iterpercent=0 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=10000 --long_running_snapshots=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=524288 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=100000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=64 --max_write_buffer_number=10 --max_write_buffer_size_to_maintain=8388608 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.1 --memtable_protection_bytes_per_key=2 --memtable_whole_key_filtering=1 --memtablerep=skip_list --min_write_buffer_number_to_merge=2 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=2 --open_files=100 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=16 --ops_per_thread=100000000 --optimize_filters_for_memory=0 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=2 --pause_background_one_in=10000 --periodic_compaction_seconds=0 --persist_user_defined_timestamps=0 --prefix_size=5 --prefixpercent=5 --prepopulate_block_cache=0 --preserve_internal_time_seconds=0 --progress_reports=0 --read_fault_one_in=0 --readahead_size=16384 --readpercent=55 --recycle_log_file_num=0 --reopen=0 --secondary_cache_fault_one_in=0 --set_options_one_in=10000 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=10 --subcompactions=1 --sync=0 --sync_fault_injection=0 --target_file_size_base=2097152 --target_file_size_multiplier=2 --test_batches_snapshots=0 --test_cf_consistency=0 --top_level_index_pinning=3 --unpartitioned_pinning=1 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=0 --use_multi_get_entity=0 --use_multiget=0 --use_put_entity_one_in=0 --use_txn=0 --use_write_buffer_manager=0 --user_timestamp_size=8 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=10000 --verify_file_checksums_one_in=0 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=524288 --wal_compression=zstd --write_buffer_size=1048576 --write_dbid_to_manifest=1 --write_fault_one_in=128 --writepercent=35\n```\n\nBefore fix:\n```\ndb_stress_tool/db_stress_env_wrapper.h:92: virtual rocksdb::IOStatus rocksdb::DbStressWritableFileWrapper::Append(const rocksdb::Slice &, const rocksdb::IOOptions &, rocksdb::IODebugContext *): Assertion `io_activity == Env::IOActivity::kUnknown || io_activity == options.io_activity' failed.\n```\n\nAfter fix:\nSucceed\n\nReviewed By: ajkr\n\nDifferential Revision: D52492030\n\nPulled By: hx235\n\nfbshipit-source-id: 842a0dcbdf135838b57ddb4a3a6f1effc8dd3e82",
        "modified_files_count": 1,
        "modified_files": [
            "db/flush_job.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/81b6296c7e5a6a4a0241559d2de878677f160b38",
        "contains_optimization_keyword": true
    },
    {
        "hash": "cd21e4e69d76ec4ec3b080c8cdae016ac2309cc5",
        "author": "Levi Tamasi",
        "date": "2023-12-13T17:34:18-08:00",
        "message": "Some further cleanup in WriteBatchWithIndex::MultiGetFromBatchAndDB (#12143)\n\nSummary:\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12143\n\nhttps://github.com/facebook/rocksdb/pull/11982 changed `WriteBatchWithIndex::MultiGetFromBatchDB` to preallocate space in the `autovector`s `key_contexts` and `merges` in order to prevent any reallocations, both as an optimization and in order to prevent pointers into the container from being invalidated during subsequent insertions. On second thought, this preallocation can actually be a pessimization in cases when only a small subset of keys require querying the underlying database. To prevent any memory regressions, the PR reverts this preallocation. In addition, it makes some small code hygiene improvements like incorporating the `PinnableWideColumns` object into `MergeTuple`.\n\nReviewed By: jaykorean\n\nDifferential Revision: D52136513\n\nfbshipit-source-id: 21aa835084433feab27b501d9d1fc5434acea609",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/write_batch_with_index/write_batch_with_index.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/cd21e4e69d76ec4ec3b080c8cdae016ac2309cc5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d68f45e777563018453c1506a94dc3a4f2cc7b82",
        "author": "Yu Zhang",
        "date": "2023-11-29T11:35:59-08:00",
        "message": "Flush buffered logs when FlushRequest is rescheduled (#12105)\n\nSummary:\nThe optimization to not find and delete obsolete files when FlushRequest is re-scheduled also inadvertently skipped flushing the `LogBuffer`, resulting in missed logs. This PR fixes the issue.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12105\n\nTest Plan:\nmanually check this test has the correct info log after the fix\n`./column_family_test --gtest_filter=ColumnFamilyRetainUDTTest.NotAllKeysExpiredFlushRescheduled`\n\nReviewed By: ajkr\n\nDifferential Revision: D51671079\n\nPulled By: jowlyzhang\n\nfbshipit-source-id: da0640e07e35c69c08988772ed611ec9e67f2e92",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl/db_impl_compaction_flush.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d68f45e777563018453c1506a94dc3a4f2cc7b82",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2818a74b95f0bab434dc65f0d271ac7a27c787a7",
        "author": "Changyu Bi",
        "date": "2023-10-31T07:39:41-07:00",
        "message": "Initialize merge operator explicitly in PrepareOptionsForRestoredDB() (#12033)\n\nSummary:\nWe are seeing the following stress test failure: `Failure in DB::Get in backup/restore with: Invalid argument: merge_operator is not properly initialized. Verification failed: Backup/restore failed: Invalid argument: merge_operator is not properly initialized.`. The reason is likely that `GetColumnFamilyOptionsFromString()` does not set merge operator if it's a customized merge operator. Fixing it by initializing merge operator explicitly.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/12033\n\nTest Plan:\nthis repro gives the error consistently before this PR\n```\n./db_stress --acquire_snapshot_one_in=10000 --adaptive_readahead=0 --allow_concurrent_memtable_write=1 --allow_data_in_errors=True --async_io=0 --atomic_flush=1 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=1 --backup_max_size=1048576000000 --backup_one_in=50 --batch_protection_bytes_per_key=8 --block_protection_bytes_per_key=2 --block_size=16384 --bloom_before_level=2147483646 --bloom_bits=31.014388066505518 --bottommost_compression_type=lz4hc --bottommost_file_compaction_delay=0 --bytes_per_sync=0 --cache_index_and_filter_blocks=0 --cache_size=33554432 --cache_type=fixed_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=1 --charge_filter_construction=0 --charge_table_reader=1 --checkpoint_one_in=1000000 --checksum_type=kxxHash --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=3 --compaction_readahead_size=0 --compaction_ttl=10 --compressed_secondary_cache_ratio=0.0 --compressed_secondary_cache_size=0 --compression_checksum=1 --compression_max_dict_buffer_bytes=4095 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=none --compression_use_zstd_dict_trainer=1 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=0 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_blackbox --db_write_buffer_size=0 --delpercent=4 --delrangepercent=1 --destroy_db_initially=1 --detect_filter_construct_corruption=0 --disable_wal=1 --enable_compaction_filter=0 --enable_pipelined_write=0 --enable_thread_tracking=0 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=1 --fifo_allow_compaction=0 --file_checksum_impl=xxh64 --flush_one_in=1000000 --format_version=2 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=10 --index_type=2 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --iterpercent=10 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=1000000 --long_running_snapshots=1 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=524288 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=100 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=1048576 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=8388608 --memtable_max_range_deletions=1000 --memtable_prefix_bloom_size_ratio=0 --memtable_protection_bytes_per_key=2 --memtable_whole_key_filtering=0 --memtablerep=skip_list --min_write_buffer_number_to_merge=2 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=0 --open_files=-1 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=16 --ops_per_thread=100000000 --optimize_filters_for_memory=1 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=0 --pause_background_one_in=1000000 --periodic_compaction_seconds=0 --prefix_size=-1 --prefixpercent=0 --prepopulate_block_cache=1 --preserve_internal_time_seconds=0 --progress_reports=0 --read_fault_one_in=0 --readahead_size=0 --readpercent=50 --recycle_log_file_num=0 --reopen=0 --secondary_cache_fault_one_in=0 --set_options_one_in=0 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=0 --subcompactions=1 --sync=0 --sync_fault_injection=1 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=0 --unpartitioned_pinning=1 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=1 --use_multi_get_entity=0 --use_multiget=1 --use_put_entity_one_in=10 --use_write_buffer_manager=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --verify_file_checksums_one_in=1000000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=524288 --wal_compression=zstd --write_buffer_size=33554432 --write_dbid_to_manifest=0 --write_fault_one_in=0 --writepercent=35\n```\n\nReviewed By: hx235\n\nDifferential Revision: D50825558\n\nPulled By: cbi42\n\nfbshipit-source-id: 8468dc0444c112415a515af8291ef3abec8a42de",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_test_base.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2818a74b95f0bab434dc65f0d271ac7a27c787a7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ef0c3f08fae318d1b1e4159e0437579a36c6cfd8",
        "author": "Peter Dillinger",
        "date": "2023-10-19T14:51:22-07:00",
        "message": "Fix rare destructor bug in AutoHCC (#11988)\n\nSummary:\nand some other small enhancements/fixes:\n* The main bug fixed is that in some rare cases, the \"published\" table size might be smaller than the actual table size. This is a transient state that can happen with concurrent growth that is normally fixed after enough insertions, but if the cache is destroyed soon enough after growth, it could fail to fully destroy some entries and cause assertion failures. We can fix this by detecting the true table size in the destructor.\n* Change the \"too many iterations\" debug threshold from 512 to 768. We might have hit at least one false positive failure. (Failed despite legitimate operation.)\n* Added some stronger assertions in some places to aid in debugging.\n* Use COERCE_CONTEXT_SWITCH to make behavior of Grow less predictable in terms of thread interleaving. (Might add in more places.) This was useful in reproducing the destructor bug.\n* Fix some comments with typos or that were based on earlier revisions of the code.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11988\n\nTest Plan:\nVariants of this bug-finding command:\n```\nUSE_CLANG=1 COMPILE_WITH_ASAN=1 COMPILE_WITH_UBSAN=1 COERCE_CONTEXT_SWITCH=1 DEBUG_LEVEL=2 make -j32 cache_bench && while ROCKSDB_DEBUG=1 ./cache_bench -cache_type=auto_hyper_clock_cache -histograms=0 -cache_size=80000000 -threads=32 -populate_cache=0 -ops_per_thread=1000 -num_shard_bits=0; do :; done\n```\n\nReviewed By: jowlyzhang\n\nDifferential Revision: D50470318\n\nPulled By: pdillinger\n\nfbshipit-source-id: d407a8bb0b6d2ddc598a954c319a1640136f12f2",
        "modified_files_count": 1,
        "modified_files": [
            "cache/clock_cache.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ef0c3f08fae318d1b1e4159e0437579a36c6cfd8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "018eede679f6df74f78b158caf21f76c00c1bad7",
        "author": "Akanksha Mahajan",
        "date": "2023-10-16T15:14:58-07:00",
        "message": "Remove assertion from PrefetchAsync (#11965)\n\nSummary:\nRemove assertion from PrefetchAsync (roundup_len2 >= alignment) as for non direct_io, buffer size can be less than alignment resulting in assertion.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11965\n\nTest Plan: Ran the issue causing db_stress without this assertion and the verification completes successfully.\n\nReviewed By: anand1976\n\nDifferential Revision: D50328955\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: 65f55ca230d2bbc63f4e2cc34c7273b22b515879",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/018eede679f6df74f78b158caf21f76c00c1bad7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "20dbf512473b570c62b83258c058eff448779601",
        "author": "Jay Huh",
        "date": "2023-09-18T23:04:38-07:00",
        "message": "DB Stress Fix - Commit pending value after checking for status (#11856)\n\nSummary:\nWe've seen occasional crash test failure in optimistic transaction DB with the following error message.\n\n```\nstderr:\nWARNING: prefix_size is non-zero but memtablerep != prefix_hash\nVerification failed for column family 0 key 0000000000001EDB0000000000000036787878787878 (789064): value_from_db: 010000000504070609080B0A0D0C0F0E111013121514171619181B1A1D1C1F1E212023222524272629282B2A2D2C2F2E313033323534373639383B3A3D3C3F3E, value_from_expected: , msg: MultiGet verification: Unexpected value found\nCrash-recovery verification failed :(\nNo writes or ops?\nVerification failed :(\n```\n\nThere was a possibility if the db stress crashes after `pending_expected_value.Commit()` but before `thread->shared->SafeTerminate();`, we may have expected value committed while actual DB value was not.\n\nMoving the `pending_expected_value.Commit()` after `s.ok()` check to fix the test.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11856\n\nTest Plan:\nRan the following in a script with while loop. (It doesn't always repro the issue even without this fix, though..)\n```\n./db_stress --acquire_snapshot_one_in=10000 --adaptive_readahead=0 --allow_data_in_errors=True --async_io=1 --atomic_flush=1 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=0 --block_protection_bytes_per_key=0 --block_size=16384 --bloom_before_level=8 --bloom_bits=11 --bottommost_compression_type=lz4 --bottommost_file_compaction_delay=0 --bytes_per_sync=0 --cache_index_and_filter_blocks=0 --cache_size=8388608 --cache_type=auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=0 --charge_filter_construction=0 --charge_table_reader=0 --checkpoint_one_in=1000000 --checksum_type=kxxHash --clear_column_family_one_in=0 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=4 --compaction_readahead_size=0 --compaction_ttl=100 --compression_checksum=0 --compression_max_dict_buffer_bytes=2097151 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=lz4 --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=0 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_blackbox --db_write_buffer_size=1048576 --delpercent=5 --delrangepercent=0 --destroy_db_initially=0 --detect_filter_construct_corruption=0 --disable_wal=1 --enable_compaction_filter=0 --enable_pipelined_write=0 --enable_thread_tracking=1 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=1 --fifo_allow_compaction=1 --file_checksum_impl=crc32c --flush_one_in=1000000 --format_version=5 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=3 --index_type=2 --ingest_external_file_one_in=0 --initial_auto_readahead_size=524288 --iterpercent=10 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=0 --lock_wal_one_in=1000000 --long_running_snapshots=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=524288 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=25000000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=16777216 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=2097152 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0.001 --memtable_protection_bytes_per_key=2 --memtable_whole_key_filtering=0 --memtablerep=skip_list --min_write_buffer_number_to_merge=1 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=1 --occ_lock_bucket_count=500 --occ_validation_policy=0 --open_files=-1 --open_metadata_write_fault_one_in=8 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=100000000 --optimize_filters_for_memory=1 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=1 --pause_background_one_in=1000000 --periodic_compaction_seconds=2 --prefix_size=5 --prefixpercent=5 --prepopulate_block_cache=1 --preserve_internal_time_seconds=36000 --progress_reports=0 --read_fault_one_in=0 --readahead_size=0 --readpercent=45 --recycle_log_file_num=1 --reopen=0 --secondary_cache_fault_one_in=0 --secondary_cache_uri=compressed_secondary_cache://capacity=8388608;enable_custom_split_merge=true --set_options_one_in=10000 --share_occ_lock_buckets=0 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=0 --subcompactions=3 --sync=0 --sync_fault_injection=0 --target_file_size_base=2097152 --target_file_size_multiplier=2 --test_batches_snapshots=0 --top_level_index_pinning=3 --unpartitioned_pinning=2 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=0 --use_multi_get_entity=0 --use_multiget=1 --use_optimistic_txn=1 --use_put_entity_one_in=0 --use_txn=1 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --verify_file_checksums_one_in=1000000 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=0 --wal_compression=zstd --write_buffer_size=4194304 --write_dbid_to_manifest=1 --write_fault_one_in=0 --writepercent=35&\n```\n```\npid=$!\nsleep 10\nkill -9 $pid\nsleep 1\n```\n```\ndb_stress --acquire_snapshot_one_in=10000 --adaptive_readahead=1 --allow_data_in_errors=True --async_io=0 --atomic_flush=1 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=8 --block_protection_bytes_per_key=0 --block_size=16384 --bloom_before_level=2147483647 --bloom_bits=75.01353068032098 --bottommost_compression_type=xpress --bottommost_file_compaction_delay=3600 --bytes_per_sync=262144 --cache_index_and_filter_blocks=0 --cache_size=8388608 --cache_type=auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=0 --charge_filter_construction=1 --charge_table_reader=0 --checkpoint_one_in=1000000 --checksum_type=kCRC32c --clear_column_family_one_in=0 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=4 --compaction_readahead_size=0 --compaction_ttl=0 --compression_checksum=0 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=xpress --compression_use_zstd_dict_trainer=0 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=1 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_blackbox --db_write_buffer_size=134217728 --delpercent=5 --delrangepercent=0 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_wal=1 --enable_compaction_filter=0 --enable_pipelined_write=0 --enable_thread_tracking=1 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=1 --fifo_allow_compaction=0 --file_checksum_impl=none --flush_one_in=1000000 --format_version=5 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=10 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=524288 --iterpercent=10 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=1 --lock_wal_one_in=1000000 --long_running_snapshots=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=524288 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=25000000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=1048576 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=1048576 --memtable_max_range_deletions=100 --memtable_prefix_bloom_size_ratio=0.01 --memtable_protection_bytes_per_key=8 --memtable_whole_key_filtering=0 --memtablerep=skip_list --min_write_buffer_number_to_merge=2 --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --num_file_reads_for_auto_readahead=2 --occ_lock_bucket_count=10 --occ_validation_policy=0 --open_files=-1 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=32 --open_write_fault_one_in=16 --ops_per_thread=100000000 --optimize_filters_for_memory=0 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=0 --pause_background_one_in=1000000 --periodic_compaction_seconds=0 --prefix_size=1 --prefixpercent=5 --prepopulate_block_cache=0 --preserve_internal_time_seconds=36000 --progress_reports=0 --read_fault_one_in=32 --readahead_size=0 --readpercent=45 --recycle_log_file_num=1 --reopen=0 --secondary_cache_fault_one_in=0 --secondary_cache_uri=compressed_secondary_cache://capacity=8388608;enable_custom_split_merge=true --set_options_one_in=10000 --share_occ_lock_buckets=0 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=104857600 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=0 --subcompactions=4 --sync=0 --sync_fault_injection=0 --target_file_size_base=2097152 --target_file_size_multiplier=2 --test_batches_snapshots=0 --top_level_index_pinning=1 --unpartitioned_pinning=1 --use_direct_io_for_flush_and_compaction=1 --use_direct_reads=0 --use_full_merge_v1=1 --use_get_entity=0 --use_merge=1 --use_multi_get_entity=0 --use_multiget=1 --use_optimistic_txn=1 --use_put_entity_one_in=0 --use_txn=1 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --verify_file_checksums_one_in=0 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=524288 --wal_compression=zstd --write_buffer_size=4194304 --write_dbid_to_manifest=1 --write_fault_one_in=0 --writepercent=35\n```\n\nReviewed By: cbi42\n\nDifferential Revision: D49403091\n\nPulled By: jaykorean\n\nfbshipit-source-id: 5ee6136133bbdc46aa733e5101c1f998f658c200",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/no_batched_ops_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/20dbf512473b570c62b83258c058eff448779601",
        "contains_optimization_keyword": true
    },
    {
        "hash": "99f8820054898d44e5cd5f67edd8c940ce1633bb",
        "author": "Jay Huh",
        "date": "2023-09-15T22:50:49-07:00",
        "message": "Fix test on IOActivity check for MultiGetEntity (#11850)\n\nSummary:\nAfter https://github.com/facebook/rocksdb/issues/11842  merged, we started to see some crash_test failures.\n\nThere is a flow inside `TestMultiGetEntity()` that it calls `GetEntity()` to compare the result between `MultiGetEntity()` and `GetEntity()` https://github.com/facebook/rocksdb/blob/1c6faf35871a236222bcbf0b69718ee43376a951/db_stress_tool/no_batched_ops_stress.cc#L1068-L1072\n\nHowever, IOActivity check inside DbStressRandomAccessFileWrapper was expecting IOActivity::MultiGet when GetEntity() was called. We are fixing the test by setting expected operation to be GetEntity before calling GetEntity()\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11850\n\nTest Plan:\nError repro'ed by the following run before fix and no more error after the fix.\n\n```\n./db_stress --acquire_snapshot_one_in=10000 --adaptive_readahead=0 --allow_concurrent_memtable_write=0 --allow_data_in_errors=True --async_io=1 --auto_readahead_size=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=8 --block_protection_bytes_per_key=0 --block_size=16384 --bloom_before_level=1 --bloom_bits=9.880688060667444 --bottommost_compression_type=zstd --bottommost_file_compaction_delay=86400 --bytes_per_sync=262144 --cache_index_and_filter_blocks=0 --cache_size=8388608 --cache_type=auto_hyper_clock_cache --charge_compression_dictionary_building_buffer=1 --charge_file_metadata=0 --charge_filter_construction=0 --charge_table_reader=1 --checkpoint_one_in=1000000 --checksum_type=kxxHash64 --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=3 --compaction_readahead_size=1048576 --compaction_ttl=0 --compression_checksum=0 --compression_max_dict_buffer_bytes=0 --compression_max_dict_bytes=0 --compression_parallel_threads=1 --compression_type=none --compression_use_zstd_dict_trainer=1 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=1 --db=/dev/shm/rocksdb_test/rocksdb_crashtest_blackbox --db_write_buffer_size=0 --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_wal=0 --enable_compaction_filter=0 --enable_pipelined_write=0 --enable_thread_tracking=1 --expected_values_dir=/dev/shm/rocksdb_test/rocksdb_crashtest_expected --fail_if_options_file_error=0 --fifo_allow_compaction=1 --file_checksum_impl=big --flush_one_in=1000000 --format_version=6 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=4 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=0 --iterpercent=10 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=0 --lock_wal_one_in=1000000 --long_running_snapshots=0 --manual_wal_flush_one_in=0 --mark_for_compaction_one_file_in=0 --max_auto_readahead_size=524288 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=25000000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=16777216 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=2097152 --memtable_max_range_deletions=0 --memtable_prefix_bloom_size_ratio=0 --memtable_protection_bytes_per_key=2 --memtable_whole_key_filtering=0 --memtablerep=skip_list --min_write_buffer_number_to_merge=2 --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --num_file_reads_for_auto_readahead=0 --open_files=-1 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=100000000 --optimize_filters_for_memory=0 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=3 --pause_background_one_in=1000000 --periodic_compaction_seconds=1 --prefix_size=-1 --prefixpercent=0 --prepopulate_block_cache=0 --preserve_internal_time_seconds=0 --progress_reports=0 --read_fault_one_in=32 --readahead_size=16384 --readpercent=50 --recycle_log_file_num=1 --reopen=0 --secondary_cache_fault_one_in=0 --secondary_cache_uri=compressed_secondary_cache://capacity=8388608;enable_custom_split_merge=true --set_options_one_in=0 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=104857600 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=0 --subcompactions=2 --sync=0 --sync_fault_injection=1 --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=3 --unpartitioned_pinning=0 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=1 --use_multi_get_entity=1 --use_multiget=1 --use_put_entity_one_in=0 --user_timestamp_size=0 --value_size_mult=32 --verification_only=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --verify_file_checksums_one_in=1000000 --verify_iterator_with_expected_state_one_in=5 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=524288 --wal_compression=none --write_buffer_size=33554432 --write_dbid_to_manifest=0 --writepercent=35\n```\n\nReviewed By: cbi42\n\nDifferential Revision: D49344996\n\nPulled By: jaykorean\n\nfbshipit-source-id: 8059b8127c0e3cb8af96cf222f47398413c92c50",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/no_batched_ops_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/99f8820054898d44e5cd5f67edd8c940ce1633bb",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4fa2c017190a99a9ad44562b12aeef6f6d2a1d16",
        "author": "Jay Huh",
        "date": "2023-08-21T12:14:57-07:00",
        "message": "Replace existing waitforcompaction with new WaitForCompact API in db_bench_tool (#11727)\n\nSummary:\nAs the new API to wait for compaction is available (https://github.com/facebook/rocksdb/issues/11436), we can now replace the existing logic of waiting in db_bench_tool with the new API.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11727\n\nTest Plan:\n```\n./db_bench --benchmarks=\"fillrandom,compactall,waitforcompaction,readrandom\"\n```\n**Before change**\n```\nSet seed to 1692635571470041 because --seed was 0\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nIntegrated BlobDB: blob cache disabled\nRocksDB:    version 8.6.0\nDate:       Mon Aug 21 09:33:40 2023\nCPU:        80 * Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz\nCPUCache:   28160 KB\nKeys:       16 bytes each (+ 0 bytes user-defined timestamp)\nValues:     100 bytes each (50 bytes after compression)\nEntries:    1000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    110.6 MB (estimated)\nFileSize:   62.9 MB (estimated)\nWrite rate: 0 bytes/second\nRead rate: 0 ops/second\nCompression: Snappy\nCompression sampling rate: 0\nMemtablerep: SkipListFactory\nPerf Level: 1\nWARNING: Optimization is disabled: benchmarks unnecessarily slow\nWARNING: Assertions are enabled; benchmarks unnecessarily slow\n------------------------------------------------\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nIntegrated BlobDB: blob cache disabled\nDB path: [/tmp/rocksdbtest-226125/dbbench]\nfillrandom   :      51.826 micros/op 19295 ops/sec 51.826 seconds 1000000 operations;    2.1 MB/s\nwaitforcompaction(/tmp/rocksdbtest-226125/dbbench): started\nwaitforcompaction(/tmp/rocksdbtest-226125/dbbench): finished\nwaitforcompaction(/tmp/rocksdbtest-226125/dbbench): started\nwaitforcompaction(/tmp/rocksdbtest-226125/dbbench): finished\nDB path: [/tmp/rocksdbtest-226125/dbbench]\nreadrandom   :      39.042 micros/op 25613 ops/sec 39.042 seconds 1000000 operations;    1.8 MB/s (632886 of 1000000 found)\n```\n**After change**\n```\nSet seed to 1692636574431745 because --seed was 0\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nIntegrated BlobDB: blob cache disabled\nRocksDB:    version 8.6.0\nDate:       Mon Aug 21 09:49:34 2023\nCPU:        80 * Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz\nCPUCache:   28160 KB\nKeys:       16 bytes each (+ 0 bytes user-defined timestamp)\nValues:     100 bytes each (50 bytes after compression)\nEntries:    1000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    110.6 MB (estimated)\nFileSize:   62.9 MB (estimated)\nWrite rate: 0 bytes/second\nRead rate: 0 ops/second\nCompression: Snappy\nCompression sampling rate: 0\nMemtablerep: SkipListFactory\nPerf Level: 1\nWARNING: Optimization is disabled: benchmarks unnecessarily slow\nWARNING: Assertions are enabled; benchmarks unnecessarily slow\n------------------------------------------------\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nIntegrated BlobDB: blob cache disabled\nDB path: [/tmp/rocksdbtest-226125/dbbench]\nfillrandom   :      51.271 micros/op 19504 ops/sec 51.271 seconds 1000000 operations;    2.2 MB/s\nwaitforcompaction(/tmp/rocksdbtest-226125/dbbench): started\nwaitforcompaction(/tmp/rocksdbtest-226125/dbbench): finished with status (OK)\nDB path: [/tmp/rocksdbtest-226125/dbbench]\nreadrandom   :      39.264 micros/op 25468 ops/sec 39.264 seconds 1000000 operations;    1.8 MB/s (632921 of 1000000 found)\n```\n\nReviewed By: ajkr\n\nDifferential Revision: D48524667\n\nPulled By: jaykorean\n\nfbshipit-source-id: 1052a15b2ed79a35165ec4d9998d0454b2552ef4",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4fa2c017190a99a9ad44562b12aeef6f6d2a1d16",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f53018c0c8a312be86eb0df956e03d4c81706030",
        "author": "Hui Xiao",
        "date": "2023-08-18T17:47:22-07:00",
        "message": "Improve PrefetchTest.Basic with explicit flush and file num variable (#11720)\n\nSummary:\n**Context/Summary:** as title, should be harmless. And it's a guessed fix to https://github.com/facebook/rocksdb/issues/11717 while no repro has obtained on my end yet.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11720\n\nTest Plan: existing tests\n\nReviewed By: cbi42\n\nDifferential Revision: D48475661\n\nPulled By: hx235\n\nfbshipit-source-id: 7c7390319f094c540e703fe2e78a8d601b7a894b",
        "modified_files_count": 1,
        "modified_files": [
            "file/prefetch_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f53018c0c8a312be86eb0df956e03d4c81706030",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a67ef998dc231f60d12eb43eedb61bf9728b51b7",
        "author": "Han Zhu",
        "date": "2023-08-18T10:19:33-07:00",
        "message": "Explicitly instantiate MaybeReadBlockAndLoadToCache as well (#11714)\n\nSummary:\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11714\n\nFixes T161017540.\n\nThe staging build starts failing with an undefined symbol error:\n```\nld.lld: error: undefined symbol: std::enable_if<rocksdb::ParsedFullFilterBlock::kCacheEntryRole == (rocksdb::CacheEntryRole)13 || true, rocksdb::Status>::type rocksdb::BlockBasedTable::MaybeReadBlockAndLoadToCache<rocksdb::ParsedFullFilterBlock>(rocksdb::FilePrefetchBuffer*, rocksdb::ReadOptions const&, rocksdb::BlockHandle const&, rocksdb::UncompressionDict const&, bool, rocksdb::CachableEntry<rocksdb::ParsedFullFilterBlock>*, rocksdb::GetContext*, rocksdb::BlockCacheLookupContext*, rocksdb::BlockContents*, bool) const\n```\nThis is the `MaybeReadBlockAndLoadToCache` function where `TBlocklike = ParsedFullFilterBlock`. The trigger was an FDO profile update D48261413.\n\n`MaybeReadBlockAndLoadToCache` is used in the same translation unit `block_based_table_reader.cc`, and also in another file `partitioned_filter_block.cc`. The later was the file that couldn't find the symbol. It seems after the FDO profile update, `MaybeReadBlockAndLoadToCache` may've got inlined into its caller in `block_based_table_reader.cc`. And with no knowledge of other usages, the symbol got stripped.\n\nExplicitly instantiate the template similar to how `RetrieveBlock` was handled.\n\nReviewed By: pdillinger, akankshamahajan15\n\nDifferential Revision: D48400574\n\nfbshipit-source-id: d4a80999bfb6ce4afa80678444139fcd8ae84aa4",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a67ef998dc231f60d12eb43eedb61bf9728b51b7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a85eccc6d6837f5ffb69427eb4074e13fa0dde10",
        "author": "Peter Dillinger",
        "date": "2023-08-10T13:05:45-07:00",
        "message": "Adjust db_stress handling of TryAgain from optimistic txn (#11691)\n\nSummary:\nWe're still getting some rare cases of 5x TryAgains in a row. Here I'm boosting the failure threshold to 10 in a row and adding more info in the output, to help us manually verify whether there's anything suspicous about the sequence of TryAgains, such as if Rollback failed to reset to new sequence numbers.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11691\n\nTest Plan: By lowering the threshold to 2 and adjusting some other db_crashtest parameters, I was able to hit my new code and saw fresh sequence number on the subsequent TryAgain.\n\nReviewed By: cbi42\n\nDifferential Revision: D48236153\n\nPulled By: pdillinger\n\nfbshipit-source-id: c0530e969ddcf8de7348e5cf7daf5d6d5dec24f4",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_test_base.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a85eccc6d6837f5ffb69427eb4074e13fa0dde10",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d2b0652b32b8671c9ec4057e6da2fa564d1cc610",
        "author": "Xinye Tao",
        "date": "2023-08-07T12:29:31-07:00",
        "message": "compute compaction score once for a batch of range file deletes (#10744)\n\nSummary:\nOnly re-calculate compaction score once for a batch of deletions. Fix performance regression brought by https://github.com/facebook/rocksdb/pull/8434.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10744\n\nTest Plan:\nIn one of our production cluster that recently upgraded to RocksDB 6.29, it takes more than 10 minutes to delete files in 30,000 ranges. The RocksDB instance contains approximately 80,000 files. After this patch, the duration reduces to 100+ ms, which is on par with RocksDB 6.4.\n\nCherry-picking downstream PR: https://github.com/tikv/rocksdb/pull/316\n\nSigned-off-by: tabokie <xy.tao@outlook.com>\n\nReviewed By: cbi42\n\nDifferential Revision: D48002581\n\nPulled By: ajkr\n\nfbshipit-source-id: 7245607ee3ad79c53b648a6396c9159f166b9437",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d2b0652b32b8671c9ec4057e6da2fa564d1cc610",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f9de217353f2d45f06fe5b9eab50b191f1a2d7a2",
        "author": "Peter Dillinger",
        "date": "2023-08-02T13:19:20-07:00",
        "message": "Some cache_bench enhancements (#11661)\n\nSummary:\n... used in validating some HyperClockCache development in progress.\n\n* Revamp the \"populate cache\" step to avoid redundant insertions (very rare in practice) and more consistently approach the desired resident_ratio while maintaining appropriate skew (still not perfect).\n* Track and print hit ratio on lookups, to ensure a fair comparison is happening between implementations etc.\n* Add an option to disable tracking and printing histograms (lots of output)\n* Add an option to specify a random seed (for more reproducibility)\n* Remove confusing/redundant \"-skewed\" option\n\nUses BitwiseAnd from https://github.com/facebook/rocksdb/issues/11660 (tested there)\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11661\n\nTest Plan: manual\n\nReviewed By: akankshamahajan15, jowlyzhang\n\nDifferential Revision: D47937671\n\nPulled By: pdillinger\n\nfbshipit-source-id: 85a2bb881b1bca4f63e015bac684105fd91c9f35",
        "modified_files_count": 1,
        "modified_files": [
            "cache/cache_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f9de217353f2d45f06fe5b9eab50b191f1a2d7a2",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2f712235ab48783af5d85e65e2fb43448d61859f",
        "author": "shuzz",
        "date": "2023-07-19T12:52:39-07:00",
        "message": "optimized code (#11614)\n\nSummary:\nimprovement code by std::move and c++17\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11614\n\nReviewed By: ajkr\n\nDifferential Revision: D47599519\n\nPulled By: jowlyzhang\n\nfbshipit-source-id: 6b897876f4e87e94a74c53d8db2a01303d500bff",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl/db_impl_compaction_flush.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2f712235ab48783af5d85e65e2fb43448d61859f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "687a2a0d9ad5b0a3588e331ecd15317f3384def0",
        "author": "Andrew Kryczka",
        "date": "2023-06-02T16:39:14-07:00",
        "message": "Small improvements to DBGet microbenchmark (#11498)\n\nSummary:\nFollow a couple best practices:\n\n- Allowed Google benchmark to decide number of iterations. Previously we hardcoded a value, which circumvented benchmark's heuristic for iterating until the result is stable.\n- Made each iteration do similar work. Previously, an iteration could do different work depending if the key was found in the first, second, third, or no L0 file.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11498\n\nTest Plan: none as I am unable to prove it is better\n\nReviewed By: hx235\n\nDifferential Revision: D46339050\n\nPulled By: ajkr\n\nfbshipit-source-id: fcfc6da4111c5b3ae86d79d908afc5f61f96675b",
        "modified_files_count": 1,
        "modified_files": [
            "microbench/db_basic_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/687a2a0d9ad5b0a3588e331ecd15317f3384def0",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3622cfa34aca061eaf0595c5fa711f2b34fd3a07",
        "author": "Hui Xiao",
        "date": "2023-04-24T15:14:23-07:00",
        "message": "Add back io_uring stress test hack with DbStressFSWrapper for FS not supporting read async (#11404)\n\nSummary:\n**Context/Summary:**\nTo better utilize `DbStressFSWrapper` for some assertion, https://github.com/facebook/rocksdb/pull/11288 removed an io_uring stress test hack for POSIX FS not supporting read async added in https://github.com/facebook/rocksdb/pull/11242 = It was removed based on the assumption that a later PR https://github.com/facebook/rocksdb/pull/11296 is sufficient to serve as an alternative workaround.\n\nBut recent stress tests has shown the opposite, mostly because 11296  approach might be subjected to incompleteness when more `ReadOptions` are passed down as what https://github.com/facebook/rocksdb/pull/11288 has done.\n\nAs a short-term solution to both work around POSIX FS constraint above and utilize `DbStressFSWrapper` for 11288 assertion, I proposed this PR.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11404\n\nTest Plan:\n- Stress test ensures 11288's assertion is still effective in `DbStressFSWrapper`\n```\n./db_stress --acquire_snapshot_one_in=10000 --adaptive_readahead=0 --allow_data_in_errors=True --async_io=1 --avoid_flush_during_recovery=1 --avoid_unnecessary_blocking_io=0 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=8 --block_size=16384 --bloom_bits=16 --bottommost_compression_type=disable --bytes_per_sync=0 --cache_index_and_filter_blocks=0 --cache_size=8388608 --cache_type=hyper_clock_cache --charge_compression_dictionary_building_buffer=0 --charge_file_metadata=1 --charge_filter_construction=1 --charge_table_reader=0 --checkpoint_one_in=1000000 --checksum_type=kxxHash64 --clear_column_family_one_in=0 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_pri=1 --compaction_ttl=0 --compression_max_dict_buffer_bytes=32767 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=lz4 --compression_use_zstd_dict_trainer=1 --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --data_block_index_type=0 --db=$db --db_write_buffer_size=0 --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --detect_filter_construct_corruption=1 --disable_wal=0 --enable_compaction_filter=0 --enable_pipelined_write=1 --enable_thread_tracking=0 --expected_values_dir=$exp --fail_if_options_file_error=1 --fifo_allow_compaction=0 --file_checksum_impl=crc32c --flush_one_in=1000000 --format_version=4 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=4 --index_type=0 --ingest_external_file_one_in=0 --initial_auto_readahead_size=16384 --iterpercent=10 --key_len_percent_dist=1,30,69 --kill_random_test=888887 --level_compaction_dynamic_level_bytes=0 --lock_wal_one_in=1000000 --log2_keys_per_lock=10 --long_running_snapshots=0 --manual_wal_flush_one_in=1000 --mark_for_compaction_one_file_in=10 --max_auto_readahead_size=16384 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=25000000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=1048576 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=8388608 --memtable_prefix_bloom_size_ratio=0.1 --memtable_protection_bytes_per_key=4 --memtable_whole_key_filtering=0 --memtablerep=skip_list --min_write_buffer_number_to_merge=2 --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --num_file_reads_for_auto_readahead=0 --open_files=100 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=20000000 --optimize_filters_for_memory=0 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=0 --pause_background_one_in=1000000 --periodic_compaction_seconds=0 --prefix_size=1 --prefixpercent=5 --prepopulate_block_cache=1 --preserve_internal_time_seconds=36000 --progress_reports=0 --read_fault_one_in=32 --readahead_size=16384 --readpercent=45 --recycle_log_file_num=0 --reopen=20 --ribbon_starting_level=1 --secondary_cache_fault_one_in=32 --secondary_cache_uri=compressed_secondary_cache://capacity=8388608 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --stats_dump_period_sec=10 --subcompactions=1 --sync=0 --sync_fault_injection=1 --target_file_size_base=2097152 --target_file_size_multiplier=2 --test_batches_snapshots=0 --top_level_index_pinning=2 --unpartitioned_pinning=3 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=1 --use_full_merge_v1=0 --use_get_entity=0 --use_merge=1 --use_multi_get_entity=0 --use_multiget=1 --use_put_entity_one_in=0 --user_timestamp_size=0 --value_size_mult=32 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --verify_sst_unique_id_in_manifest=1 --wal_bytes_per_sync=524288 --wal_compression=none --write_buffer_size=4194304 --write_dbid_to_manifest=1 --writepercent=35\n```\n- Monitor future stress test to show `MultiGet error: Not implemented: ReadAsync` is gone\n\nReviewed By: ltamasi\n\nDifferential Revision: D45242280\n\nPulled By: hx235\n\nfbshipit-source-id: 9823e3fbd4e9672efdd31478a2f2cbd68a98bdf5",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3622cfa34aca061eaf0595c5fa711f2b34fd3a07",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b8555ba470ba0592c66a3caafd4d8ea74387d565",
        "author": "Andrew Kryczka",
        "date": "2023-04-17T10:36:22-07:00",
        "message": "Deflake DBBloomFilterTest.OptimizeFiltersForHits (#11383)\n\nSummary:\nIn CircleCI build-linux-arm-test-full job (https://app.circleci.com/pipelines/github/facebook/rocksdb/26462/workflows/a9d39d2c-c970-4b0f-9c10-7743beb9771b/jobs/591722), this test exhibited the following flaky failure:\n\n```\ndb/db_bloom_filter_test.cc:2506: Failure\nExpected: (TestGetTickerCount(options, BLOOM_FILTER_USEFUL)) > (65000 * 2), actual: 120558 vs 130000\n```\n\nI ssh'd to an instance and observed it cuts memtables at slightly different points across runs. Logging in `ConcurrentArena` pointed to `try_lock()` returning false at different points across runs.\n\nThis PR changes the approach to allow a fixed number of keys per memtable flush. I verified the bloom filter useful count is deterministic now even on the CircleCI ARM instance.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11383\n\nReviewed By: cbi42\n\nDifferential Revision: D45036829\n\nPulled By: ajkr\n\nfbshipit-source-id: b602dacb63955f1af09bf0ed409cde0552805a08",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bloom_filter_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b8555ba470ba0592c66a3caafd4d8ea74387d565",
        "contains_optimization_keyword": true
    },
    {
        "hash": "760b773f58277f9ce449389c0773a1eee2d14363",
        "author": "Andrew Kryczka",
        "date": "2023-04-10T13:59:44-07:00",
        "message": "fix optimization-disabled test builds with platform010 (#11361)\n\nSummary:\nFixed the following failure:\n\n```\nthird-party/gtest-1.8.1/fused-src/gtest/gtest-all.cc: In function \u2018bool testing::internal::StackGrowsDown()\u2019:\nthird-party/gtest-1.8.1/fused-src/gtest/gtest-all.cc:8681:24: error: \u2018dummy\u2019 may be used uninitialized [-Werror=maybe-uninitialized]\n 8681 |   StackLowerThanAddress(&dummy, &result);\n      |   ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\nthird-party/gtest-1.8.1/fused-src/gtest/gtest-all.cc:8671:13: note: by argument 1 of type \u2018const void*\u2019 to \u2018void testing::internal::StackLowerThanAddress(const void*, bool*)\u2019 declared here\n 8671 | static void StackLowerThanAddress(const void* ptr, bool* result) {\n      |             ^~~~~~~~~~~~~~~~~~~~~\nthird-party/gtest-1.8.1/fused-src/gtest/gtest-all.cc:8679:7: note: \u2018dummy\u2019 declared here\n 8679 |   int dummy;\n      |       ^~~~~\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11361\n\nReviewed By: cbi42\n\nDifferential Revision: D44838033\n\nPulled By: ajkr\n\nfbshipit-source-id: 27d68b5a24a15723bbaaa7de45ccd70a60fe259e",
        "modified_files_count": 1,
        "modified_files": [
            "third-party/gtest-1.8.1/fused-src/gtest/gtest-all.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/760b773f58277f9ce449389c0773a1eee2d14363",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e17f31057b63a96aed50b0b4469f9629e0089093",
        "author": "Peter Dillinger",
        "date": "2023-02-03T13:21:03-08:00",
        "message": "Support stack traces with gdb (and debugger invocation) (#11150)\n\nSummary:\nLIB_MODE=shared is much more efficient for building all the unit tests but comes with the downside of ugly stack traces, generally missing name demangling and source line info. Searching the internet suggests the reliable way to get stack traces with dynamic loading is with gdb.\n\nThis change automatically tries to use gdb to get a stack trace if built with LIB_MODE=shared, and only on Linux because that's where we have the capability to attach to the proper thread. (We could revise the exact conditions in the future.) If there's a failure invoking gdb, it falls back on the old method. Obscure details of making the output reasonable / pretty are in the source code comments.\n\nBased on this, it was easy to make it so that running a test command with ROCKSDB_DEBUG=1 would invoke gdb whenever the stack trace handler was invoked, so I included that.\n\nIntended follow-up: make LIB_MODE=shared the new default `make` build config\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11150\n\nTest Plan:\nmanual, mostly by injecting an \"assert(false)\" into a unit test and trying different build modes etc.\n\nAlthough gdb is slower to start showing stack trace output, it seems overall faster in many if not most cases, presumably because it doesn't reload the symbol table for each stack entry. At least with parallel test runs, having many tests dumping stacks with the old method can take so long it appears to hang the test run.\n\nReviewed By: cbi42\n\nDifferential Revision: D42894064\n\nPulled By: pdillinger\n\nfbshipit-source-id: 608143309d8c69c40049c9a4abcde4f22e87b4d8",
        "modified_files_count": 1,
        "modified_files": [
            "port/stack_trace.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e17f31057b63a96aed50b0b4469f9629e0089093",
        "contains_optimization_keyword": true
    },
    {
        "hash": "df680b24ef2bbd8c9ee8cf882baa7636bcc21be4",
        "author": "Levi Tamasi",
        "date": "2023-02-01T10:03:07-08:00",
        "message": "Clean up InvokeFilterIfNeeded a bit (#11174)\n\nSummary:\nThe patch makes some code quality enhancements in `CompactionIterator::InvokeFilterIfNeeded`\nincluding the renaming of `filter` (which is most likely a remnant of the days before the `FilterV2`\nAPI when the compaction filter used to return a boolean) to `decision`, the removal of some\noutdated comments, the elimination of an `error` flag which was only used in one failure case\nout of many, as well as some small stylistic improvements. (Some the above will also come in\nhandy when adding compaction filter support for wide-column entities.)\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11174\n\nTest Plan: `make check`\n\nReviewed By: akankshamahajan15\n\nDifferential Revision: D42901408\n\nPulled By: ltamasi\n\nfbshipit-source-id: ab382d59a4990c5dfe1cee219d49e1d80902b666",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction/compaction_iterator.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/df680b24ef2bbd8c9ee8cf882baa7636bcc21be4",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e0ea0dc6bdb6c7160914800823ac0755f26bb4fc",
        "author": "Changyu Bi",
        "date": "2023-01-24T12:12:19-08:00",
        "message": "Improve documentation for `allow_ingest_behind` (#11119)\n\nSummary:\nupdate documentation to mention that only universal compaction is supported.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11119\n\nReviewed By: ajkr\n\nDifferential Revision: D42715986\n\nPulled By: cbi42\n\nfbshipit-source-id: 91b145d3318334cb92857c5c0ffc0efed6fa4363",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e0ea0dc6bdb6c7160914800823ac0755f26bb4fc",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a510880346d68b7710078bf99240e560610c02b3",
        "author": "anand76",
        "date": "2023-01-12T18:09:07-08:00",
        "message": "Add a unit test for async prefetch fix in #11049 (#11084)\n\nSummary:\nAdd a unit test in prefetch_test for https://github.com/facebook/rocksdb/issues/11049\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/11084\n\nTest Plan: Verify the test fails without https://github.com/facebook/rocksdb/issues/11049 and passes with it\n\nReviewed By: akankshamahajan15\n\nDifferential Revision: D42485828\n\nPulled By: anand1976\n\nfbshipit-source-id: ae512f2d121745a1f5212645a9b58868976c1f83",
        "modified_files_count": 1,
        "modified_files": [
            "file/prefetch_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a510880346d68b7710078bf99240e560610c02b3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "db9cbddc6fbf95d9baee7c51ff0bda3e9d36982d",
        "author": "Andrew Kryczka",
        "date": "2022-11-22T13:07:17-08:00",
        "message": "Deflake DBTest2.TraceAndReplay by relaxing latency checks (#10979)\n\nSummary:\nSince the latency measurement uses real time it is possible for the operation to complete in zero microseconds and then fail these checks. We saw this with the operation that invokes Get() on an invalid CF. This PR relaxes the assertions to allow for operations completing in zero microseconds.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10979\n\nReviewed By: riversand963\n\nDifferential Revision: D41478300\n\nPulled By: ajkr\n\nfbshipit-source-id: 50ef096bd8f0162b31adb46f54ae6ddc337d0a5e",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test2.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/db9cbddc6fbf95d9baee7c51ff0bda3e9d36982d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1562524e63229f41547f8d72c88001a4e259a4c3",
        "author": "Akanksha Mahajan",
        "date": "2022-11-14T16:14:41-08:00",
        "message": "Fix db_stress failure in async_io in FilePrefetchBuffer (#10949)\n\nSummary:\nFix db_stress failure in async_io in FilePrefetchBuffer.\n\nFrom the logs, assertion was caused when\n- prev_offset_ = offset but somehow prev_len != 0 and explicit_prefetch_submitted_ = true. That scenario is when we send async request to prefetch buffer during seek but in second seek that data is found in cache. prev_offset_ and prev_len_ get updated but we were not setting explicit_prefetch_submitted_ = false because of which buffers were getting out of sync.\nIt's possible a read by another thread might have loaded the block into the cache in the meantime.\n\nParticular assertion example:\n```\nprev_offset: 0, prev_len_: 8097 , offset: 0, length: 8097, actual_length: 8097 , actual_offset: 0 ,\ncurr_: 0, bufs_[curr_].offset_: 4096 ,bufs_[curr_].CurrentSize(): 48541 , async_len_to_read: 278528, bufs_[curr_].async_in_progress_: false\nsecond: 1, bufs_[second].offset_: 282624 ,bufs_[second].CurrentSize(): 0, async_len_to_read: 262144 ,bufs_[second].async_in_progress_: true ,\nexplicit_prefetch_submitted_: true , copy_to_third_buffer: false\n```\nAs we can see curr_ was expected to read 278528 but it read 48541. Also buffers are out of sync.\nAlso `explicit_prefetch_submitted_` is set true but prev_len not 0.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10949\n\nTest Plan:\n- Ran db_bench for regression to make sure there is no regression;\n- Ran db_stress failing without this fix,\n- Ran build-linux-mini-crashtest 7- 8 times locally + CircleCI\n\nReviewed By: anand1976\n\nDifferential Revision: D41257786\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: 1d100f94f8c06bbbe4cc76ca27f1bbc820c2494f",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1562524e63229f41547f8d72c88001a4e259a4c3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1ee747d7950d04d324437e86448d98763b3147a9",
        "author": "Peter Dillinger",
        "date": "2022-10-13T09:08:09-07:00",
        "message": "Deflake^2 DBBloomFilterTest.OptimizeFiltersForHits (#10816)\n\nSummary:\nThis reverts https://github.com/facebook/rocksdb/issues/10792 and uses a different strategy to stabilize the test: remove the unnecessary randomness by providing a constant seed for shuffling keys.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10816\n\nTest Plan: `gtest-parallel ./db_bloom_filter_test -r1000 --gtest_filter=*ForHits*`\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D40347957\n\nPulled By: pdillinger\n\nfbshipit-source-id: a270e157485cbd94ed03b80cdd21b954ebd57d57",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bloom_filter_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1ee747d7950d04d324437e86448d98763b3147a9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "62ba5c80343e5ab097415f709f92043309c86d6e",
        "author": "Jay Zhuang",
        "date": "2022-10-10T12:34:25-07:00",
        "message": "Deflake DBBloomFilterTest.OptimizeFiltersForHits (#10792)\n\nSummary:\nThe test may fail because the L5 files may only cover small portion of the whole key range.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10792\n\nTest Plan:\n```\ngtest-parallel ./db_bloom_filter_test --gtest_filter=DBBloomFilterTest.OptimizeFiltersForHits -r 1000 -w 100\n```\n\nReviewed By: siying\n\nDifferential Revision: D40217600\n\nPulled By: siying\n\nfbshipit-source-id: 18db549184bccf5e513eaa7e31ab17385b71ef71",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bloom_filter_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/62ba5c80343e5ab097415f709f92043309c86d6e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2280b2612a1be080d5219762e0ded53a2c2c974d",
        "author": "Levi Tamasi",
        "date": "2022-09-26T15:33:36-07:00",
        "message": "Small cleanup in NonBatchedOpsStressTest::VerifyDb (#10740)\n\nSummary:\nThe PR cleans up the logic in `NonBatchedOpsStressTest::VerifyDb` so that\nthe verification method is picked using a single random number generation.\nIt also eliminates some repeated key comparisons and makes some small\ncode hygiene improvements.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10740\n\nTest Plan: Ran a simple blackbox crash test.\n\nReviewed By: riversand963\n\nDifferential Revision: D39828646\n\nPulled By: ltamasi\n\nfbshipit-source-id: 60ee5a3bb1851278f62c7d83b0c93b902ed9702e",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/no_batched_ops_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2280b2612a1be080d5219762e0ded53a2c2c974d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7dad48527853ba8c89fb32e0fcdb4fc1cd8e7e37",
        "author": "Levi Tamasi",
        "date": "2022-09-15T13:44:46-07:00",
        "message": "Support JemallocNodumpAllocator for the block/blob cache in db_bench (#10685)\n\nSummary:\nThe patch makes it possible to use the `JemallocNodumpAllocator` with the\nblock/blob caches in `db_bench`. In addition to its stated purpose of excluding\ncache contents from core dumps, `JemallocNodumpAllocator` also uses\na dedicated arena and jemalloc tcaches for cache allocations, which can\nreduce fragmentation and thus memory usage.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10685\n\nReviewed By: riversand963\n\nDifferential Revision: D39552261\n\nPulled By: ltamasi\n\nfbshipit-source-id: b5c58eab6b7c1baa9a307d9f1248df1d7a77d2b5",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7dad48527853ba8c89fb32e0fcdb4fc1cd8e7e37",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f9cfc6a808c9dc3ab7366edb10368559155d5172",
        "author": "Changyu Bi",
        "date": "2022-07-06T09:30:25-07:00",
        "message": "Updated NewDataBlockIterator to not fetch compression dict for non-da\u2026 (#10310)\n\nSummary:\n\u2026ta blocks\n\nDuring MyShadow testing, ajkr helped me find out that with partitioned index and dictionary compression enabled, `PartitionedIndexIterator::InitPartitionedIndexBlock()` spent considerable amount of time (1-2% CPU) on fetching uncompression dictionary. Fetching uncompression dict was not needed since the index blocks were not compressed (and even if they were, they use empty dictionary). This should only affect use cases with partitioned index, dictionary compression and without uncompression dictionary pinned. This PR updates NewDataBlockIterator to not fetch uncompression dictionary when it is not for data blocks.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10310\n\nTest Plan:\n1. `make check`\n2. Perf benchmark: 1.5% (143950 -> 146176) improvement in op/sec for partitioned index + dict compression benchmark.\nFor default config without partitioned index and without dict compression, there is no regression in readrandom perf from multiple runs of db_bench.\n\n```\n# Set up for partitioned index with dictionary compression\nTEST_TMPDIR=/dev/shm ./db_bench_main -benchmarks=filluniquerandom,compact -max_background_jobs=24 -memtablerep=vector -allow_concurrent_memtable_write=false -partition_index=true  -compression_max_dict_bytes=16384 -compression_zstd_max_train_bytes=1638400\n\n# Pre PR\nTEST_TMPDIR=/dev/shm ./db_bench_main -use_existing_db=true -benchmarks=readrandom[-X50] -partition_index=true\nreadrandom [AVG    50 runs] : 143950 (\u00b1 1108) ops/sec;   15.9 (\u00b1 0.1) MB/sec\nreadrandom [MEDIAN 50 runs] : 144406 ops/sec;   16.0 MB/sec\n\n# Post PR\nTEST_TMPDIR=/dev/shm ./db_bench_opt -use_existing_db=true -benchmarks=readrandom[-X50] -partition_index=true\nreadrandom [AVG    50 runs] : 146176 (\u00b1 1121) ops/sec;   16.2 (\u00b1 0.1) MB/sec\nreadrandom [MEDIAN 50 runs] : 146014 ops/sec;   16.2 MB/sec\n\n# Set up for no partitioned index and no dictionary compression\nTEST_TMPDIR=/dev/shm/baseline ./db_bench_main -benchmarks=filluniquerandom,compact -max_background_jobs=24 -memtablerep=vector -allow_concurrent_memtable_write=false\n# Pre PR\nTEST_TMPDIR=/dev/shm/baseline/ ./db_bench_main --use_existing_db=true \"--benchmarks=readrandom[-X50]\"\nreadrandom [AVG    50 runs] : 158546 (\u00b1 1000) ops/sec;   17.5 (\u00b1 0.1) MB/sec\nreadrandom [MEDIAN 50 runs] : 158280 ops/sec;   17.5 MB/sec\n\n# Post PR\nTEST_TMPDIR=/dev/shm/baseline/ ./db_bench_opt --use_existing_db=true \"--benchmarks=readrandom[-X50]\"\nreadrandom [AVG    50 runs] : 161061 (\u00b1 1520) ops/sec;   17.8 (\u00b1 0.2) MB/sec\nreadrandom [MEDIAN 50 runs] : 161596 ops/sec;   17.9 MB/sec\n```\n\nReviewed By: ajkr\n\nDifferential Revision: D37631358\n\nPulled By: cbi42\n\nfbshipit-source-id: 6ca2665e270e63871968e061ba4a99d3136785d9",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_reader_impl.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f9cfc6a808c9dc3ab7366edb10368559155d5172",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f87adcfb3f247a02c9b08a662c4b8aca971e620e",
        "author": "Peter Dillinger",
        "date": "2022-06-17T12:53:57-07:00",
        "message": "Fix overflow in ribbon_bench after #10184 (#10195)\n\nSummary:\nRibbon micro-bench needs updating after re-numbering\n`BloomLikeFilterPolicy::GetAllFixedImpls()` entries. (CircleCI nightly\nfailure.)\n\nAlso fixed memory leaks while using ASAN to validate my fix. (I assume\nthe leaks weren't intentional for some performance characteristic.)\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10195\n\nTest Plan: run with ASAN\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D37244459\n\nPulled By: pdillinger\n\nfbshipit-source-id: 5a363e10de3c4c9c88099c937e3dc3b4cf24fd30",
        "modified_files_count": 1,
        "modified_files": [
            "microbench/ribbon_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f87adcfb3f247a02c9b08a662c4b8aca971e620e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2e5a323dbd4dbfad5b1e3d45d489e6dca37f4257",
        "author": "Ali Saidi",
        "date": "2022-06-15T13:08:11-07:00",
        "message": "Change the instruction used for a pause on arm64 (#10118)\n\nSummary:\nWhile the yield instruction conseptually sounds correct on most platforms it is\na simple nop that doesn't delay the execution anywhere close to what an x86\npause instruction does. In other projects with spin-wait loops an isb has been\nobserved to be much closer to the x86 behavior.\n\nOn a Graviton3 system the following test improves on average by 2x with this\nchange averaged over 20 runs:\n\n```\n./db_bench  -benchmarks=fillrandom -threads=64 -batch_size=1\n-memtablerep=skip_list -value_size=100 --num=100000\nlevel0_slowdown_writes_trigger=9999 -level0_stop_writes_trigger=9999\n-disable_auto_compactions --max_write_buffer_number=8 -max_background_flushes=8\n--disable_wal --write_buffer_size=160000000 --block_size=16384\n--allow_concurrent_memtable_write -compression_type none\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10118\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D37120578\n\nfbshipit-source-id: c20bde4298222edfab7ff7cb6d42497e7012400d",
        "modified_files_count": 1,
        "modified_files": [
            "port/port_posix.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2e5a323dbd4dbfad5b1e3d45d489e6dca37f4257",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b550fc0b090f63e95b567d34bdc3be258fbfe43a",
        "author": "Ali Saidi",
        "date": "2022-06-14T17:58:44-07:00",
        "message": "Modify the instructions emited for PREFETCH on arm64 (#10117)\n\nSummary:\n__builtin_prefetch(...., 1) prefetches into the L2 cache on x86 while the same\nemits a pldl3keep instruction on arm64 which doesn't seem to be close enough.\n\nTesting on a Graviton3, and M1 system with memtablerep_bench fillrandom and\nskiplist througpuh increased as follows adjusting the 1 to 2 or 3:\n```\n           1 -> 2     1 -> 3\n----------------------------\nGraviton3   +10%        +15%\nM1          +10%        +10%\n```\n\nGiven that prefetching into the L1 cache seems to help, I chose that conversion\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10117\n\nReviewed By: pdillinger\n\nDifferential Revision: D37120475\n\nfbshipit-source-id: db1ef43f941445019c68316500a2250acc643d5e",
        "modified_files_count": 1,
        "modified_files": [
            "port/port_posix.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b550fc0b090f63e95b567d34bdc3be258fbfe43a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "eb99e080760bf86a9fd2b725df2fdeb1dd4df884",
        "author": "Guido Tagliavini Ponce",
        "date": "2022-06-03T13:40:09-07:00",
        "message": "Add support for FastLRUCache in cache_bench (#10095)\n\nSummary:\ncache_bench can now run with FastLRUCache.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10095\n\nTest Plan:\n- Temporarily add an ``assert(false)`` in the execution path that sets up the FastLRUCache. Run ``make -j24 cache_bench``. Then test the appropriate code is used by running ``./cache_bench -cache_type=fast_lru_cache`` and checking that the assert is called. Repeat for LRUCache.\n- Verify that FastLRUCache (currently a clone of LRUCache) has similar latency distribution than LRUCache, by comparing the outputs of ``./cache_bench -cache_type=fast_lru_cache`` and ``./cache_bench -cache_type=lru_cache``.\n\nReviewed By: pdillinger\n\nDifferential Revision: D36875834\n\nPulled By: guidotag\n\nfbshipit-source-id: eb2ad0bb32c2717a258a6ac66ed736e06f826cd8",
        "modified_files_count": 1,
        "modified_files": [
            "cache/cache_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/eb99e080760bf86a9fd2b725df2fdeb1dd4df884",
        "contains_optimization_keyword": true
    },
    {
        "hash": "cf856077956c9852a56e91199f4c7c4d710c01a5",
        "author": "Guido Tagliavini Ponce",
        "date": "2022-06-03T11:16:49-07:00",
        "message": "Add support for FastLRUCache in db_bench. (#10096)\n\nSummary:\ndb_bench can now run with FastLRUCache.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10096\n\nTest Plan:\n- Temporarily add an ``assert(false)`` in the execution path that sets up the FastLRUCache. Run ``make -j24 db_bench``. Then test the appropriate code is used by running ``./db_bench -cache_type=fast_lru_cache`` and checking that the assert is called. Repeat for LRUCache.\n- Verify that FastLRUCache (currently a clone of LRUCache) produces similar benchmark data than LRUCache, by comparing the outputs of ``./db_bench -benchmarks=fillseq,fillrandom,readseq,readrandom -cache_type=fast_lru_cache`` and ``./db_bench -benchmarks=fillseq,fillrandom,readseq,readrandom -cache_type=lru_cache``.\n\nReviewed By: gitbw95\n\nDifferential Revision: D36898774\n\nPulled By: guidotag\n\nfbshipit-source-id: f9f6b6f6da124f88b21b3c8dee742fbb04eff773",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/cf856077956c9852a56e91199f4c7c4d710c01a5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "dde774db646d74054a312878244206431ee939e4",
        "author": "Hui Xiao",
        "date": "2022-05-18T22:25:54-07:00",
        "message": "Mark old reserve* option deprecated (#10016)\n\nSummary:\n**Context/Summary:**\nhttps://github.com/facebook/rocksdb/pull/9926 removed inefficient `reserve*` option API but forgot to mark them deprecated in `block_based_table_type_info` for compatible table format.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/10016\n\nTest Plan: build-format-compatible\n\nReviewed By: pdillinger\n\nDifferential Revision: D36484247\n\nPulled By: hx235\n\nfbshipit-source-id: c41b90cc99fb7ab7098934052f0af7290b221f98",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_factory.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/dde774db646d74054a312878244206431ee939e4",
        "contains_optimization_keyword": true
    },
    {
        "hash": "89571b30e520571600074d6de7d68983533ca280",
        "author": "Wang Yuan",
        "date": "2022-05-09T12:27:38-07:00",
        "message": "Improve the precision of row entry charge in row_cache (#9337)\n\nSummary:\n- For entry charge, we should only calculate the value size instead of including key size in LRUCache\n- The capacity of string could show the memory usage precisely\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9337\n\nReviewed By: ajkr\n\nDifferential Revision: D36219855\n\nfbshipit-source-id: 393e48ca419d230dc552ae62dd0eb1cc9f45961d",
        "modified_files_count": 1,
        "modified_files": [
            "db/table_cache.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/89571b30e520571600074d6de7d68983533ca280",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b82edffc7b48145f1a9818a69d04e23228002896",
        "author": "Siying Dong",
        "date": "2022-05-03T16:21:31-07:00",
        "message": "Improve comments to options.allow_mmap_reads (#9936)\n\nSummary:\nIt confused users and use that with options.allow_mmap_reads = true, CPU is high with checksum verification. Add a comment to explain it.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9936\n\nReviewed By: anand1976\n\nDifferential Revision: D36106529\n\nfbshipit-source-id: 3d723bd686f96a84c694c8b2d91ad28d9ccfd979",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b82edffc7b48145f1a9818a69d04e23228002896",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fb9a167a55e0970b1ef6f67c1600c8d9c4c6114f",
        "author": "Jaromir Vanek",
        "date": "2022-04-25T14:49:54-07:00",
        "message": "Add 95% confidence intervals to db_bench output (#9882)\n\nSummary:\nEnhancing `db_bench` output with 95% statistical confidence intervals for better performance evaluation. The goal is to unambiguously separate random variance when running benchmark over multiple iterations.\n\nOutput enhanced with confidence intervals exposed in brackets:\n\n```\n$ ./db_bench --benchmarks=fillseq[-X10]\n\nRunning benchmark for 10 times\nfillseq      :       4.961 micros/op 201578 ops/sec;   22.3 MB/s\nfillseq      :       5.030 micros/op 198824 ops/sec;   22.0 MB/s\nfillseq [AVG 2 runs] : 200201 (\u00b1 2698) ops/sec;   22.1 (\u00b1 0.3) MB/sec\nfillseq      :       4.963 micros/op 201471 ops/sec;   22.3 MB/s\nfillseq [AVG 3 runs] : 200624 (\u00b1 1765) ops/sec;   22.2 (\u00b1 0.2) MB/sec\nfillseq      :       5.035 micros/op 198625 ops/sec;   22.0 MB/s\nfillseq [AVG 4 runs] : 200124 (\u00b1 1586) ops/sec;   22.1 (\u00b1 0.2) MB/sec\nfillseq      :       4.979 micros/op 200861 ops/sec;   22.2 MB/s\nfillseq [AVG 5 runs] : 200272 (\u00b1 1262) ops/sec;   22.2 (\u00b1 0.1) MB/sec\nfillseq      :       4.893 micros/op 204367 ops/sec;   22.6 MB/s\nfillseq [AVG 6 runs] : 200954 (\u00b1 1688) ops/sec;   22.2 (\u00b1 0.2) MB/sec\nfillseq      :       4.914 micros/op 203502 ops/sec;   22.5 MB/s\nfillseq [AVG 7 runs] : 201318 (\u00b1 1595) ops/sec;   22.3 (\u00b1 0.2) MB/sec\nfillseq      :       4.998 micros/op 200074 ops/sec;   22.1 MB/s\nfillseq [AVG 8 runs] : 201163 (\u00b1 1415) ops/sec;   22.3 (\u00b1 0.2) MB/sec\nfillseq      :       4.946 micros/op 202188 ops/sec;   22.4 MB/s\nfillseq [AVG 9 runs] : 201277 (\u00b1 1267) ops/sec;   22.3 (\u00b1 0.1) MB/sec\nfillseq      :       5.093 micros/op 196331 ops/sec;   21.7 MB/s\nfillseq [AVG 10 runs] : 200782 (\u00b1 1491) ops/sec;   22.2 (\u00b1 0.2) MB/sec\nfillseq [AVG    10 runs] : 200782 (\u00b1 1491) ops/sec;   22.2 (\u00b1 0.2) MB/sec\nfillseq [MEDIAN 10 runs] : 201166 ops/sec;   22.3 MB/s\n```\n\nFor more explicit interval representation, use `--confidence_interval_only` flag:\n\n```\n$ ./db_bench --benchmarks=fillseq[-X10] --confidence_interval_only\n\nRunning benchmark for 10 times\nfillseq      :       4.935 micros/op 202648 ops/sec;   22.4 MB/s\nfillseq      :       5.078 micros/op 196943 ops/sec;   21.8 MB/s\nfillseq [CI95 2 runs] : (194205, 205385) ops/sec; (21.5, 22.7) MB/sec\nfillseq      :       5.159 micros/op 193816 ops/sec;   21.4 MB/s\nfillseq [CI95 3 runs] : (192735, 202869) ops/sec; (21.3, 22.4) MB/sec\nfillseq      :       4.947 micros/op 202158 ops/sec;   22.4 MB/s\nfillseq [CI95 4 runs] : (194721, 203061) ops/sec; (21.5, 22.5) MB/sec\nfillseq      :       4.908 micros/op 203756 ops/sec;   22.5 MB/s\nfillseq [CI95 5 runs] : (196113, 203615) ops/sec; (21.7, 22.5) MB/sec\nfillseq      :       5.063 micros/op 197528 ops/sec;   21.9 MB/s\nfillseq [CI95 6 runs] : (196319, 202631) ops/sec; (21.7, 22.4) MB/sec\nfillseq      :       5.214 micros/op 191799 ops/sec;   21.2 MB/s\nfillseq [CI95 7 runs] : (194953, 201803) ops/sec; (21.6, 22.3) MB/sec\nfillseq      :       5.260 micros/op 190095 ops/sec;   21.0 MB/s\nfillseq [CI95 8 runs] : (193749, 200937) ops/sec; (21.4, 22.2) MB/sec\nfillseq      :       5.076 micros/op 196992 ops/sec;   21.8 MB/s\nfillseq [CI95 9 runs] : (194134, 200474) ops/sec; (21.5, 22.2) MB/sec\nfillseq      :       5.388 micros/op 185603 ops/sec;   20.5 MB/s\nfillseq [CI95 10 runs] : (192487, 199781) ops/sec; (21.3, 22.1) MB/sec\nfillseq [AVG    10 runs] : 196134 (\u00b1 3647) ops/sec;   21.7 (\u00b1 0.4) MB/sec\nfillseq [MEDIAN 10 runs] : 196968 ops/sec;   21.8 MB/sec\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9882\n\nReviewed By: pdillinger\n\nDifferential Revision: D35796148\n\nPulled By: vanekjar\n\nfbshipit-source-id: 8313712d16728ff982b8aff28195ee56622385b8",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fb9a167a55e0970b1ef6f67c1600c8d9c4c6114f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a5063c89311a6cea0babca3abe809dbade866b0b",
        "author": "Hui Xiao",
        "date": "2022-04-19T19:02:00-07:00",
        "message": "Fix issue of opening too many files in BlockBasedTableReaderCapMemoryTest.CapMemoryUsageUnderCacheCapacity (#9869)\n\nSummary:\n**Context:**\nUnit test for https://github.com/facebook/rocksdb/pull/9748 keeps opening new files to see whether the new feature is able to correctly constrain the opening based on block cache capacity.\n\nHowever, the unit test has two places written inefficiently that can lead to opening too many new files relative to underlying operating system/file system constraint, even before hitting the block cache capacity:\n(1) [opened_table_reader_num < 2 * max_table_reader_num](https://github.com/facebook/rocksdb/pull/9748/files?show-viewed-files=true&file-filters%5B%5D=#diff-ec9f5353e317df71093094734ba29193b94a998f0f9c9af924e4c99692195eeaR438), which can leads to 1200 + open files because of (2) below\n(2) NewLRUCache(6 * CacheReservationManagerImpl<CacheEntryRole::kBlockBasedTableReader>::GetDummyEntrySize()) in [here](https://github.com/facebook/rocksdb/pull/9748/files?show-viewed-files=true&file-filters%5B%5D=#diff-ec9f5353e317df71093094734ba29193b94a998f0f9c9af924e4c99692195eeaR364)\n\nTherefore we see CI failures like this on machine with a strict open file limit ~1000 (see the \"table_1021\" naming in following error msg)\nhttps://app.circleci.com/pipelines/github/facebook/rocksdb/12886/workflows/75524682-3fa4-41ee-9a61-81827b51d99b/jobs/345270\n```\nfs_->NewWritableFile(path, foptions, &file, nullptr)\nIO error: While open a file for appending: /dev/shm/rocksdb.Jedwt/run-block_based_table_reader_test-CapMemoryUsageUnderCacheCapacity-BlockBasedTableReaderCapMemoryTest.CapMemoryUsageUnderCacheCapacity-0/block_based_table_reader_test_1668910_829492452552920927/**table_1021**: Too many open files\n```\n\n**Summary:**\n- Revised the test more efficiently on the above 2 places,  including using 1.1 instead 2 in the threshold and lowering down the block cache capacity a bit\n- Renamed some variables for clarity\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9869\n\nTest Plan:\n- Manual inspection of max opened table reader in all test case, which is around ~389\n- Circle CI to see if error is gone\n\nReviewed By: ajkr\n\nDifferential Revision: D35752655\n\nPulled By: hx235\n\nfbshipit-source-id: 8a0953d39d561babfa4257b8ed8550bb21b04839",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_reader_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a5063c89311a6cea0babca3abe809dbade866b0b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f3bcac39a601585dadbf27f632522bb48d622d13",
        "author": "Akanksha Mahajan",
        "date": "2022-04-11T10:56:11-07:00",
        "message": "Fix stress test failure in ReadAsync. (#9824)\n\nSummary:\nFix stress test failure in ReadAsync by ignoring errors\ninjected during async read by FaultInjectionFS.\nFailure:\n```\n WARNING: prefix_size is non-zero but memtablerep != prefix_hash\nDidn't get expected error from MultiGet.\nnum_keys 14 Expected 1 errors, seen 0\nCallstack that injected the fault\nInjected error type = 32538\nMessage: error;\n#0   ./db_stress() [0x6f7dd4] rocksdb::port::SaveStack(int*, int)\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/port/stack_trace.cc:152\nhttps://github.com/facebook/rocksdb/issues/1   ./db_stress() [0x7f2bda] rocksdb::FaultInjectionTestFS::InjectThreadSpecificReadError(rocksdb::FaultInjectionTestFS::ErrorOperation, rocksdb::Slice*, bool, char*, bool, bool*)\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/utilities/fault_injection_fs.cc:891\nhttps://github.com/facebook/rocksdb/issues/2   ./db_stress() [0x7f2e78] rocksdb::TestFSRandomAccessFile::Read(unsigned long, unsigned long, rocksdb::IOOptions const&, rocksdb::Slice*, char*, rocksdb::IODebugContext*) const\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/utilities/fault_injection_fs.cc:367\nhttps://github.com/facebook/rocksdb/issues/3   ./db_stress() [0x6483d7] rocksdb::(anonymous namespace)::CompositeRandomAccessFileWrapper::Read(unsigned long, unsigned long, rocksdb::Slice*, char*) const\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/env/composite_env.cc:61\nhttps://github.com/facebook/rocksdb/issues/4   ./db_stress() [0x654564] rocksdb::(anonymous namespace)::LegacyRandomAccessFileWrapper::Read(unsigned long, unsigned long, rocksdb::IOOptions const&, rocksdb::Slice*, char*, rocksdb::IODebugContext*) const\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/env/env.cc:152\nhttps://github.com/facebook/rocksdb/issues/5   ./db_stress() [0x659b3b] rocksdb::FSRandomAccessFile::ReadAsync(rocksdb::FSReadRequest&, rocksdb::IOOptions const&, std::function<void (rocksdb::FSReadRequest const&, void*)>, void*, void**, std::function<void (void*)>*, rocksdb::IODebugContext*)\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/./include/rocksdb/file_system.h:896\nhttps://github.com/facebook/rocksdb/issues/6   ./db_stress() [0x8b8bab] rocksdb::RandomAccessFileReader::ReadAsync(rocksdb::FSReadRequest&, rocksdb::IOOptions const&, std::function<void (rocksdb::FSReadRequest const&, void*)>, void*, void**, std::function<void (void*)>*, rocksdb::Env::IOPriority)\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/file/random_access_file_reader.cc:459\nhttps://github.com/facebook/rocksdb/issues/7   ./db_stress() [0x8b501f] rocksdb::FilePrefetchBuffer::ReadAsync(rocksdb::IOOptions const&, rocksdb::RandomAccessFileReader*, rocksdb::Env::IOPriority, unsigned long, unsigned long, unsigned long, unsigned int)\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/file/file_prefetch_buffer.cc:124\nhttps://github.com/facebook/rocksdb/issues/8   ./db_stress() [0x8b55fc] rocksdb::FilePrefetchBuffer::PrefetchAsync(rocksdb::IOOptions const&, rocksdb::RandomAccessFileReader*, unsigned long, unsigned long, unsigned long, rocksdb::Env::IOPriority, bool&)\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/file/file_prefetch_buffer.cc:363\nhttps://github.com/facebook/rocksdb/issues/9   ./db_stress() [0x8b61f8] rocksdb::FilePrefetchBuffer::TryReadFromCacheAsync(rocksdb::IOOptions const&, rocksdb::RandomAccessFileReader*, unsigned long, unsigned long, rocksdb::Slice*, rocksdb::Status*, rocksdb::Env::IOPriority, bool)\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/file/file_prefetch_buffer.cc:482\nhttps://github.com/facebook/rocksdb/issues/10  ./db_stress() [0x745e04] rocksdb::BlockFetcher::TryGetFromPrefetchBuffer()\t/data/sandcastle/boxes/trunk-hg-fbcode-fbsource/fbcode/internal_repo_rocksdb/repo/table/block_fetcher.cc:76\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9824\n\nTest Plan:\n```\n./db_stress --acquire_snapshot_one_in=10000 --adaptive_readahead=1 --allow_concurrent_memtable_write=0 --async_io=1 --atomic_flush=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=0 -- backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=0 --block_size=16384 --bloom_bits=5.037629726741734 --bottommost_compression_type=lz4hc --cache_index_and_filter_blocks=0 --cache_size=8388608 --checkpoint_one_in=1000000 --checksum_type=kxxHash --clear_column_family_one_in=0 --column_families=1 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_ttl=100 --compression_max_dict_buffer_bytes=1073741823 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=zstd --compression_zstd_max_train_bytes=0 --continuous_verification_interval=0 --db=/home/akankshamahajan/dev/shm/rocksdb/rocksdb_crashtest_blackbox --db_write_buffer_size=8388608 --delpercent=0 --delrangepercent=0 --destroy_db_initially=0 - detect_filter_construct_corruption=1 --disable_wal=1 --enable_compaction_filter=0 --enable_pipelined_write=0 --expected_values_dir=/home/akankshamahajan/dev/shm/rocksdb/rocksdb_crashtest_expected --experimental_mempurge_threshold=8.772789063014715 --fail_if_options_file_error=0 --file_checksum_impl=crc32c --flush_one_in=1000000 --format_version=3 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=15 --index_type=3 --iterpercent=0 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=False --long_running_snapshots=0 --mark_for_compaction_one_file_in=0 --max_background_compactions=1 --max_bytes_for_level_base=67108864 --max_key=25000000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=16777216 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=2097152 --memtable_prefix_bloom_size_ratio=0.001 --memtable_whole_key_filtering=1 --memtablerep=skip_list --mmap_read=0 --mock_direct_io=True --nooverwritepercent=1 --open_files=-1 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=0 --open_write_fault_one_in=0 --ops_per_thread=100000000 --optimize_filters_for_memory=0 --paranoid_file_checks=1 --partition_filters=0 --partition_pinning=2 --pause_background_one_in=1000000 --periodic_compaction_seconds=1000 --prefix_size=-1 --prefixpercent=0 --prepopulate_block_cache=0 --progress_reports=0 --read_fault_one_in=32 --readpercent=100 --recycle_log_file_num=1 --reopen=0 --reserve_table_reader_memory=1 --ribbon_starting_level=999 --secondary_cache_fault_one_in=0 --set_options_one_in=0 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=0 --sst_file_manager_bytes_per_truncate=0 --subcompactions=2 --sync=0 --sync_fault_injection=False --target_file_size_base=16777216 --target_file_size_multiplier=1 --test_batches_snapshots=0 --top_level_index_pinning=3 --unpartitioned_pinning=2 --use_block_based_filter=0 --use_clock_cache=0 --use_direct_io_for_flush_and_compaction=1 --use_direct_reads=0 --use_full_merge_v1=0 --use_merge=1 --use_multiget=1 --user_timestamp_size=0 --value_size_mult=32 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --wal_compression=none --write_buffer_size=33554432 --write_dbid_to_manifest=1 --write_fault_one_in=0 --writepercent=0\n```\n\nReviewed By: anand1976\n\nDifferential Revision: D35514566\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: e2a868fdd7422604774c1419738f9926a21e92a4",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f3bcac39a601585dadbf27f632522bb48d622d13",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3fc2eaf5612de7077446a55a00ba80744ccd0ce6",
        "author": "Akanksha Mahajan",
        "date": "2022-04-07T10:31:50-07:00",
        "message": "Fix valgrind test failure for async read (#9819)\n\nSummary:\nSince all plaftorms don't support io_uring. So updated the unit\ntest to take that into consideration when testing async reads in unit tests.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9819\n\nTest Plan:\nvalgrind --error-exitcode=2 --leak-check=full ./prefetch_test\n--gtest_filter=PrefetchTest2.ReadAsyncWithPosixFS\nCircleCI jobs\n\nReviewed By: pdillinger\n\nDifferential Revision: D35469959\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: b170459ec816487fc0a13b1d55dbbe4f754b2eba",
        "modified_files_count": 1,
        "modified_files": [
            "file/prefetch_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3fc2eaf5612de7077446a55a00ba80744ccd0ce6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7ea26abb8b8625ccd4916f95fbd9466c8947f5bd",
        "author": "Akanksha Mahajan",
        "date": "2022-04-06T18:36:23-07:00",
        "message": "Fix reseting of async_read_in_progress_ variable in FilePrefetchBuffer to call Poll API (#9815)\n\nSummary:\nCurrently RocksDB reset async_read_in_progress_ in callback\ndue to which underlying filesystem relying on Poll API won't be called\nleading to stale memory access.\nIn order to fix it, async_read_in_progress_ will be reset after Poll API\nis called to make sure underlying file_system waiting on Poll can clear\nits state or take appropriate action.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9815\n\nTest Plan: CircleCI tests\n\nReviewed By: anand1976\n\nDifferential Revision: D35451534\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: b70ef6251a7aa9ed4876ba5e5100baa33d7d474c",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7ea26abb8b8625ccd4916f95fbd9466c8947f5bd",
        "contains_optimization_keyword": true
    },
    {
        "hash": "33f8a08af22620702e81ad3b16deec04f5690992",
        "author": "Akanksha Mahajan",
        "date": "2022-03-25T18:26:22-07:00",
        "message": "Fix some errors in async prefetching in FilePrefetchBuffer (#9734)\n\nSummary:\nIn ReadOption `async_io` which prefetches the data asynchronously, db_bench and db_stress runs were failing  because wrong data was prefetched which resulted in Error: Checksum mismatched. Wrong data was copied because capacity was less than actual size needed. It has been fixed in this PR.\n\nSince there are two separate methods for async and sync prefetching, these changes are in async prefetching methods and any changes would not effect normal prefetching. I ran the regressions to make sure normal prefetching is fine.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9734\n\nTest Plan:\n1. CircleCI jobs\n\n2.  Ran db_bench\n```\n. /db_bench -use_existing_db=true\n-db=/tmp/prefix_scan_prefetch_main -benchmarks=\"seekrandom\" -key_size=32\n-value_size=512 -num=5000000 -use_direct_reads=true -seek_nexts=327680\n-duration=120 -ops_between_duration_checks=1 -async_io=1 -adaptive_readahead=1\n\n```\n3. Ran db_stress test\n```\nexport CRASH_TEST_EXT_ARGS=\" --async_io=1 --adaptive_readahead=1\"\nmake crash_test -j\n```\n\n4. Run regressions for async_io disabled.\n\nOld flow without any async changes:\n```\n./db_bench -use_existing_db=true -db=/tmp/prefix_scan_prefetch_main -benchmarks=\"seekrandom\" -key_size=32 -value_size=512 -num=5000000 -use_direct_reads=true -seek_nexts=327680 -duration=120 -ops_between_duration_checks=1\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nRocksDB:    version 7.0\nDate:       Thu Mar 17 13:11:34 2022\nCPU:        24 * Intel Core Processor (Broadwell)\nCPUCache:   16384 KB\nKeys:       32 bytes each (+ 0 bytes user-defined timestamp)\nValues:     512 bytes each (256 bytes after compression)\nEntries:    5000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    2594.0 MB (estimated)\nFileSize:   1373.3 MB (estimated)\nWrite rate: 0 bytes/second\nRead rate: 0 ops/second\nCompression: Snappy\nCompression sampling rate: 0\nMemtablerep: SkipListFactory\nPerf Level: 1\n------------------------------------------------\nDB path: [/tmp/prefix_scan_prefetch_main]\nseekrandom   :  483618.390 micros/op 2 ops/sec;  338.9 MB/s (249 of 249 found)\n```\n\nWith async prefetching changes and async_io disabled to make sure in normal prefetching there is no regression.\n ```\n ./db_bench -use_existing_db=true -db=/tmp/prefix_scan_prefetch_main -benchmarks=\"seekrandom\" -key_size=32 -value_size=512 -num=5000000 -use_direct_reads=true -seek_nexts=327680 -duration=120 -ops_between_duration_checks=1 --async_io=0\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nRocksDB:    version 7.1\nDate:       Wed Mar 23 15:56:37 2022\nCPU:        24 * Intel Core Processor (Broadwell)\nCPUCache:   16384 KB\nKeys:       32 bytes each (+ 0 bytes user-defined timestamp)\nValues:     512 bytes each (256 bytes after compression)\nEntries:    5000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    2594.0 MB (estimated)\nFileSize:   1373.3 MB (estimated)\nWrite rate: 0 bytes/second\nRead rate: 0 ops/second\nCompression: Snappy\nCompression sampling rate: 0\nMemtablerep: SkipListFactory\nPerf Level: 1\n------------------------------------------------\nDB path: [/tmp/prefix_scan_prefetch_main]\nseekrandom   :  481819.816 micros/op 2 ops/sec;  340.2 MB/s (250 of 250 found)\n```\n\nReviewed By: riversand963\n\nDifferential Revision: D35058471\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: 9233a1e6d97cea0c7a8111bfb9e8ac3251c341ce",
        "modified_files_count": 1,
        "modified_files": [
            "file/file_prefetch_buffer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/33f8a08af22620702e81ad3b16deec04f5690992",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1a130fa3c11ac9b93cd138ec28bb319ce37326b1",
        "author": "Mark Callaghan",
        "date": "2022-03-25T10:12:27-07:00",
        "message": "db_bench should use a good seed when --seed is not set or set to 0 (#9740)\n\nSummary:\nThis is for https://github.com/facebook/rocksdb/issues/9737\n\nI have wasted more than a few hours running db_bench benchmarks where --seed was not set\nand getting better than expected results because cache hit rates are great because\nmultiple invocations of db_bench used the same value for --seed or did not set it,\nand then all used 0. The result is that all see the same sequence of keys.\n\nOthers have done the same. The problem is worse in that it is easy to miss and the result is a benchmark with results that are misleading.\n\nA good way to avoid this is to set it to the equivalent of gettimeofday() when either\n--seed is not set or it is set to 0 (the default).\n\nWith this change the actual seed is printed when it was 0 at process start:\n  Set seed to 1647992570365606 because --seed was 0\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9740\n\nTest Plan:\nPerf results:\n\n./db_bench --benchmarks=fillseq,readrandom --num=1000000 --reads=4000000\n  readrandom   :       6.469 micros/op 154583 ops/sec;   17.1 MB/s (4000000 of 4000000 found)\n\n./db_bench --benchmarks=fillseq,readrandom --num=1000000 --reads=4000000 --seed=0\n  readrandom   :       6.565 micros/op 152321 ops/sec;   16.9 MB/s (4000000 of 4000000 found)\n\n./db_bench --benchmarks=fillseq,readrandom --num=1000000 --reads=4000000 --seed=1\n  readrandom   :       6.461 micros/op 154777 ops/sec;   17.1 MB/s (4000000 of 4000000 found)\n\n./db_bench --benchmarks=fillseq,readrandom --num=1000000 --reads=4000000 --seed=2\n  readrandom   :       6.525 micros/op 153244 ops/sec;   17.0 MB/s (4000000 of 4000000 found)\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D35145361\n\nPulled By: mdcallag\n\nfbshipit-source-id: 2b35b153ccec46b27d7c9405997523555fc51267",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1a130fa3c11ac9b93cd138ec28bb319ce37326b1",
        "contains_optimization_keyword": true
    },
    {
        "hash": "409635cb2a0f607b9c3687f4b3657edede0160ec",
        "author": "Mark Callaghan",
        "date": "2022-03-24T13:39:01-07:00",
        "message": "Add --slow_usecs option to determine when long op message is printed (#9732)\n\nSummary:\nThis adds the --slow_usecs option with a default value of 1M. Operations that\ntake this much time have a message printed when --histogram=1, --stats_interval=0\nand --stats_interval_seconds=0. The current code hardwired this to 20,000 usecs\nand for some stress tests that reduced throughput by 20% or more.\n\nThis is for https://github.com/facebook/rocksdb/issues/9620\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9732\n\nTest Plan:\n./db_bench --benchmarks=fillrandom,readrandom --compression_type=lz4 --slow_usecs=100 --histogram=1\n./db_bench --benchmarks=fillrandom,readrandom --compression_type=lz4 --slow_usecs=100000 --histogram=1\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D35121522\n\nPulled By: mdcallag\n\nfbshipit-source-id: daf27f937efd748980545d6395db332712fc078b",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/409635cb2a0f607b9c3687f4b3657edede0160ec",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f07eec1bf88f0a8250e9e0ecbd01946bd767e0d9",
        "author": "Akanksha Mahajan",
        "date": "2022-03-22T17:21:35-07:00",
        "message": "Add async_io read option in db_bench (#9735)\n\nSummary:\nAdd async_io Read option in db_bench\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9735\n\nTest Plan:\n./db_bench -use_existing_db=true\n-db=/tmp/prefix_scan_prefetch_main -benchmarks=\"seekrandom\" -key_size=32\n-value_size=512 -num=5000000 -use_direct_reads=true -seek_nexts=327680\n-duration=120 -ops_between_duration_checks=1 -async_io=1\n\nReviewed By: riversand963\n\nDifferential Revision: D35058482\n\nPulled By: akankshamahajan15\n\nfbshipit-source-id: 1522b638c79f6d85bb7408c67f6ab76dbabeeee7",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f07eec1bf88f0a8250e9e0ecbd01946bd767e0d9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1ca1562e3565ac3d9ccfeeec2e206a21791f3aa3",
        "author": "Mark Callaghan",
        "date": "2022-03-21T17:30:51-07:00",
        "message": "Make mixgraph easier to use (#9711)\n\nSummary:\nChanges:\n* improves monitoring by displaying average size of a Put value and average scan length\n* forces the minimum value size to be 10. Before this it was 0 if you didn't set the distribution parameters.\n* uses reasonable defaults for the distribution parameters that determine value size and scan length\n* includes seeks in \"reads ... found\" message, before this they were missing\n\nThis is for https://github.com/facebook/rocksdb/issues/9672\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9711\n\nTest Plan:\nBefore this change:\n\n./db_bench --benchmarks=fillseq,mixgraph --mix_get_ratio=50 --mix_put_ratio=25 --mix_seek_ratio=25 --num=100000 --value_k=0.2615 --value_sigma=25.45 --iter_k=2.517 --iter_sigma=14.236\nfillseq      :       4.289 micros/op 233138 ops/sec;   25.8 MB/s\nmixgraph     :      18.461 micros/op 54166 ops/sec;  755.0 MB/s ( Gets:50164 Puts:24919 Seek:24917 of 50164 in 75081 found)\n\nAfter this change:\n\n./db_bench --benchmarks=fillseq,mixgraph --mix_get_ratio=50 --mix_put_ratio=25 --mix_seek_ratio=25 --num=100000 --value_k=0.2615 --value_sigma=25.45 --iter_k=2.517 --iter_sigma=14.236\nfillseq      :       3.974 micros/op 251553 ops/sec;   27.8 MB/s\nmixgraph     :      16.722 micros/op 59795 ops/sec;  833.5 MB/s ( Gets:50164 Puts:24919 Seek:24917, reads 75081 in 75081 found, avg size: 36.0 value, 504.9 scan)\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D35030190\n\nPulled By: mdcallag\n\nfbshipit-source-id: d8f555f28d869f752ddb674a524108884511b151",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1ca1562e3565ac3d9ccfeeec2e206a21791f3aa3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f0fca81fc6f3f06340eeb2d81e8c92f169a3d515",
        "author": "Jay Zhuang",
        "date": "2022-03-17T13:30:28-07:00",
        "message": "Deflake DeleteSchedulerTest.StartBGEmptyTrashMultipleTimes (#9706)\n\nSummary:\nThe designed sync point may not be hit if trash file is generated faster\nthan deleting. Then the file will be deleted directly instead of waiting\nfor background trash empty thread to do it.\nIncrease SstFileManager Trash/DB ratio to avoid that.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9706\n\nTest Plan:\n`gtest-parallel ./delete_scheduler_test\n--gtest_filter=DeleteSchedulerTest.StartBGEmptyTrashMultipleTimes -r\n10000 -w 100`\nIt was likely to happen on one of the host.\n\nReviewed By: riversand963\n\nDifferential Revision: D34964735\n\nPulled By: jay-zhuang\n\nfbshipit-source-id: bb78015489b5f6b3f11783aae7e5853ea197702c",
        "modified_files_count": 1,
        "modified_files": [
            "file/delete_scheduler_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f0fca81fc6f3f06340eeb2d81e8c92f169a3d515",
        "contains_optimization_keyword": true
    },
    {
        "hash": "0a89cea5f55b6a57036236cc5995a0634c980735",
        "author": "Andrew Kryczka",
        "date": "2022-02-28T23:45:08-08:00",
        "message": "Handle failures in block-based table size/offset approximation (#9615)\n\nSummary:\nIn crash test with fault injection, we were seeing stack traces like the following:\n\n```\nhttps://github.com/facebook/rocksdb/issues/3 0x00007f75f763c533 in __GI___assert_fail (assertion=assertion@entry=0x1c5b2a0 \"end_offset >= start_offset\", file=file@entry=0x1c580a0 \"table/block_based/block_based_table_reader.cc\", line=line@entry=3245,\nfunction=function@entry=0x1c60e60 \"virtual uint64_t rocksdb::BlockBasedTable::ApproximateSize(const rocksdb::Slice&, const rocksdb::Slice&, rocksdb::TableReaderCaller)\") at assert.c:101\nhttps://github.com/facebook/rocksdb/issues/4 0x00000000010ea9b4 in rocksdb::BlockBasedTable::ApproximateSize (this=<optimized out>, start=..., end=..., caller=<optimized out>) at table/block_based/block_based_table_reader.cc:3224\nhttps://github.com/facebook/rocksdb/issues/5 0x0000000000be61fb in rocksdb::TableCache::ApproximateSize (this=0x60f0000161b0, start=..., end=..., fd=..., caller=caller@entry=rocksdb::kCompaction, internal_comparator=..., prefix_extractor=...) at db/table_cache.cc:719\nhttps://github.com/facebook/rocksdb/issues/6 0x0000000000c3eaec in rocksdb::VersionSet::ApproximateSize (this=<optimized out>, v=<optimized out>, f=..., start=..., end=..., caller=<optimized out>) at ./db/version_set.h:850\nhttps://github.com/facebook/rocksdb/issues/7 0x0000000000c6ebc3 in rocksdb::VersionSet::ApproximateSize (this=<optimized out>, options=..., v=v@entry=0x621000047500, start=..., end=..., start_level=start_level@entry=0, end_level=<optimized out>, caller=<optimized out>)\nat db/version_set.cc:5657\nhttps://github.com/facebook/rocksdb/issues/8 0x000000000166e894 in rocksdb::CompactionJob::GenSubcompactionBoundaries (this=<optimized out>) at ./include/rocksdb/options.h:1869\nhttps://github.com/facebook/rocksdb/issues/9 0x000000000168c526 in rocksdb::CompactionJob::Prepare (this=this@entry=0x7f75f3ffcf00) at db/compaction/compaction_job.cc:546\n```\n\nThe problem occurred in `ApproximateSize()` when the index `Seek()` for the first `ApproximateDataOffsetOf()` encountered an I/O error, while the second `Seek()` did not. In the old code that scenario caused `start_offset == data_size` , thus it was easy to trip the assertion that `end_offset >= start_offset`.\n\nThe fix is to set `start_offset == 0` when the first index `Seek()` fails, and `end_offset == data_size` when the second index `Seek()` fails. I doubt these give an \"on average correct\" answer for how this function is used, but I/O errors in index seeks are hopefully rare, it looked consistent with what was already there, and it was easier to calculate.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9615\n\nTest Plan:\nrun the repro command for a while and stopped seeing coredumps -\n\n```\n$ while !  ./db_stress --block_size=128 --cache_size=32768 --clear_column_family_one_in=0 --column_families=1 --continuous_verification_interval=0 --db=/dev/shm/rocksdb_crashtest --delpercent=4 --delrangepercent=1 --destroy_db_initially=0 --expected_values_dir=/dev/shm/rocksdb_crashtest_expected --index_type=2 --iterpercent=10  --kill_random_test=18887 --max_key=1000000 --max_bytes_for_level_base=2048576 --nooverwritepercent=1 --open_files=-1 --open_read_fault_one_in=32 --ops_per_thread=1000000 --prefixpercent=5 --read_fault_one_in=0 --readpercent=45 --reopen=0 --skip_verifydb=1 --subcompactions=2 --target_file_size_base=524288 --test_batches_snapshots=0 --value_size_mult=32 --write_buffer_size=524288 --writepercent=35  ; do : ; done\n```\n\nReviewed By: pdillinger\n\nDifferential Revision: D34383069\n\nPulled By: ajkr\n\nfbshipit-source-id: fac26c3b20ea962e75387515ba5f2724dc48719f",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/0a89cea5f55b6a57036236cc5995a0634c980735",
        "contains_optimization_keyword": true
    },
    {
        "hash": "791723c1ec828baba4d2be825f6eb10541c834d8",
        "author": "Andrew Kryczka",
        "date": "2021-12-20T13:05:23-08:00",
        "message": "Fix race condition in db_stress thread setup (#9314)\n\nSummary:\nWe need to grab `SharedState`'s mutex while calling `IncThreads()` or `IncBgThreads()`. Otherwise the newly launched threads can simultaneously access the thread counters to check if every thread has finished initializing.\n\nRepro command:\n\n```\n$ rm -rf /dev/shm/rocksdb/rocksdb_crashtest_{whitebox,expected}/ && mkdir -p /dev/shm/rocksdb/rocksdb_crashtest_{whitebox,expected}/ && ./db_stress --acquire_snapshot_one_in=10000 --atomic_flush=1 --avoid_flush_during_recovery=0 --avoid_unnecessary_blocking_io=1 --backup_max_size=104857600 --backup_one_in=100000 --batch_protection_bytes_per_key=0 --block_size=16384 --bloom_bits=131.8094496796033 --bottommost_compression_type=zlib --cache_index_and_filter_blocks=1 --cache_size=1048576 --checkpoint_one_in=1000000 --checksum_type=kCRC32c --clear_column_family_one_in=0 --compact_files_one_in=1000000 --compact_range_one_in=1000000 --compaction_style=1 --compaction_ttl=0 --compression_max_dict_buffer_bytes=134217727 --compression_max_dict_bytes=16384 --compression_parallel_threads=1 --compression_type=zstd --compression_zstd_max_train_bytes=65536 --continuous_verification_interval=0 --db=/dev/shm/rocksdb/rocksdb_crashtest_whitebox --db_write_buffer_size=8388608 --delpercent=5 --delrangepercent=0 --destroy_db_initially=0 --disable_wal=1 --enable_compaction_filter=0 --enable_pipelined_write=0 --fail_if_options_file_error=1 --file_checksum_impl=crc32c --flush_one_in=1000000 --format_version=5 --get_current_wal_file_one_in=0 --get_live_files_one_in=1000000 --get_property_one_in=1000000 --get_sorted_wal_files_one_in=0 --index_block_restart_interval=15 --index_type=3 --iterpercent=10 --key_len_percent_dist=1,30,69 --level_compaction_dynamic_level_bytes=True --log2_keys_per_lock=22 --long_running_snapshots=0 --mark_for_compaction_one_file_in=10 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --max_key=1000000 --max_key_len=3 --max_manifest_file_size=1073741824 --max_write_batch_group_size_bytes=1048576 --max_write_buffer_number=3 --max_write_buffer_size_to_maintain=4194304 --memtablerep=skip_list --mmap_read=1 --mock_direct_io=False --nooverwritepercent=1 --open_files=500000 --open_metadata_write_fault_one_in=0 --open_read_fault_one_in=32 --open_write_fault_one_in=0 --ops_per_thread=20000 --optimize_filters_for_memory=1 --paranoid_file_checks=0 --partition_filters=0 --partition_pinning=0 --pause_background_one_in=1000000 --periodic_compaction_seconds=0 --prefixpercent=5 --prepopulate_block_cache=1 --progress_reports=0 --read_fault_one_in=1000 --readpercent=45 --recycle_log_file_num=1 --reopen=0 --ribbon_starting_level=999 --secondary_cache_fault_one_in=32 --snapshot_hold_ops=100000 --sst_file_manager_bytes_per_sec=104857600 --sst_file_manager_bytes_per_truncate=1048576 --subcompactions=2 --sync=0 --sync_fault_injection=False --target_file_size_base=2097152 --target_file_size_multiplier=2 --test_batches_snapshots=1 --test_cf_consistency=1 --top_level_index_pinning=0 --unpartitioned_pinning=0 --use_block_based_filter=1 --use_clock_cache=0 --use_direct_io_for_flush_and_compaction=0 --use_direct_reads=0 --use_full_merge_v1=1 --use_merge=0 --use_multiget=1 --user_timestamp_size=0 --verify_checksum=1 --verify_checksum_one_in=1000000 --verify_db_one_in=100000 --write_buffer_size=1048576 --write_dbid_to_manifest=1 --write_fault_one_in=0 --writepercent=35\n```\n\nTSAN error:\n\n```\nWARNING: ThreadSanitizer: data race (pid=2750142)\n  Read of size 4 at 0x7ffc21d7f58c by thread T39 (mutexes: write M670895590377780496):\n    #0 rocksdb::SharedState::AllInitialized() const db_stress_tool/db_stress_shared_state.h:204 (db_stress+0x4fd307)\n    https://github.com/facebook/rocksdb/issues/1 rocksdb::ThreadBody(void*) db_stress_tool/db_stress_driver.cc:26 (db_stress+0x4fd307)\n    https://github.com/facebook/rocksdb/issues/2 StartThreadWrapper env/env_posix.cc:454 (db_stress+0x84472f)\n\n  Previous write of size 4 at 0x7ffc21d7f58c by main thread:\n    #0 rocksdb::SharedState::IncThreads() db_stress_tool/db_stress_shared_state.h:194 (db_stress+0x4fd779)\n    https://github.com/facebook/rocksdb/issues/1 rocksdb::RunStressTest(rocksdb::StressTest*) db_stress_tool/db_stress_driver.cc:78 (db_stress+0x4fd779)\n    https://github.com/facebook/rocksdb/issues/2 rocksdb::db_stress_tool(int, char**) db_stress_tool/db_stress_tool.cc:348 (db_stress+0x4b97dc)\n    https://github.com/facebook/rocksdb/issues/3 main db_stress_tool/db_stress.cc:21 (db_stress+0x47a351)\n\n  Location is stack of main thread.\n\n  Location is global '<null>' at 0x000000000000 ([stack]+0x00000001d58c)\n\n  Mutex M670895590377780496 is already destroyed.\n\n  Thread T39 (tid=2750211, running) created by main thread at:\n    #0 pthread_create /home/engshare/third-party2/gcc/9.x/src/gcc-10.x/libsanitizer/tsan/tsan_interceptors.cc:964 (libtsan.so.0+0x613c3)\n    https://github.com/facebook/rocksdb/issues/1 StartThread env/env_posix.cc:464 (db_stress+0x8463c2)\n    https://github.com/facebook/rocksdb/issues/2 rocksdb::CompositeEnvWrapper::StartThread(void (*)(void*), void*) env/composite_env_wrapper.h:288 (db_stress+0x4bcd20)\n    https://github.com/facebook/rocksdb/issues/3 rocksdb::EnvWrapper::StartThread(void (*)(void*), void*) include/rocksdb/env.h:1475 (db_stress+0x4bb950)\n    https://github.com/facebook/rocksdb/issues/4 rocksdb::RunStressTest(rocksdb::StressTest*) db_stress_tool/db_stress_driver.cc:80 (db_stress+0x4fd9d2)\n    https://github.com/facebook/rocksdb/issues/5 rocksdb::db_stress_tool(int, char**) db_stress_tool/db_stress_tool.cc:348 (db_stress+0x4b97dc)\n    https://github.com/facebook/rocksdb/issues/6 main db_stress_tool/db_stress.cc:21 (db_stress+0x47a351)\n\n ThreadSanitizer: data race db_stress_tool/db_stress_shared_state.h:204 in rocksdb::SharedState::AllInitialized() const\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9314\n\nTest Plan: verified repro command works after this PR.\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D33217698\n\nPulled By: ajkr\n\nfbshipit-source-id: 79358fe5adb779fc9dcf80643cc102d4b467fc38",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_driver.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/791723c1ec828baba4d2be825f6eb10541c834d8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fc93553e6d2235e41c84c087d515bbd0767ac7aa",
        "author": "Adam Retter",
        "date": "2021-11-09T17:07:03-08:00",
        "message": "Fix an error on GCC 4.8.5 where -Werror=unused-parameter fails (#9144)\n\nSummary:\nBefore this fix compilation with GCC 4.8.5 20150623 (Red Hat 4.8.5-36) would fail with the following error:\n```\nCC       jls/db/db_impl/db_impl.o\nIn file included from ./env/file_system_tracer.h:8:0,\n                 from ./file/random_access_file_reader.h:15,\n                 from ./file/file_prefetch_buffer.h:15,\n                 from ./table/format.h:13,\n                 from ./table/internal_iterator.h:14,\n                 from ./db/pinned_iterators_manager.h:12,\n                 from ./db/range_tombstone_fragmenter.h:15,\n                 from ./db/memtable.h:22,\n                 from ./db/memtable_list.h:16,\n                 from ./db/column_family.h:17,\n                 from ./db/db_impl/db_impl.h:22,\n                 from db/db_impl/db_impl.cc:9:\n./include/rocksdb/file_system.h:108:8: error: unused parameter 'opts'\n[-Werror=unused-parameter]\n struct FileOptions : EnvOptions {\n        ^\ndb/db_impl/db_impl.cc: In member function 'virtual rocksdb::Status\nrocksdb::DBImpl::SetDBOptions(const\nstd::unordered_map<std::basic_string<char>, std::basic_string<char>\n>&)':\ndb/db_impl/db_impl.cc:1230:36: note: synthesized method\n'rocksdb::FileOptions& rocksdb::FileOptions::operator=(const\nrocksdb::FileOptions&)' first required here\n       file_options_for_compaction_ = FileOptions(new_db_options);\n                                    ^\n  CC       jls/db/db_impl/db_impl_compaction_flush.o\ncc1plus: all warnings being treated as errors\nmake[1]: *** [jls/db/db_impl/db_impl.o] Error 1\nmake[1]: *** Waiting for unfinished jobs....\nmake[1]: Leaving directory `/rocksdb-local-build'\nmake: *** [rocksdbjavastatic] Error 2\nMakefile:2222: recipe for target 'rocksdbjavastaticdockerarm64v8' failed\nmake: *** [rocksdbjavastaticdockerarm64v8] Error 2\n```\n\nThis was detected on both ppc64le and arm64v8, however it does not seem to appear in the same GCC 4.8 version we use for x64 in CircleCI - https://app.circleci.com/pipelines/github/facebook/rocksdb/9691/workflows/c2a94367-14f3-4039-be95-325c34643d41/jobs/227906\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9144\n\nReviewed By: riversand963\n\nDifferential Revision: D32290770\n\nfbshipit-source-id: c90a54ba2a618e1ff3660fff3f3368ab36c3c527",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/file_system.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fc93553e6d2235e41c84c087d515bbd0767ac7aa",
        "contains_optimization_keyword": true
    },
    {
        "hash": "28bab0ef7ebe4e915cff9adc93df98e15ccf4a74",
        "author": "Siying Dong",
        "date": "2021-11-04T16:38:09-07:00",
        "message": "Improve comments on options.writable_file_max_buffer_size (#9131)\n\nSummary:\nComments of options.writable_file_max_buffer_size mentioned Windows, which is confusing. Remove it.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9131\n\nReviewed By: anand1976\n\nDifferential Revision: D32187003\n\nfbshipit-source-id: 1f134d7ecdc4a9d13825d461ab1da56769b9455f",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/28bab0ef7ebe4e915cff9adc93df98e15ccf4a74",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a5ec5e3ea01a0dfffc4863666e29784ce21a1592",
        "author": "hx235",
        "date": "2021-11-01T14:28:09-07:00",
        "message": "Minor improvement to #8428 (Account for dictionary-building buffer in global memory limit) (#9032)\n\nSummary:\nSummary/Context:\n- Renamed `cache_rev_mng` to `compression_dict_buffer_cache_res_mgr`\n   - It is to distinguish with other potential `cache_res_mgr` in `BlockBasedTableBuilder` and to use correct short-hand for the words \"reservation\", \"manager\"\n- Added `table_options.block_cache == nullptr` in additional to `table_options.no_block_cache == true` to be conditions where we don't create a `CacheReservationManager`\n   - Theoretically `table_options.no_block_cache == true` is equivalent to `table_options.block_cache == nullptr` by API. But since segment fault will be generated by passing `nullptr` into `CacheReservationManager`'s constructor, it does not hurt to directly verify  `table_options.block_cache != nullptr` before passing in\n- Renamed `is_cache_full` to `exceeds_global_block_cache_limit`\n   - It is to hide implementation detail of cache reservation and to emphasize on the concept/design intent of caping memory within global block cache limit\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9032\n\nTest Plan: - Passing existing tests\n\nReviewed By: ajkr\n\nDifferential Revision: D32005807\n\nPulled By: hx235\n\nfbshipit-source-id: 619fd17bb924199de3db5924d8ab7dae53b1efa2",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a5ec5e3ea01a0dfffc4863666e29784ce21a1592",
        "contains_optimization_keyword": true
    },
    {
        "hash": "8f4f302316017fd89a2dbf4f965b797b8a85210e",
        "author": "Yanqin Jin",
        "date": "2021-10-31T22:08:48-07:00",
        "message": "Attempt to deflake DBFlushTest.FireOnFlushCompletedAfterCommittedResult (#9083)\n\nSummary:\nDBFlushTest.FireOnFlushCompletedAfterCommittedResult uses test sync\npoints to coordinate interleaving of different threads. Before this PR,\nthe test writes some data to memtable, triggers a manual flush, and\ntriggers a second manual flush after a first bg flush thread starts\nexecuting. Though unlikely, it is possible for the second bg flush\nthread to run faster than the first bg flush thread and deques flush\nqueue first. In this case, the original test will fail.\nThe fix is to wait until the first bg flush thread deques the flush\nqueue before triggering second manual flush.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9083\n\nTest Plan: ./db_flush_test --gtest_filter=DBFlushTest.FireOnFlushCompletedAfterCommittedResult\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D31951239\n\nPulled By: riversand963\n\nfbshipit-source-id: f32d7cdabe6ad6808fd18e54e663936dc0a9edb4",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_flush_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/8f4f302316017fd89a2dbf4f965b797b8a85210e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "44d04582cbcbf1af87e04c077f2eacca2fe13ae5",
        "author": "Levi Tamasi",
        "date": "2021-10-29T17:47:02-07:00",
        "message": "Aggregate blob file related changes in VersionBuilder as VersionEdits are applied (#9085)\n\nSummary:\nThe current VersionBuilder code on mainline keeps track of blob file related\nchanges (\"delta\") induced by a series of `VersionEdit`s in the form of\n`BlobFileMetaDataDelta` objects. Specifically, `BlobFileMetaDataDelta`\ncontains the amount of additional garbage generated by compactions, as well\nas the set of newly linked/unlinked SSTs. This is very handy for detecting trivial moves,\nsince in that case the newly linked and unlinked SSTs cancel each other out.\nHowever, this representation does not allow us to easily tell whether a certain\nblob file is obsolete after applying a set of `VersionEdit`s or not. In order to\nsolve this issue, the patch introduces `MutableBlobFileMetaData`, which, in addition\nto the delta, also contains the materialized state after applying a set of version edits\n(i.e. the total amount of garbage and the resulting set of linked SSTs). This will\nenable us to add further consistency checks and to improve certain pieces of\nfunctionality where knowing up front which blob files get obsoleted is beneficial.\n(Note: this patch is just the refactoring part; I plan to create separate PRs for\nthe enhancements.)\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9085\n\nTest Plan: Ran `make check` and the stress tests in BlobDB mode.\n\nReviewed By: riversand963\n\nDifferential Revision: D31980867\n\nPulled By: ltamasi\n\nfbshipit-source-id: cc4286778b10900af720423d6b772c77f28a93e3",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/44d04582cbcbf1af87e04c077f2eacca2fe13ae5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f053851af643755dc2ee252f92e3853b30a12be3",
        "author": "sdong",
        "date": "2021-10-19T12:48:18-07:00",
        "message": "Ignore non-overlapping levels when determinig grandparent files (#9051)\n\nSummary:\nRight now, when picking a compaction, grand parent files are from output_level + 1. This usually works, but if the level doesn't have any overlapping file, it will be more efficient to go further down. This is because the files are likely to be trivial moved further and might create a violation of max_compaction_bytes. This situation can naturally happen and might happen even more with TTL compactions. There is no harm to fix it.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/9051\n\nTest Plan: Run existing tests and see it passes. Also briefly run crash test.\n\nReviewed By: ajkr\n\nDifferential Revision: D31748829\n\nfbshipit-source-id: 52b99ab4284dc816d22f34406d528a3c98ff6719",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction/compaction_picker.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f053851af643755dc2ee252f92e3853b30a12be3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "65411b8d4e57e1888ccdc1eb67f6350a832867c8",
        "author": "Hui Xiao",
        "date": "2021-09-17T09:23:31-07:00",
        "message": "Improve rate_limiter_test.cc (#8904)\n\nSummary:\n- Fixed a bug in `RateLimiterTest.GeneratePriorityIterationOrder` that the callbacks in this test were not called starting from `i = 1`. Fix by increasing `rate_bytes_per_sec` and requested bytes.\n   - The bug is due to the previous `rate_bytes_per_sec` was set too small, resulting in `refill_bytes_per_period`  less than  `kMinRefillBytesPerPeriod`. Hence the actual `refill_bytes_per_period` was equal to `kMinRefillBytesPerPeriod` due to the logic [here](https://github.com/facebook/rocksdb/blob/main/util/rate_limiter.cc#L302-L303)  and it ended up being greater than the previously set requested bytes. Therefore starting from `i = 1`, `RefillBytesAndGrantRequests()` and `GeneratePriorityIterationOrder` won't be called and the test callbacks was not triggered to execute the assertion.\n- Added internal flag to assert callbacks are called in `RateLimiterTest.GeneratePriorityIterationOrder` to prevent any future changes defeat the purpose of the test [as suggested](https://github.com/facebook/rocksdb/pull/8890#discussion_r704915134)\n- Increased `rate_bytes_per_sec` and bytes of each request in `RateLimiterTest.GetTotalBytesThrough`, `RateLimiterTest.GetTotalRequests`, `RateLimiterTest.GetTotalPendingRequests` to trigger the \"long path\" of execution (i.e, the one trigger RefillBytesAndGrantRequests()) to increase test coverage\n   - This increased the running time of the three tests, see test plan for time difference running locally\n- Cleared up sync point effects after each test by calling `SyncPoint::GetInstance()->DisableProcessing();` and `SyncPoint::GetInstance()->ClearAllCallBacks();` in `~RateLimiterTest()` [as suggested](https://github.com/facebook/rocksdb/pull/8595/files#r697534279)\n  - It's fine to call these two methods even when `EnableProcessing()` or `SetCallBack()` is not called in the test or is already cleaned up. In those cases, calling these two functions in destructor is effectively no-op.\n  - This will allow cleaning up sync point effects of previous test even when the previous test failed in assertion.\n- Added missing `SyncPoint::GetInstance()->DisableProcessing();` and `SyncPoint::GetInstance()->ClearCallBacks(..);` in existing tests for completeness\n- Called `SyncPoint::GetInstance()->DisableProcessing();` and `SyncPoint::GetInstance()->ClearCallBacks(..);` in loop in `RateLimiterTest.GeneratePriorityIterationOrder` for completeness\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8904\n\nTest Plan:\n- Passing existing tests\n- To verify the 1st change, run `RateLimiterTest.GeneratePriorityIterationOrder` with assertions of callbacks are indeed called under original `rate_bytes_per_sec` and request byte and under updated `rate_bytes_per_sec` and request byte. The former will fail the assertion while the latter succeeds.\n- Here is the increased test time due to the 3rd change mentioned above in the summary. The relevant 3 tests mentioned in total increase the test time by 6s (~6000/33848 = 17.7% of the original total test time), which IMO is acceptable for better test coverage through running the \"long path\".\n   - current (run on branch rate_limiter_ut_improve locally)\n\n   [ RUN      ] RateLimiterTest.GetTotalBytesThrough\n   [       OK ] RateLimiterTest.GetTotalBytesThrough (3000 ms)\n   [ RUN      ] RateLimiterTest.GetTotalRequests\n   [       OK ] RateLimiterTest.GetTotalRequests (3001 ms)\n   [ RUN      ] RateLimiterTest.GetTotalPendingRequests\n   [       OK ] RateLimiterTest.GetTotalPendingRequests (0 ms)\n   ...\n   [----------] 10 tests from RateLimiterTest (43349 ms total)\n\n   [----------] Global test environment tear-down\n   [==========] 10 tests from 1 test case ran. (43349 ms total)\n   [  PASSED  ] 10 tests.\n\n   - previous (run on branch main locally)\n\n   [ RUN      ] RateLimiterTest.GetTotalBytesThrough\n   [       OK ] RateLimiterTest.GetTotalBytesThrough (0 ms)\n   [ RUN      ] RateLimiterTest.GetTotalRequests\n   [       OK ] RateLimiterTest.GetTotalRequests (0 ms)\n   [ RUN      ] RateLimiterTest.GetTotalPendingRequests\n   [       OK ] RateLimiterTest.GetTotalPendingRequests (0 ms)\n   ...\n   [----------] 10 tests from RateLimiterTest (33848 ms total)\n\n  [----------] Global test environment tear-down\n  [==========] 10 tests from 1 test case ran. (33848 ms total)\n  [  PASSED  ] 10 tests.\n\nReviewed By: ajkr\n\nDifferential Revision: D30872544\n\nPulled By: hx235\n\nfbshipit-source-id: ff894f5c1a4bef70e8e407d53b00be45f776b3e4",
        "modified_files_count": 1,
        "modified_files": [
            "util/rate_limiter_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/65411b8d4e57e1888ccdc1eb67f6350a832867c8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "c268859aaac3bf9d7ee854d9888ae36e89ccab20",
        "author": "anand76",
        "date": "2021-08-04T15:48:54-07:00",
        "message": "Remove corruption error injection in FaultInjectionTestFS (#8616)\n\nSummary:\n```FaultInjectionTestFS``` injects various types of read errors in ```FileSystem``` APIs. One type of error is corruption errors, where data is intentionally corrupted or truncated. There is corresponding validation in db_stress to verify that an injected error results in a user visible Get/MultiGet error. However, for corruption errors, its hard to know when a corruption is supposed to be detected by the user request, due to prefetching and, in case of direct IO, padding. This results in false positives. So remove that functionality.\n\nBlock checksum validation for Get/MultiGet is confined to ```BlockFetcher```, so we don't lose a lot by disabling this since its a small surface area to test.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8616\n\nReviewed By: zhichao-cao\n\nDifferential Revision: D30074422\n\nPulled By: anand1976\n\nfbshipit-source-id: 6a61fac18f95514c15364b75013799ddf83294df",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/fault_injection_fs.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/c268859aaac3bf9d7ee854d9888ae36e89ccab20",
        "contains_optimization_keyword": true
    },
    {
        "hash": "6b4cdacf41fa3565786fc2376210ba59c289e22d",
        "author": "Baptiste Lemaire",
        "date": "2021-07-21T11:33:33-07:00",
        "message": "Add overwrite_probability for filluniquerandom benchmark in db_bench (#8569)\n\nSummary:\nAdd flags `overwrite_probability` and `overwrite_window_size` flag to `db_bench`.\nAdd the possibility of performing a `filluniquerandom` benchmark with an overwrite probability.\nFor each write operation, there is a probability _p_ that the write is an overwrite (_p_=`overwrite_probability`).\nWhen an overwrite is decided, the key is randomly chosen from the last _N_ keys previously inserted into the DB (with _N_=`overwrite_window_size`).\nWhen a pure write is decided, the key inserted into the DB is unique and therefore will not be an overwrite.\nThe `overwrite_window_size` is used so that the user can decide if the overwrite are mostly targeting recently inserted keys (when `overwrite_window_size` is small compared to the total number of writes), or can also target keys inserted \"a long time ago\" (when `overwrite_window_size` is comparable to total number of writes).\nNote that total number of writes = # of unique insertions + # of overwrites.\nNo unit test specifically added.\nLocal testing show the following **throughputs** for `filluniquerandom` with 1M total writes:\n- bypass the code inserts (no `overwrite_probability` flag specified): ~14.0MB/s\n- `overwrite_probability=0.99`, `overwrite_window_size=10`: ~17.0MB/s\n- `overwrite_probability=0.10`, `overwrite_window_size=10`: ~14.0MB/s\n- `overwrite_probability=0.99`, `overwrite_window_size=1M`: ~14.5MB/s\n- `overwrite_probability=0.10`, `overwrite_window_size=1M`: ~14.0MB/s\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8569\n\nReviewed By: pdillinger\n\nDifferential Revision: D29818631\n\nPulled By: bjlemaire\n\nfbshipit-source-id: d472b4ea4e457a4da7c4ee4f14b40cccd6a4587a",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/6b4cdacf41fa3565786fc2376210ba59c289e22d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d5f3b77f23b2fbd887f6af045d5f8785ba4caa9c",
        "author": "Peter Dillinger",
        "date": "2021-07-19T08:10:29-07:00",
        "message": "Add GetMapProperty to db_stress (#8551)\n\nSummary:\nAlready has good coverage for GetProperty and GetIntProperty\nbut this one was missing.\n\nThis should add more confidence to https://github.com/facebook/rocksdb/issues/8538\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8551\n\nTest Plan:\nbrief local run with boosted probability showed no immediate\nissues\n\nReviewed By: siying\n\nDifferential Revision: D29746383\n\nPulled By: pdillinger\n\nfbshipit-source-id: 9f9f525bc1a7607f85e563e33bda1979ef197127",
        "modified_files_count": 1,
        "modified_files": [
            "db_stress_tool/db_stress_test_base.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d5f3b77f23b2fbd887f6af045d5f8785ba4caa9c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b20737709f504ce2d5b619a0e539e9f9b426d47e",
        "author": "Peter (Stig) Edwards",
        "date": "2021-07-01T18:42:19-07:00",
        "message": "Add -report_open_timing to db_bench (#8464)\n\nSummary:\nHello and thanks for RocksDB,\n\nThis PR adds support for ```-report_open_timing true``` to ```db_bench```.\nIt can be useful when tuning RocksDB on filesystem/env with high latencies for file level operations (create/delete/rename...) seen during ```((Optimistic)Transaction)DB::Open```.\n\nSome examples:\n\n```\n> db_bench -benchmarks updaterandom -num 1 -db /dev/shm/db_bench\n> db_bench -benchmarks updaterandom -num 0 -db /dev/shm/db_bench -use_existing_db true -report_open_timing true -readonly true 2>&1 | grep OpenDb\nOpenDb:     3.90133 milliseconds\n> db_bench -benchmarks updaterandom -num 0 -db /dev/shm/db_bench -use_existing_db true -report_open_timing true -use_secondary_db true 2>&1 | grep OpenDb\nOpenDb:     3.33414 milliseconds\n> db_bench -benchmarks updaterandom -num 0 -db /dev/shm/db_bench -use_existing_db true -report_open_timing true 2>&1 | grep -A1 OpenDb\nOpenDb:     6.05423 milliseconds\n\n> db_bench -benchmarks updaterandom -num 1\n> db_bench -benchmarks updaterandom -num 0 -use_existing_db true -report_open_timing true -readonly true 2>&1 | grep OpenDb\nOpenDb:     4.06859 milliseconds\n> db_bench -benchmarks updaterandom -num 0 -use_existing_db true -report_open_timing true -use_secondary_db true 2>&1 | grep OpenDb\nOpenDb:     2.85794 milliseconds\n> db_bench -benchmarks updaterandom -num 0 -use_existing_db true -report_open_timing true 2>&1 | grep OpenDb\nOpenDb:     6.46376 milliseconds\n\n> db_bench -benchmarks updaterandom -num 1 -db /clustered_fs/db_bench\n> db_bench -benchmarks updaterandom -num 0 -db /clustered_fs/db_bench -use_existing_db true -report_open_timing true -readonly true 2>&1 | grep OpenDb\nOpenDb:     3.79805 milliseconds\n> db_bench -benchmarks updaterandom -num 0 -db /clustered_fs/db_bench -use_existing_db true -report_open_timing true -use_secondary_db true 2>&1 | grep OpenDb\nOpenDb:     3.00174 milliseconds\n> db_bench -benchmarks updaterandom -num 0 -db /clustered_fs/db_bench -use_existing_db true -report_open_timing true 2>&1 | grep OpenDb\nOpenDb:     24.8732 milliseconds\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8464\n\nReviewed By: hx235\n\nDifferential Revision: D29398096\n\nPulled By: zhichao-cao\n\nfbshipit-source-id: 8f05dc3284f084612a3f30234e39e1c37548f50c",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b20737709f504ce2d5b619a0e539e9f9b426d47e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "75741eb0cee59ed9b13f144fc541dcdb3a524d37",
        "author": "Peter (Stig) Edwards",
        "date": "2021-06-24T11:56:51-07:00",
        "message": "Add more ops to: db_bench -report_file_operations (#8448)\n\nSummary:\nHello and thanks for RocksDB,\n\nHere is a PR to add file deletes, renames and ```Flush()```, ```Sync()```, ```Fsync()``` and ```Close()``` to file ops report.\n\nThe reason is to help tune RocksDB options when using an env/filesystem with high latencies for file level (\"metadata\") operations, typically seen during ```DB::Open``` (```db_bench -num 0``` also see https://github.com/facebook/rocksdb/pull/7203 where IOTracing does not trace ```DB::Open```).\n\nBefore:\n```\n> db_bench -benchmarks updaterandom -num 0 -report_file_operations true\n...\nEntries:    0\n...\nNum files opened: 12\nNum Read(): 6\nNum Append(): 8\nNum bytes read: 6216\nNum bytes written: 6289\n```\nAfter:\n```\n> db_bench -benchmarks updaterandom -num 0 -report_file_operations true\n...\nEntries:    0\n...\nNum files opened: 12\nNum files deleted: 3\nNum files renamed: 4\nNum Flush(): 10\nNum Sync(): 5\nNum Fsync(): 1\nNum Close(): 2\nNum Read(): 6\nNum Append(): 8\nNum bytes read: 6216\nNum bytes written: 6289\n```\n\nBefore:\n```\n> db_bench -benchmarks updaterandom -report_file_operations true\n...\nEntries:    1000000\n...\nNum files opened: 18\nNum Read(): 396339\nNum Append(): 1000058\nNum bytes read: 892030224\nNum bytes written: 187569238\n```\nAfter:\n```\n> db_bench -benchmarks updaterandom -report_file_operations true\n...\nEntries:    1000000\n...\nNum files opened: 18\nNum files deleted: 5\nNum files renamed: 4\nNum Flush(): 1000068\nNum Sync(): 9\nNum Fsync(): 1\nNum Close(): 6\nNum Read(): 396339\nNum Append(): 1000058\nNum bytes read: 892030224\nNum bytes written: 187569238\n```\n\nAnother example showing how using ```DB::OpenForReadOnly``` reduces file operations compared to ```((Optimistic)Transaction)DB::Open```:\n\n```\n> db_bench -benchmarks updaterandom -num 1\n> db_bench -benchmarks updaterandom -num 0 -use_existing_db true -readonly true -report_file_operations true\n...\nEntries:    0\n...\nNum files opened: 8\nNum files deleted: 0\nNum files renamed: 0\nNum Flush(): 0\nNum Sync(): 0\nNum Fsync(): 0\nNum Close(): 0\nNum Read(): 13\nNum Append(): 0\nNum bytes read: 374\nNum bytes written: 0\n```\n\n```\n> db_bench -benchmarks updaterandom -num 1\n> db_bench -benchmarks updaterandom -num 0 -use_existing_db true -report_file_operations true\n...\nEntries:    0\n...\nNum files opened: 14\nNum files deleted: 3\nNum files renamed: 4\nNum Flush(): 14\nNum Sync(): 5\nNum Fsync(): 1\nNum Close(): 3\nNum Read(): 11\nNum Append(): 10\nNum bytes read: 7291\nNum bytes written: 7357\n```\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8448\n\nReviewed By: anand1976\n\nDifferential Revision: D29333818\n\nPulled By: zhichao-cao\n\nfbshipit-source-id: a06a8c87f799806462319115195b3e94faf5f542",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/75741eb0cee59ed9b13f144fc541dcdb3a524d37",
        "contains_optimization_keyword": true
    },
    {
        "hash": "dfa6b408fea2f39d3403b4d044e558c16b6462c5",
        "author": "sdong",
        "date": "2021-05-26T18:23:03-07:00",
        "message": "Improve comments of iterate_upper_bound (#8331)\n\nSummary:\nReadOptions.iterate_upper_bound's comment is confusing. Improve it.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8331\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D28696635\n\nfbshipit-source-id: 7d9fa6fd1642562572140998c89d434058db8dda",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/dfa6b408fea2f39d3403b4d044e558c16b6462c5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ce0fc71adf5b767694d3c2d7f3125792110f75bf",
        "author": "sdong",
        "date": "2021-05-19T10:28:08-07:00",
        "message": "Minor improvements in env_test (#8317)\n\nSummary:\nFix typo in comments in env_test and add PermitUncheckedError() to two statuses.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8317\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D28525093\n\nfbshipit-source-id: 7a1ed3e45b6f500b8d2ae19fa339c9368111e922",
        "modified_files_count": 1,
        "modified_files": [
            "env/env_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ce0fc71adf5b767694d3c2d7f3125792110f75bf",
        "contains_optimization_keyword": true
    },
    {
        "hash": "cc1c3ee54eace876ad18c39f931e8e5039823930",
        "author": "Saketh Are",
        "date": "2021-04-23T12:45:02-07:00",
        "message": "Eliminate double-buffering of keys in block_based_table_builder (#8219)\n\nSummary:\nThe block_based_table_builder buffers some blocks in memory to construct a good compression dictionary. Before this commit, the keys from each block were buffered separately for convenience. However, the buffered block data implicitly contains all keys. This commit eliminates the redundant key buffers and reduces memory usage.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8219\n\nReviewed By: ajkr\n\nDifferential Revision: D27945851\n\nPulled By: saketh-are\n\nfbshipit-source-id: caf3cac1217201e080a1e24b542bedf20973afee",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based/block_based_table_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/cc1c3ee54eace876ad18c39f931e8e5039823930",
        "contains_optimization_keyword": true
    },
    {
        "hash": "da6b90ab48851c375e90b0e88b36ae43057715df",
        "author": "Peter Dillinger",
        "date": "2021-03-23T21:42:40-07:00",
        "message": "Improve bloom_test bits_per_key flag (#8093)\n\nSummary:\nImproved handling of -bits_per_key other than 10, but at least\nthe OptimizeForMemory test is simply not designed for generally handling\nother settings. (ribbon_test does have a statistical framework for this\nkind of testing, but it's not important to do that same for Bloom right\nnow.)\n\nCloses https://github.com/facebook/rocksdb/issues/7019\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8093\n\nTest Plan: for I in `seq 1 20`; do ./bloom_test --gtest_filter=-*OptimizeForMemory* --bits_per_key=$I &> /dev/null || echo FAILED; done\n\nReviewed By: mrambacher\n\nDifferential Revision: D27275875\n\nPulled By: pdillinger\n\nfbshipit-source-id: 7362e8ac2c41ea11f639412e4f30c8b375f04388",
        "modified_files_count": 1,
        "modified_files": [
            "util/bloom_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/da6b90ab48851c375e90b0e88b36ae43057715df",
        "contains_optimization_keyword": true
    },
    {
        "hash": "063a68b9cd9e66727bde1e59f17aba53c0ae03db",
        "author": "Yanqin Jin",
        "date": "2021-03-18T14:43:34-07:00",
        "message": "Check and handle failure in ldb (#8072)\n\nSummary:\nCurrently, a few ldb commands do not check the execution result of\ndatabase operations. This PR checks the execution results and tries to\nimprove the error reporting.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/8072\n\nTest Plan:\n```\nmake check\n```\nand\n```\nASSERT_STATUS_CHECKED=1 make -j20 ldb\npython tools/ldb_test.py\n```\n\nReviewed By: zhichao-cao\n\nDifferential Revision: D27152466\n\nPulled By: riversand963\n\nfbshipit-source-id: b94220496a4b3591b61c1d350f665860a6579f30",
        "modified_files_count": 1,
        "modified_files": [
            "tools/ldb_cmd.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/063a68b9cd9e66727bde1e59f17aba53c0ae03db",
        "contains_optimization_keyword": true
    },
    {
        "hash": "11c4be2222e9a26c43a965b907b30d4c8e269a67",
        "author": "Yanqin Jin",
        "date": "2020-12-08T02:37:38-08:00",
        "message": "Refactor ProcessManifestWrites a little bit (#7751)\n\nSummary:\nThis PR removes a nested loop inside ProcessManifestWrites. The new\nimplementation has the same behavior as the old code with simpler logic\nand lower complexity.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/7751\n\nTest Plan:\nmake check\nRun make crash_test on devserver and succeeds 3 times.\n\nReviewed By: ltamasi\n\nDifferential Revision: D25363526\n\nPulled By: riversand963\n\nfbshipit-source-id: 27e681949dacd7501a752e5e517b9e85b54ccb2e",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/11c4be2222e9a26c43a965b907b30d4c8e269a67",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1861de455eede00202576bc5432787e9ee50a376",
        "author": "Mammo, Mulugeta",
        "date": "2020-11-16T13:06:30-08:00",
        "message": "Add arena_block_size flag to db_bench (#7654)\n\nSummary:\ndb_bench currently does not allow overriding the default `arena_block_size `calculation ([memtable size/8](https://github.com/facebook/rocksdb/blob/master/db/column_family.cc#L216)). For memtables whose size is in gigabytes, the `arena_block_size` defaults to hundreds of megabytes (affecting performance).\n\nExposing this option in db_bench would allow us to test the workloads with various `arena_block_size` values.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/7654\n\nReviewed By: jay-zhuang\n\nDifferential Revision: D24996812\n\nPulled By: ajkr\n\nfbshipit-source-id: a5e3d2c83d9f89e1bb8382f2e8dd476c79e33bef",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1861de455eede00202576bc5432787e9ee50a376",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b005f9693768a4a33eb5e108f43242cba1e4ecb5",
        "author": "rockeet",
        "date": "2020-09-23T09:53:24-07:00",
        "message": "db_iter.cc: DBIter::Next(): minor improve (#7407)\n\nSummary: Pull Request resolved: https://github.com/facebook/rocksdb/pull/7407\n\nReviewed By: ajkr\n\nDifferential Revision: D23817122\n\nPulled By: jay-zhuang\n\nfbshipit-source-id: 62bf43e4d780fad8c682edd750b4800b5b8f4a77",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_iter.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b005f9693768a4a33eb5e108f43242cba1e4ecb5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2ad88ceae93fe564abad6dea1028dab620db077e",
        "author": "matthewvon",
        "date": "2020-08-17T11:57:47-07:00",
        "message": "Populate cf_id member of CompactionJobInfo for OnCompactionBegin (#6938)\n\nSummary:\nLooks like somebody simply missed initializing a member variable. The column family ID, cf_id, is not set during OnCompactionBegin. But it is set properly in the next function for OnCompactionCompleted. Need this cf_id for tracking progress of a Stardog optimize since there may be multiple compactions required for a given column family.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6938\n\nReviewed By: siying\n\nDifferential Revision: D23153235\n\nPulled By: ajkr\n\nfbshipit-source-id: 932938de3a4ebbc7ac89702f655583862587d251",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl/db_impl_compaction_flush.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2ad88ceae93fe564abad6dea1028dab620db077e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "18efd760c51886d94fc883acd9dfeb6237a08aab",
        "author": "Adam Retter",
        "date": "2020-08-03T14:34:49-07:00",
        "message": "Add defaults to ReadOptions doc (#7215)\n\nSummary:\nVery small improvements to document the defaults.\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/7215\n\nReviewed By: zhichao-cao\n\nDifferential Revision: D22902286\n\nfbshipit-source-id: a754d172a0d8e4c03754f6f1771d4a693d60a770",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/18efd760c51886d94fc883acd9dfeb6237a08aab",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4a60cb20ad384e9a847e945f46493a47691019ae",
        "author": "Jason Volk",
        "date": "2020-07-22T15:03:22-07:00",
        "message": "Fix bug in MultiRead() coalescing introduced in 4fc216649d (#6446). (#6979)\n\nSummary:\nTryMerge() overzealously creates one huge file read request in an attempt to merge smaller disjoint requests. For example, ~30 input requests of ~100 bytes output as 1 request of 100 MiB causing alarmingly large read throughputs to be repeatedly observed by the environment.\n\nSigned-off-by: Jason Volk <jason@zemos.net>\n\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6979\n\nReviewed By: siying\n\nDifferential Revision: D22668892\n\nPulled By: cheng-chang\n\nfbshipit-source-id: 7506fe9621b7f1a747dadf6b8ddb1b1a141c1937",
        "modified_files_count": 1,
        "modified_files": [
            "file/random_access_file_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4a60cb20ad384e9a847e945f46493a47691019ae",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4b107ceb7ebdf0d2b04443f7ced89587adc291d8",
        "author": "Peter Dillinger",
        "date": "2020-07-06T16:17:02-07:00",
        "message": "Improve code comments in EstimateLiveDataSize (#7072)\n\nSummary: Pull Request resolved: https://github.com/facebook/rocksdb/pull/7072\n\nReviewed By: ajkr\n\nDifferential Revision: D22391641\n\nPulled By: pdillinger\n\nfbshipit-source-id: 0ef355576454514263ab684eb1a5c06787f3242a",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4b107ceb7ebdf0d2b04443f7ced89587adc291d8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1551f1011ab419d21ac50be4b894cf3f688b30d7",
        "author": "Levi Tamasi",
        "date": "2020-05-19T10:00:04-07:00",
        "message": "Refactor the blob file related logic in VersionBuilder (#6835)\n\nSummary:\nThis patch is groundwork for an upcoming change to store the set of\nlinked SSTs in `BlobFileMetaData`. With the current code, a new\n`BlobFileMetaData` object is created each time a `VersionEdit` touches\na certain blob file. This is fine as long as these objects are lightweight\nand cheap to create; however, with the addition of the linked SST set, it would\nbe very inefficient since the set would have to be copied over and over again.\nNote that this is the same kind of problem that `VersionBuilder` is solving\nw/r/t `Version`s and files, and we can apply the same solution; that is, we can\naccumulate the changes in a different mutable object, and apply the delta in\none shot when the changes are committed. The patch does exactly that by\nadding a new `BlobFileMetaDataDelta` class to `VersionBuilder`. In addition,\nit turns the existing `GetBlobFileMetaData` helper into `IsBlobFileInVersion`\n(which is fine since that's the only thing the method's clients care about now),\nand adds a couple of helper methods that can create a `BlobFileMetaData`\nobject from the `BlobFileMetaData` in the base (if applicable) and the delta\nwhen the `Version` is saved.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6835\n\nTest Plan: `make check`\n\nReviewed By: riversand963\n\nDifferential Revision: D21505187\n\nPulled By: ltamasi\n\nfbshipit-source-id: d81a48c5f2ca7b79d7124c935332a6bcf3d5d988",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1551f1011ab419d21ac50be4b894cf3f688b30d7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d9e170d82beb044dd4d043ae87ceaf8577246ea9",
        "author": "Mian Qin",
        "date": "2020-05-04T10:55:14-07:00",
        "message": "Fix issues for reproducing synthetic ZippyDB workloads in the FAST20' paper (#6795)\n\nSummary:\nFix issues for reproducing synthetic ZippyDB workloads in the FAST20' paper using db_bench. Details changes as follows.\n1, add a separate random mode in MixGraph to produce all_random workload.\n2, fix power inverse function for generating prefix_dist workload.\n3, make sure key_offset in prefix mode is always unsigned.\nnote: Need to carefully choose key_dist_a/b to avoid aliasing. Power inverse function range should be close to overall key space.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6795\n\nReviewed By: akankshamahajan15\n\nDifferential Revision: D21371095\n\nPulled By: zhichao-cao\n\nfbshipit-source-id: 80744381e242392c8c7cf8ac3d68fe67fe876048",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d9e170d82beb044dd4d043ae87ceaf8577246ea9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e91d1a21a6ad7dc4eb4ce46c5b3f6ad550357640",
        "author": "Peter Dillinger",
        "date": "2020-03-26T19:36:32-07:00",
        "message": "Streamline persistent_cache_test for testing efficiency (#6601)\n\nSummary:\nThis test was written like a stress test, using up to 3x26GB\nRSS memory during parallel 'make check'. Now, while this code is mostly\ndormant, I've made the \"for Travis\" versions of the expensive tests the\ncanonical versions and disabled the expensive versions. This has the\nside benefit of removing some arbitrary conditional compilation.\n\nFor unknown reason, the super expensive tests were gated on\nSnappy_Supported, which appears to be irrelevant, so I removed it.\n\nThe tests can be fixed / improved / migrated to stress test if/when they\nare deemed important again.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6601\n\nTest Plan:\nmake check + CI\n\n./persistent_cache_test Before:\n...\n[==========] 10 tests from 2 test cases ran. (114541 ms total)\n[  PASSED  ] 10 tests.\nYOU HAVE 1 DISABLED TEST\n\nAfter:\n...\n[==========] 3 tests from 2 test cases ran. (1714 ms total)\n[  PASSED  ] 3 tests.\nYOU HAVE 10 DISABLED TESTS\n\nReviewed By: siying\n\nDifferential Revision: D20680983\n\nPulled By: pdillinger\n\nfbshipit-source-id: 2be0fde13eeb0a71110ac7f5477cfe63996a509e",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/persistent_cache/persistent_cache_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e91d1a21a6ad7dc4eb4ce46c5b3f6ad550357640",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9b3c9ef0e8a0019394d0bc6c2e271cddb1da4617",
        "author": "sdong",
        "date": "2020-03-02T11:55:28-08:00",
        "message": "Add --index_with_first_key and --index_shortening_mode to DB bench (#5859)\n\nSummary:\nSome combinatino of --index_with_first_key and --index_shortening_mode can signifcantly improve performance for large values. Expose them in db_bench.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5859\n\nTest Plan: Run them with the new options and observe the behavior.\n\nDifferential Revision: D20104434\n\nfbshipit-source-id: 21d48a732a9caf20b82312c7d7557d747ea3c304",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9b3c9ef0e8a0019394d0bc6c2e271cddb1da4617",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ab65278b1f29f9a75f1c184317a6708419dcd27e",
        "author": "Peter Dillinger",
        "date": "2020-02-21T13:31:57-08:00",
        "message": "Misc filter_bench improvements (#6444)\n\nSummary:\nUseful in validating/testing internal fragmentation changes (https://github.com/facebook/rocksdb/issues/6427)\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6444\n\nTest Plan: manual (no changes to production code)\n\nDifferential Revision: D20040076\n\nPulled By: pdillinger\n\nfbshipit-source-id: 32d26f363d2a9ab9f5bebd281dcebd9915ae340e",
        "modified_files_count": 1,
        "modified_files": [
            "util/filter_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ab65278b1f29f9a75f1c184317a6708419dcd27e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "946c43a026bc8855e200bcce888b8aeb6a6b0390",
        "author": "Yanqin Jin",
        "date": "2020-01-06T10:57:22-08:00",
        "message": "Improve error msg for SstFileWriter Merge (#6261)\n\nSummary:\nReword the error message when keys are not added in strict ascending order.\nSpecifically, original error message is not clear when application tries to\ncall SstFileWriter::Merge() with duplicate keys.\n\nTest plan (dev server)\n```\nmake check\n```\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6261\n\nDifferential Revision: D19290398\n\nPulled By: riversand963\n\nfbshipit-source-id: 4dc30a701414e6894db2eb024e3734470c22b371",
        "modified_files_count": 1,
        "modified_files": [
            "table/sst_file_writer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/946c43a026bc8855e200bcce888b8aeb6a6b0390",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4e0dcd36dfcf3109c4a8ede9c793bc8b1f76e68b",
        "author": "sdong",
        "date": "2019-11-19T13:17:03-08:00",
        "message": "db_stress sometimes generates keys close to SST file boundaries (#6037)\n\nSummary:\nRecently, a bug was found related to a seek key that is close to SST file boundary. However, it only occurs in a very small chance in db_stress, because the chance that a random key hits SST file boundaries is small. To boost the chance, with 1/16 chance, we pick keys that are close to SST file boundaries.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/6037\n\nTest Plan: Did some manual printing out, and hack to cover the key generation logic to be correct.\n\nDifferential Revision: D18598476\n\nfbshipit-source-id: 13b76687d106c5be4e3e02a0c77fa5578105a071",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4e0dcd36dfcf3109c4a8ede9c793bc8b1f76e68b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "111ebf3161e3ef03986f02a16f1b2207be2567fe",
        "author": "sdong",
        "date": "2019-11-06T17:38:25-08:00",
        "message": "db_stress: improve TestGet() failure printing (#5989)\n\nSummary:\nRight now, in db_stress's CF consistency test's TestGet case, if failure happens, we do normal string printing, rather than hex printing, so that some text is not printed out, which makes debugging harder. Fix it by printing hex instead.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5989\n\nTest Plan: Build db_stress and see t passes.\n\nDifferential Revision: D18363552\n\nfbshipit-source-id: 09d1b8f6fbff37441cbe7e63a1aef27551226cec",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/111ebf3161e3ef03986f02a16f1b2207be2567fe",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3f891c40a06f03bec99cf57e62237280d273995b",
        "author": "Peter Dillinger",
        "date": "2019-10-25T13:27:07-07:00",
        "message": "More improvements to filter_bench (#5968)\n\nSummary:\n* Adds support for plain table filter. This is not critical right now, but does add a -impl flag that will be useful for new filter implementations initially targeted at block-based table (and maybe later ported to plain table)\n* Better mixing of inside vs. outside queries, for more realism\n* A -best_case option handy for implementation tuning inner loop\n* Option for whether to include hashing time in dry run / net timings\n\nNo modifications to production code, just filter_bench.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5968\n\nDifferential Revision: D18139872\n\nPulled By: pdillinger\n\nfbshipit-source-id: 5b09eba963111b48f9e0525a706e9921070990e8",
        "modified_files_count": 1,
        "modified_files": [
            "util/filter_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3f891c40a06f03bec99cf57e62237280d273995b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2837008525d8d473ba0798144d427465ae71715d",
        "author": "Peter Dillinger",
        "date": "2019-10-24T13:08:30-07:00",
        "message": "Vary key size and alignment in filter_bench (#5933)\n\nSummary:\nThe first version of filter_bench has selectable key size\nbut that size does not vary throughout a test run. This artificially\nfavors \"branchy\" hash functions like the existing BloomHash,\nMurmurHash1, probably because of optimal return for branch prediction.\n\nThis change primarily varies those key sizes from -2 to +2 bytes vs.\nthe average selected size. We also set the default key size at 24 to\nbetter reflect our best guess of typical key size.\n\nBut steadily random key sizes may not be realistic either. So this\nchange introduces a new filter_bench option:\n-vary_key_size_log2_interval=n where the same key size is used 2^n\ntimes and then changes to another size. I've set the default at 5\n(32 times same size) as a compromise between deployments with\nrather consistent vs. rather variable key sizes. On my Skylake\nsystem, the performance boost to MurmurHash1 largely lies between\nn=10 and n=15.\n\nAlso added -vary_key_alignment (bool, now default=true), though this\ndoesn't currently seem to matter in hash functions under\nconsideration.\n\nThis change also does a \"dry run\" for each testing scenario, to improve\nthe accuracy of those numbers, as there was more difference between\nscenarios than expected. Subtracting gross test run times from dry run\ntimes is now also embedded in the output, because these \"net\" times are\ngenerally the most useful.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5933\n\nDifferential Revision: D18121683\n\nPulled By: pdillinger\n\nfbshipit-source-id: 3c7efee1c5661a5fe43de555e786754ddf80dc1e",
        "modified_files_count": 1,
        "modified_files": [
            "util/filter_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2837008525d8d473ba0798144d427465ae71715d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "915d72d849d0ba1fe88310ea81ce627270b88df7",
        "author": "Peter Dillinger",
        "date": "2019-09-16T09:37:42-07:00",
        "message": "Improve accuracy testing for DynamicBloom (#5805)\n\nSummary:\nDynamicBloom unit test now tests non-sequential as well as\nsequential keys in testing FP rates. Also now verifies larger structures.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5805\n\nTest Plan: thisisthetest\n\nDifferential Revision: D17398109\n\nPulled By: pdillinger\n\nfbshipit-source-id: 374074206c76d242efa378afc27830448a0e892a",
        "modified_files_count": 1,
        "modified_files": [
            "util/dynamic_bloom_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/915d72d849d0ba1fe88310ea81ce627270b88df7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f9fb9f14211ccc49921c3e31294e2619eb4e2fb7",
        "author": "Maysam Yabandeh",
        "date": "2019-09-04T14:31:10-07:00",
        "message": "Add a unit test to detect infinite loops with reseek optimizations (#5727)\n\nSummary:\nIterators reseek to the target key after iterating over max_sequential_skip_in_iterations invalid values. The logic is susceptible to an infinite loop bug, which has been present with WritePrepared Transactions up until 6.2 release. Although the bug is not present on master, the patch adds a unit test to prevent it from resurfacing again.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5727\n\nDifferential Revision: D16952759\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: d0d973dddc8dfabd5a794931232aa4c862c74f51",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/transactions/transaction_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f9fb9f14211ccc49921c3e31294e2619eb4e2fb7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "672befea2a514e32c8506389883f552129d2d5eb",
        "author": "Yanqin Jin",
        "date": "2019-08-30T12:42:01-07:00",
        "message": "Fix assertion failure in FIFO compaction with TTL (#5754)\n\nSummary:\nBefore this PR, the following sequence of events can cause assertion failure as shown below.\nStack trace (partial):\n```\n(gdb) bt\n2  0x00007f59b350ad15 in __assert_fail_base (fmt=<optimized out>, assertion=assertion@entry=0x9f8390 \"mark_as_compacted ? !inputs_[i][j]->being_compacted : inputs_[i][j]->being_compacted\", file=file@entry=0x9e347c \"db/compaction/compaction.cc\", line=line@entry=395, function=function@entry=0xa21ec0 <rocksdb::Compaction::MarkFilesBeingCompacted(bool)::__PRETTY_FUNCTION__> \"void rocksdb::Compaction::MarkFilesBeingCompacted(bool)\") at assert.c:92\n3  0x00007f59b350adc3 in __GI___assert_fail (assertion=assertion@entry=0x9f8390 \"mark_as_compacted ? !inputs_[i][j]->being_compacted : inputs_[i][j]->being_compacted\", file=file@entry=0x9e347c \"db/compaction/compaction.cc\", line=line@entry=395, function=function@entry=0xa21ec0 <rocksdb::Compaction::MarkFilesBeingCompacted(bool)::__PRETTY_FUNCTION__> \"void rocksdb::Compaction::MarkFilesBeingCompacted(bool)\") at assert.c:101\n4  0x0000000000492ccd in rocksdb::Compaction::MarkFilesBeingCompacted (this=<optimized out>, mark_as_compacted=<optimized out>) at db/compaction/compaction.cc:394\n5  0x000000000049467a in rocksdb::Compaction::Compaction (this=0x7f59af013000, vstorage=0x7f581af53030, _immutable_cf_options=..., _mutable_cf_options=..., _inputs=..., _output_level=<optimized out>, _target_file_size=0, _max_compaction_bytes=0, _output_path_id=0, _compression=<incomplete type>, _compression_opts=..., _max_subcompactions=0, _grandparents=..., _manual_compaction=false, _score=4, _deletion_compaction=true, _compaction_reason=rocksdb::CompactionReason::kFIFOTtl) at db/compaction/compaction.cc:241\n6  0x00000000004af9bc in rocksdb::FIFOCompactionPicker::PickTTLCompaction (this=0x7f59b31a6900, cf_name=..., mutable_cf_options=..., vstorage=0x7f581af53030, log_buffer=log_buffer@entry=0x7f59b1bfa930) at db/compaction/compaction_picker_fifo.cc:101\n7  0x00000000004b0771 in rocksdb::FIFOCompactionPicker::PickCompaction (this=0x7f59b31a6900, cf_name=..., mutable_cf_options=..., vstorage=0x7f581af53030, log_buffer=0x7f59b1bfa930) at db/compaction/compaction_picker_fifo.cc:201\n8  0x00000000004838cc in rocksdb::ColumnFamilyData::PickCompaction (this=this@entry=0x7f59b31b3700, mutable_options=..., log_buffer=log_buffer@entry=0x7f59b1bfa930) at db/column_family.cc:933\n9  0x00000000004f3645 in rocksdb::DBImpl::BackgroundCompaction (this=this@entry=0x7f59b3176000, made_progress=made_progress@entry=0x7f59b1bfa6bf, job_context=job_context@entry=0x7f59b1bfa760, log_buffer=log_buffer@entry=0x7f59b1bfa930, prepicked_compaction=prepicked_compaction@entry=0x0, thread_pri=rocksdb::Env::LOW) at db/db_impl/db_impl_compaction_flush.cc:2541\n10 0x00000000004f5e2a in rocksdb::DBImpl::BackgroundCallCompaction (this=this@entry=0x7f59b3176000, prepicked_compaction=prepicked_compaction@entry=0x0, bg_thread_pri=bg_thread_pri@entry=rocksdb::Env::LOW) at db/db_impl/db_impl_compaction_flush.cc:2312\n11 0x00000000004f648e in rocksdb::DBImpl::BGWorkCompaction (arg=<optimized out>) at db/db_impl/db_impl_compaction_flush.cc:2087\n```\nThis can be caused by the following sequence of events.\n```\nTime\n|      thr          bg_compact_thr1                     bg_compact_thr2\n|      write\n|      flush\n|                   mark all l0 as being compacted\n|      write\n|      flush\n|                   add cf to queue again\n|                                                       mark all l0 as being\n|                                                       compacted, fail the\n|                                                       assertion\nV\n```\nTest plan (on devserver)\nSince bg_compact_thr1 and bg_compact_thr2 are two threads executing the same\ncode, it is difficult to use sync point dependency to\ncoordinate their execution. Therefore, I choose to use db_stress.\n```\n$TEST_TMPDIR=/dev/shm/rocksdb ./db_stress --periodic_compaction_seconds=1 --max_background_compactions=20 --format_version=2 --memtablerep=skip_list --max_write_buffer_number=3 --cache_index_and_filter_blocks=1 --reopen=20 --recycle_log_file_num=0 --acquire_snapshot_one_in=10000 --delpercent=4 --log2_keys_per_lock=22 --compaction_ttl=1 --block_size=16384 --use_multiget=1 --compact_files_one_in=1000000 --target_file_size_multiplier=2 --clear_column_family_one_in=0 --max_bytes_for_level_base=10485760 --use_full_merge_v1=1 --target_file_size_base=2097152 --checkpoint_one_in=1000000 --mmap_read=0 --compression_type=zstd --writepercent=35 --readpercent=45 --subcompactions=4 --use_merge=0 --write_buffer_size=4194304 --test_batches_snapshots=0 --db=/dev/shm/rocksdb/rocksdb_crashtest_whitebox --use_direct_reads=0 --compact_range_one_in=1000000 --open_files=-1 --destroy_db_initially=0 --progress_reports=0 --compression_zstd_max_train_bytes=0 --snapshot_hold_ops=100000 --enable_pipelined_write=0 --nooverwritepercent=1 --compression_max_dict_bytes=0 --max_key=1000000 --prefixpercent=5 --flush_one_in=1000000 --ops_per_thread=40000 --index_block_restart_interval=7 --cache_size=1048576 --compaction_style=2 --verify_checksum=1 --delrangepercent=1 --use_direct_io_for_flush_and_compaction=0\n```\nThis should see no assertion failure.\nLast but not least,\n```\n$COMPILE_WITH_ASAN=1 make -j32 all\n$make check\n```\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5754\n\nDifferential Revision: D17109791\n\nPulled By: riversand963\n\nfbshipit-source-id: 25fc46101235add158554e096540b72c324be078",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction/compaction_picker_fifo.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/672befea2a514e32c8506389883f552129d2d5eb",
        "contains_optimization_keyword": true
    },
    {
        "hash": "26293c89a65625c34f362385779358cb16905e38",
        "author": "DaiZhiwei",
        "date": "2019-08-23T11:04:08-07:00",
        "message": "crc32c_arm64 performance optimization (#5675)\n\nSummary:\nCrc32c Parallel computation coding optimization:\nMacro unfolding removes the \"for\" loop and is good to decrease branch-miss in arm64 micro architecture\n1024 Bytes is divided into  8(head) + 1008( 6 * 7 * 3 * 8 ) + 8(tail)  three parts\nMacro unfolding 42 loops to 6 CRC32C7X24BYTESs\n1 CRC32C7X24BYTES containing 7 CRC32C24BYTESs\n\n1, crc32c_test\n[==========] Running 4 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 4 tests from CRC\n[ RUN      ] CRC.StandardResults\n[       OK ] CRC.StandardResults (1 ms)\n[ RUN      ] CRC.Values\n[       OK ] CRC.Values (0 ms)\n[ RUN      ] CRC.Extend\n[       OK ] CRC.Extend (0 ms)\n[ RUN      ] CRC.Mask\n[       OK ] CRC.Mask (0 ms)\n[----------] 4 tests from CRC (1 ms total)\n\n[----------] Global test environment tear-down\n[==========] 4 tests from 1 test case ran. (1 ms total)\n[  PASSED  ] 4 tests.\n\n2, db_bench --benchmarks=\"crc32c\"\ncrc32c : 0.218 micros/op 4595390 ops/sec; 17950.7 MB/s (4096 per op)\n\n3, repeated crc32c_test case  60000 times\nperf stat -e branch-miss -- ./crc32c_test\nbefore optimization:\n739,426,504      branch-miss\nafter optimization:\n1,128,572      branch-miss\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5675\n\nDifferential Revision: D16989210\n\nfbshipit-source-id: 7204e6069bb6ed066d49c2d1b3ac385065a98557",
        "modified_files_count": 1,
        "modified_files": [
            "util/crc32c_arm64.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/26293c89a65625c34f362385779358cb16905e38",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a2e46eae46825e3eb87f9205d8dbc962b1d0107a",
        "author": "jsteemann",
        "date": "2019-08-16T14:38:08-07:00",
        "message": "fix compiling with `-DNPERF_CONTEXT` (#5704)\n\nSummary:\nThis was previously broken, as the performance context-related\nmacro signatures in file monitoring/perf_context_imp.h\ndeviated for the case when NPERF_CONTEXT was defined and when it\nwas not.\n\nUpdate the macros for the `-DNPERF_CONTEXT` case, so it compiles.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5704\n\nDifferential Revision: D16867746\n\nfbshipit-source-id: 05539724cb1f7955ecc42828365836a677759ad9",
        "modified_files_count": 1,
        "modified_files": [
            "monitoring/perf_context_imp.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a2e46eae46825e3eb87f9205d8dbc962b1d0107a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1dfc5eaab03f998ab13a6953b53e41cdfd2c8237",
        "author": "Levi Tamasi",
        "date": "2019-07-31T15:20:26-07:00",
        "message": "Test the various configurations in parallel in MergeOperatorPinningTest (#5659)\n\nSummary:\nMergeOperatorPinningTest.Randomized frequently times out under TSAN\nbecause it tests ~40 option configurations sequentially in a loop. The\npatch parallelizes the tests of the various configurations to make the\ntest complete faster.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5659\n\nTest Plan: Tested using buck test mode/dev-tsan ...\n\nDifferential Revision: D16587518\n\nPulled By: ltamasi\n\nfbshipit-source-id: 65bd25c0ad9a23587fed5592e69c1a0097fa27f6",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_merge_operator_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1dfc5eaab03f998ab13a6953b53e41cdfd2c8237",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3f89af1c39da4991ef6c544fc5e3f164a688b375",
        "author": "Levi Tamasi",
        "date": "2019-07-26T15:53:34-07:00",
        "message": "Reduce the number of random iterations in compact_on_deletion_collector_test (#5635)\n\nSummary:\nThis test frequently times out under TSAN; reducing the number of random\niterations to make it complete faster.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5635\n\nTest Plan: buck test mode/dev-tsan internal_repo_rocksdb/repo:compact_on_deletion_collector_test\n\nDifferential Revision: D16523505\n\nPulled By: ltamasi\n\nfbshipit-source-id: 6a69909bce9d204c891150fcb3d536547b3253d0",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/table_properties_collectors/compact_on_deletion_collector_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3f89af1c39da4991ef6c544fc5e3f164a688b375",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f786b4a5b4f1f162a7e7452b33e2e5cf0d755b9b",
        "author": "Yanqin Jin",
        "date": "2019-07-09T16:27:22-07:00",
        "message": "Improve result print on atomic flush stress test failure (#5549)\n\nSummary:\nWhen atomic flush stress test fails, we print internal keys within the range with mismatched key/values for all column families.\n\nTest plan (on devserver)\nManually hack the code to randomly insert wrong data. Run the test.\n```\n$make clean && COMPILE_WITH_TSAN=1 make -j32 db_stress\n$./db_stress -test_atomic_flush=true -ops_per_thread=10000\n```\nCheck that proper error messages are printed, as follows:\n```\n2019/07/08-17:40:14  Starting verification\nVerification failed\nLatest Sequence Number: 190903\n[default] 000000000000050B => 56290000525350515E5F5C5D5A5B5859\n[3] 0000000000000533 => EE100000EAEBE8E9E6E7E4E5E2E3E0E1FEFFFCFDFAFBF8F9\nInternal keys in CF 'default', [000000000000050B, 0000000000000533] (max 8)\n  key 000000000000050B seq 139920 type 1\n  key 0000000000000533 seq 0 type 1\nInternal keys in CF '3', [000000000000050B, 0000000000000533] (max 8)\n  key 0000000000000533 seq 0 type 1\n```\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5549\n\nDifferential Revision: D16158709\n\nPulled By: riversand963\n\nfbshipit-source-id: f07fa87763f87b3bd908da03c956709c6456bcab",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f786b4a5b4f1f162a7e7452b33e2e5cf0d755b9b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "84c5c9aab15896e1c55c3febfa1fac5ed2009069",
        "author": "Sagar Vemuri",
        "date": "2019-07-03T19:06:46-07:00",
        "message": "Fix a bug in compaction reads causing checksum mismatches and asan errors (#5531)\n\nSummary:\nFixed a bug in compaction reads due to which incorrect number of bytes were being read/utilized. The bug was introduced in https://github.com/facebook/rocksdb/issues/5498 , resulting in \"Corruption: block checksum mismatch\" and \"heap-buffer-overflow\" asan errors in our tests.\n\nhttps://github.com/facebook/rocksdb/issues/5498 was introduced recently and is not in any released versions.\n\nASAN:\n```\n> ==2280939==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6250005e83da at pc 0x000000d57f62 bp 0x7f954f483770 sp 0x7f954f482f20\n> === How to use this, how to get the raw stack trace, and more: fburl.com/ASAN ===\n> READ of size 4 at 0x6250005e83da thread T4\n> SCARINESS: 27 (4-byte-read-heap-buffer-overflow-far-from-bounds)\n\n>      #0 tests+0xd57f61                           __asan_memcpy\n>      https://github.com/facebook/rocksdb/issues/1 rocksdb/src/util/coding.h:124            rocksdb::DecodeFixed32(char const*)\n>      https://github.com/facebook/rocksdb/issues/2 rocksdb/src/table/block_fetcher.cc:39    rocksdb::BlockFetcher::CheckBlockChecksum()\n>      https://github.com/facebook/rocksdb/issues/3 rocksdb/src/table/block_fetcher.cc:99    rocksdb::BlockFetcher::TryGetFromPrefetchBuffer()\n>      https://github.com/facebook/rocksdb/issues/4 rocksdb/src/table/block_fetcher.cc:209   rocksdb::BlockFetcher::ReadBlockContents()\n>      https://github.com/facebook/rocksdb/issues/5 rocksdb/src/table/block_based/block_based_table_reader.cc:93 rocksdb::(anonymous namespace)::ReadBlockFromFile(rocksdb::RandomAccessFileReader*, rocksdb::FilePrefetchBuffer*, rocksdb::Footer const&, rocksdb::ReadOptions const&, rocksdb::BlockHandle const&, std::unique_ptr<...>*, rocksdb::ImmutableCFOptions const&, bool, bool, rocksdb::UncompressionDict\n const&, rocksdb::PersistentCacheOptions const&, unsigned long, unsigned long, rocksdb::MemoryAllocator*, bool)\n>      https://github.com/facebook/rocksdb/issues/6 rocksdb/src/table/block_based/block_based_table_reader.cc:2331 rocksdb::BlockBasedTable::RetrieveBlock(rocksdb::FilePrefetchBuffer*, rocksdb::ReadOptions const&, rocksdb::BlockHandle const&, rocksdb::UncompressionDict const&, rocksdb::CachableEntry<...>*, rocksdb::BlockType, rocksdb::GetContext*, rocksdb::BlockCacheLookupContext*, bool) const\n>      https://github.com/facebook/rocksdb/issues/7 rocksdb/src/table/block_based/block_based_table_reader.cc:2090 rocksdb::DataBlockIter* rocksdb::BlockBasedTable::NewDataBlockIterator<...>(rocksdb::ReadOptions const&, rocksdb::BlockHandle const&, rocksdb::DataBlockIter*, rocksdb::BlockType, bool, bool, rocksdb::GetContext*, rocksdb::BlockCacheLookupContext*, rocksdb::Status, rocksdb::FilePrefetchBuffe\nr*, bool) const\n>      https://github.com/facebook/rocksdb/issues/8 rocksdb/src/table/block_based/block_based_table_reader.cc:2720 rocksdb::BlockBasedTableIterator<...>::InitDataBlock()\n>      https://github.com/facebook/rocksdb/issues/9 rocksdb/src/table/block_based/block_based_table_reader.cc:2607 rocksdb::BlockBasedTableIterator<...>::SeekToFirst()\n>     https://github.com/facebook/rocksdb/issues/10 rocksdb/src/table/iterator_wrapper.h:83  rocksdb::IteratorWrapperBase<...>::SeekToFirst()\n>     https://github.com/facebook/rocksdb/issues/11 rocksdb/src/table/merging_iterator.cc:100 rocksdb::MergingIterator::SeekToFirst()\n>     https://github.com/facebook/rocksdb/issues/12 rocksdb/compaction/compaction_job.cc:877 rocksdb::CompactionJob::ProcessKeyValueCompaction(rocksdb::CompactionJob::SubcompactionState*)\n>     https://github.com/facebook/rocksdb/issues/13 rocksdb/compaction/compaction_job.cc:590 rocksdb::CompactionJob::Run()\n>     https://github.com/facebook/rocksdb/issues/14 rocksdb/db_impl/db_impl_compaction_flush.cc:2689 rocksdb::DBImpl::BackgroundCompaction(bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::DBImpl::PrepickedCompaction*, rocksdb::Env::Priority)\n>     https://github.com/facebook/rocksdb/issues/15 rocksdb/db_impl/db_impl_compaction_flush.cc:2248 rocksdb::DBImpl::BackgroundCallCompaction(rocksdb::DBImpl::PrepickedCompaction*, rocksdb::Env::Priority)\n>     https://github.com/facebook/rocksdb/issues/16 rocksdb/db_impl/db_impl_compaction_flush.cc:2024 rocksdb::DBImpl::BGWorkCompaction(void*)\n>     https://github.com/facebook/rocksdb/issues/23 rocksdb/src/util/threadpool_imp.cc:266   rocksdb::ThreadPoolImpl::Impl::BGThread(unsigned long)\n>     https://github.com/facebook/rocksdb/issues/24 rocksdb/src/util/threadpool_imp.cc:307   rocksdb::ThreadPoolImpl::Impl::BGThreadWrapper(void*)\n```\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5531\n\nTest Plan: Verified that this fixes the fb-internal Logdevice test which caught the issue.\n\nDifferential Revision: D16109702\n\nPulled By: sagar0\n\nfbshipit-source-id: 1fc08549cf7b553e338a133ae11eb9f4d5011914",
        "modified_files_count": 1,
        "modified_files": [
            "util/file_reader_writer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/84c5c9aab15896e1c55c3febfa1fac5ed2009069",
        "contains_optimization_keyword": true
    },
    {
        "hash": "22028aa9ab27cf860b74d12e006f82ff551caee0",
        "author": "Vijay Nadimpalli",
        "date": "2019-06-21T21:31:49-07:00",
        "message": "Compaction Reads should read no more than compaction_readahead_size bytes, when set! (#5498)\n\nSummary:\nAs a result of https://github.com/facebook/rocksdb/issues/5431 the compaction_readahead_size given by a user was not used exactly, the reason being the code behind readahead for user-read and compaction-read was unified in the above PR and the behavior for user-read is to read readahead_size+n bytes (see FilePrefetchBuffer::TryReadFromCache method). Before the unification the ReadaheadRandomAccessFileReader used compaction_readahead_size as it is.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5498\n\nTest Plan:\nRan strace command : strace -e pread64 -f -T -t ./db_compaction_test --gtest_filter=DBCompactionTest.PartialManualCompaction\n\nIn the test the compaction_readahead_size was configured to 2MB and verified the pread syscall did indeed request 2MB. Before the change it was requesting more than 2MB.\n\nStrace Output:\nstrace: Process 3798982 attached\nNote: Google Test filter = DBCompactionTest.PartialManualCompaction\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from DBCompactionTest\n[ RUN      ] DBCompactionTest.PartialManualCompaction\nstrace: Process 3798983 attached\nstrace: Process 3798984 attached\nstrace: Process 3798985 attached\nstrace: Process 3798986 attached\nstrace: Process 3798987 attached\nstrace: Process 3798992 attached\n[pid 3798987] 12:07:05 +++ exited with 0 +++\nstrace: Process 3798993 attached\n[pid 3798993] 12:07:05 +++ exited with 0 +++\nstrace: Process 3798994 attached\nstrace: Process 3799008 attached\nstrace: Process 3799009 attached\n[pid 3799008] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799010 attached\n[pid 3799009] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799011 attached\n[pid 3799010] 12:07:05 +++ exited with 0 +++\n[pid 3799011] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799012 attached\n[pid 3799012] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799013 attached\nstrace: Process 3799014 attached\n[pid 3799013] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799015 attached\n[pid 3799014] 12:07:05 +++ exited with 0 +++\n[pid 3799015] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799016 attached\n[pid 3799016] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799017 attached\n[pid 3799017] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799019 attached\n[pid 3799019] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799020 attached\nstrace: Process 3799021 attached\n[pid 3799020] 12:07:05 +++ exited with 0 +++\n[pid 3799021] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799022 attached\n[pid 3799022] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799023 attached\n[pid 3799023] 12:07:05 +++ exited with 0 +++\nstrace: Process 3799047 attached\nstrace: Process 3799048 attached\n[pid 3799047] 12:07:06 +++ exited with 0 +++\n[pid 3799048] 12:07:06 +++ exited with 0 +++\n[pid 3798994] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799052 attached\n[pid 3799052] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799054 attached\nstrace: Process 3799069 attached\nstrace: Process 3799070 attached\n[pid 3799069] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799071 attached\n[pid 3799070] 12:07:06 +++ exited with 0 +++\n[pid 3799071] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799072 attached\nstrace: Process 3799073 attached\n[pid 3799072] 12:07:06 +++ exited with 0 +++\n[pid 3799073] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799074 attached\n[pid 3799074] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799075 attached\n[pid 3799075] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799076 attached\n[pid 3799076] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799077 attached\n[pid 3799077] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799078 attached\n[pid 3799078] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799079 attached\n[pid 3799079] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799080 attached\n[pid 3799080] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799081 attached\n[pid 3799081] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799082 attached\n[pid 3799082] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799083 attached\n[pid 3799083] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799086 attached\nstrace: Process 3799087 attached\n[pid 3798984] 12:07:06 pread64(9, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000121>\n[pid 3798984] 12:07:06 pread64(9, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000106>\n[pid 3798984] 12:07:06 pread64(9, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000081>\n[pid 3798984] 12:07:06 pread64(9, \"\\0\\v\\3foo\\2\\7\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\2\\3\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000138>\n[pid 3798984] 12:07:06 pread64(11, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000097>\n[pid 3798984] 12:07:06 pread64(11, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000086>\n[pid 3798984] 12:07:06 pread64(11, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000064>\n[pid 3798984] 12:07:06 pread64(11, \"\\0\\v\\3foo\\2\\21\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\2\\r\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000064>\n[pid 3798984] 12:07:06 pread64(12, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000080>\n[pid 3798984] 12:07:06 pread64(12, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000090>\n[pid 3798984] 12:07:06 pread64(12, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000059>\n[pid 3798984] 12:07:06 pread64(12, \"\\0\\v\\3foo\\2\\33\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\2\\27\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000065>\n[pid 3798984] 12:07:06 pread64(13, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000070>\n[pid 3798984] 12:07:06 pread64(13, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000059>\n[pid 3798984] 12:07:06 pread64(13, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000061>\n[pid 3798984] 12:07:06 pread64(13, \"\\0\\v\\3foo\\2%\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\2!\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000065>\n[pid 3798984] 12:07:06 pread64(14, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000118>\n[pid 3798984] 12:07:06 pread64(14, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000093>\n[pid 3798984] 12:07:06 pread64(14, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000050>\n[pid 3798984] 12:07:06 pread64(14, \"\\0\\v\\3foo\\2/\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\2+\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000082>\n[pid 3798984] 12:07:06 pread64(15, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000080>\n[pid 3798984] 12:07:06 pread64(15, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000086>\n[pid 3798984] 12:07:06 pread64(15, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000091>\n[pid 3798984] 12:07:06 pread64(15, \"\\0\\v\\3foo\\0029\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\0025\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000174>\n[pid 3798984] 12:07:06 pread64(16, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000080>\n[pid 3798984] 12:07:06 pread64(16, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000093>\n[pid 3798984] 12:07:06 pread64(16, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000194>\n[pid 3798984] 12:07:06 pread64(16, \"\\0\\v\\3foo\\2C\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\2?\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000086>\n[pid 3798984] 12:07:06 pread64(17, \"\\1\\203W!\\241QE\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 11177) = 53 <0.000079>\n[pid 3798984] 12:07:06 pread64(17, \"\\0\\22\\4rocksdb.properties\\353Q\\223\\5\\0\\0\\0\\0\\1\\0\\0\"..., 38, 11139) = 38 <0.000047>\n[pid 3798984] 12:07:06 pread64(17, \"\\0$\\4rocksdb.block.based.table.ind\"..., 664, 10475) = 664 <0.000045>\n[pid 3798984] 12:07:06 pread64(17, \"\\0\\v\\3foo\\2M\\0\\0\\0\\0\\0\\0\\0\\270 \\0\\v\\4foo\\2I\\0\\0\\0\\0\\0\\0\\275\"..., 74, 10401) = 74 <0.000107>\n[pid 3798983] 12:07:06 pread64(17, \"\\0\\v\\200\\10foo\\2P\\0\\0\\0\\0\\0\\0)U?MSg_)j(roFn($e\"..., 2097152, 0) = 11230 <0.000091>\n[pid 3798983] 12:07:06 pread64(17, \"\", 2085922, 11230) = 0 <0.000073>\n[pid 3798983] 12:07:06 pread64(16, \"\\0\\v\\200\\10foo\\2F\\0\\0\\0\\0\\0\\0k[h3%.OPH_^:\\\\S7T&\"..., 2097152, 0) = 11230 <0.000083>\n[pid 3798983] 12:07:06 pread64(16, \"\", 2085922, 11230) = 0 <0.000078>\n[pid 3798983] 12:07:06 pread64(15, \"\\0\\v\\200\\10foo\\2<\\0\\0\\0\\0\\0\\0+qToi_c{*S+4:N(:\"..., 2097152, 0) = 11230 <0.000095>\n[pid 3798983] 12:07:06 pread64(15, \"\", 2085922, 11230) = 0 <0.000067>\n[pid 3798983] 12:07:06 pread64(14, \"\\0\\v\\200\\10foo\\0022\\0\\0\\0\\0\\0\\0%hw%OMa\\\"}9I609Q!B\"..., 2097152, 0) = 11230 <0.000111>\n[pid 3798983] 12:07:06 pread64(14, \"\", 2085922, 11230) = 0 <0.000093>\n[pid 3798983] 12:07:06 pread64(13, \"\\0\\v\\200\\10foo\\2(\\0\\0\\0\\0\\0\\0p}Y&mu^DcaSGb2&nP\"..., 2097152, 0) = 11230 <0.000128>\n[pid 3798983] 12:07:06 pread64(13, \"\", 2085922, 11230) = 0 <0.000076>\n[pid 3798983] 12:07:06 pread64(12, \"\\0\\v\\200\\10foo\\2\\36\\0\\0\\0\\0\\0\\0YIyW#]oSs^6VHfB<`\"..., 2097152, 0) = 11230 <0.000092>\n[pid 3798983] 12:07:06 pread64(12, \"\", 2085922, 11230) = 0 <0.000073>\n[pid 3798983] 12:07:06 pread64(11, \"\\0\\v\\200\\10foo\\2\\24\\0\\0\\0\\0\\0\\0mfF8Jel/*Zf :-#s(\"..., 2097152, 0) = 11230 <0.000088>\n[pid 3798983] 12:07:06 pread64(11, \"\", 2085922, 11230) = 0 <0.000067>\n[pid 3798983] 12:07:06 pread64(9, \"\\0\\v\\200\\10foo\\2\\n\\0\\0\\0\\0\\0\\0\\\\X'cjiHX)D,RSj1X!\"..., 2097152, 0) = 11230 <0.000115>\n[pid 3798983] 12:07:06 pread64(9, \"\", 2085922, 11230) = 0 <0.000073>\n[pid 3798983] 12:07:06 pread64(8, \"\\1\\315\\5 \\36\\30\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 53, 754) = 53 <0.000098>\n[pid 3798983] 12:07:06 pread64(8, \"\\0\\22\\3rocksdb.properties;\\215\\5\\0\\0\\0\\0\\1\\0\\0\\0\"..., 37, 717) = 37 <0.000064>\n[pid 3798983] 12:07:06 pread64(8, \"\\0$\\4rocksdb.block.based.table.ind\"..., 658, 59) = 658 <0.000074>\n[pid 3798983] 12:07:06 pread64(8, \"\\0\\v\\2foo\\1\\0\\0\\0\\0\\0\\0\\0\\0\\31\\0\\0\\0\\0\\1\\0\\0\\0\\0\\212\\216\\222P\", 29, 30) = 29 <0.000064>\n[pid 3799086] 12:07:06 +++ exited with 0 +++\n[pid 3799087] 12:07:06 +++ exited with 0 +++\n[pid 3799054] 12:07:06 +++ exited with 0 +++\nstrace: Process 3799104 attached\n[pid 3799104] 12:07:06 +++ exited with 0 +++\n[       OK ] DBCompactionTest.PartialManualCompaction (757 ms)\n[----------] 1 test from DBCompactionTest (758 ms total)\n\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (759 ms total)\n[  PASSED  ] 1 test.\n[pid 3798983] 12:07:06 +++ exited with 0 +++\n[pid 3798984] 12:07:06 +++ exited with 0 +++\n[pid 3798992] 12:07:06 +++ exited with 0 +++\n[pid 3798986] 12:07:06 +++ exited with 0 +++\n[pid 3798982] 12:07:06 +++ exited with 0 +++\n[pid 3798985] 12:07:06 +++ exited with 0 +++\n12:07:06 +++ exited with 0 +++\n\nDifferential Revision: D15948422\n\nPulled By: vjnadimpalli\n\nfbshipit-source-id: 9b189d1e8675d290c7784e4b33e5d3b5761d2ac8",
        "modified_files_count": 1,
        "modified_files": [
            "util/file_reader_writer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/22028aa9ab27cf860b74d12e006f82ff551caee0",
        "contains_optimization_keyword": true
    },
    {
        "hash": "79edf0a7a8ab75f60692efd54b1e0ed7da7aafca",
        "author": "Yuan Zhou",
        "date": "2019-05-31T17:17:57-07:00",
        "message": "util: fix log_write_bench (#5335)\n\nSummary:\nlog_write_bench doesn't compile due to some recent API changes.\nThis patch fixes the compile by adding the missing params for\nOptimizeForLogWrite() and WritableFileWriter().\n\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5335\n\nDifferential Revision: D15588875\n\nPulled By: miasantreble\n\nfbshipit-source-id: 726ff4dc227733e915c3b796df25bd3ab0b431ac",
        "modified_files_count": 1,
        "modified_files": [
            "util/log_write_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/79edf0a7a8ab75f60692efd54b1e0ed7da7aafca",
        "contains_optimization_keyword": true
    },
    {
        "hash": "6267ed251ae5162b7b5c41521061e5541af301f5",
        "author": "Siying Dong",
        "date": "2019-05-24T13:09:55-07:00",
        "message": "Improve comment in db_impl.h (#5338)\n\nSummary:\nAdd some comments in db_impl.h. Also reordered function order a little bit so that I can add a comment to flag the area of functions implementing DB interface.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5338\n\nDifferential Revision: D15498284\n\nPulled By: siying\n\nfbshipit-source-id: 3d7c59c8303577fe44d13c74ae84c7ce05164f77",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/6267ed251ae5162b7b5c41521061e5541af301f5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f69e63dc5fa99277bc1e1ef6140383207be3c8ac",
        "author": "Siying Dong",
        "date": "2019-05-24T12:24:28-07:00",
        "message": "Improve comments in compaction.h\n\nSummary: Pull Request resolved: https://github.com/facebook/rocksdb/pull/5356\n\nDifferential Revision: D15499033\n\nPulled By: siying\n\nfbshipit-source-id: 069ae48669484beaf668dd90389b8743b3309dc3",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f69e63dc5fa99277bc1e1ef6140383207be3c8ac",
        "contains_optimization_keyword": true
    },
    {
        "hash": "88ff80780b3ccdbf802625c8302b9e4405a09b66",
        "author": "Zhongyi Xie",
        "date": "2019-05-24T10:40:30-07:00",
        "message": "improve comment for WalManager (#5350)\n\nSummary:\natt\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5350\n\nDifferential Revision: D15496467\n\nPulled By: miasantreble\n\nfbshipit-source-id: c29c0b143bf4df2040695a82be0feb9814ddb641",
        "modified_files_count": 1,
        "modified_files": [
            "db/wal_manager.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/88ff80780b3ccdbf802625c8302b9e4405a09b66",
        "contains_optimization_keyword": true
    },
    {
        "hash": "94c78b11e411d15f23bbc0c3c3f95c7e070ea528",
        "author": "Zhongyi Xie",
        "date": "2019-05-24T10:33:57-07:00",
        "message": "improve comments for statistics.h\n\nSummary: Pull Request resolved: https://github.com/facebook/rocksdb/pull/5351\n\nDifferential Revision: D15496346\n\nPulled By: miasantreble\n\nfbshipit-source-id: eeb619e6bd8616003ba35b0cd4bb8050e6a8cb4d",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/statistics.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/94c78b11e411d15f23bbc0c3c3f95c7e070ea528",
        "contains_optimization_keyword": true
    },
    {
        "hash": "cd43446d017fd3929e5883bccf1206afafd57952",
        "author": "Siying Dong",
        "date": "2019-05-20T13:50:53-07:00",
        "message": "Improve DBTablePropertiesTest.GetPropertiesOfTablesInRange (#5302)\n\nSummary:\nDBTablePropertiesTest.GetPropertiesOfTablesInRange sometimes hits the assert that generated LSM-tree doesn't have L1 file. Tighten the compaction triggering condition even further, hoping it goes away.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5302\n\nDifferential Revision: D15325971\n\nPulled By: siying\n\nfbshipit-source-id: 3e032bdb16fe8d98d5fcfcd65dd8be9781f3d6ae",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_table_properties_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/cd43446d017fd3929e5883bccf1206afafd57952",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f82e693a31d07ab8b391888ff60eb7ff5b95bd13",
        "author": "Siying Dong",
        "date": "2019-05-16T15:24:28-07:00",
        "message": "RangeDelAggregator::StripeRep::Invalidate() to be skipped if empty (#5312)\n\nSummary:\nRangeDelAggregator::StripeRep::Invalidate() clears up several vectors. If we know there isn't anything to there, we can safe these small CPUs. Profiling shows that it sometimes take non-negligible amount of CPU. Worth a small optimization.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5312\n\nDifferential Revision: D15380511\n\nPulled By: siying\n\nfbshipit-source-id: 53c5f34c33b4cb1e743643c6086ac56d0b84ec2e",
        "modified_files_count": 1,
        "modified_files": [
            "db/range_del_aggregator.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f82e693a31d07ab8b391888ff60eb7ff5b95bd13",
        "contains_optimization_keyword": true
    },
    {
        "hash": "eea1cad850c2e268b0bfde208a005e44289dea47",
        "author": "Zhongyi Xie",
        "date": "2019-05-07T20:20:40-07:00",
        "message": "avoid updating index type during iterator creation (#5288)\n\nSummary:\nRight now there is a potential race condition where two threads are created to iterate through the DB (https://gist.github.com/miasantreble/88f5798a397ee7cb8e7baff9db2d9e85).  The problem is that in `BlockBasedTable::NewIndexIterator`, if both threads failed to find index_reader from block cache, they will call `CreateIndexReader->UpdateIndexType()` which creates a race to update `index_type` in the shared rep_ object. By checking the code, we realize the index type is always populated by `PrefetchIndexAndFilterBlocks` during the table `Open` call, so there is no need to update index type every time during iterator creation. This PR attempts to fix the race condition by removing the unnecessary call to `UpdateIndexType`\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5288\n\nDifferential Revision: D15252509\n\nPulled By: miasantreble\n\nfbshipit-source-id: 6e3258652121d5c76d267f7ac457e15c5e84756e",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/eea1cad850c2e268b0bfde208a005e44289dea47",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d51eb0b583fe28ede2b4a6d778de4489433f1bbf",
        "author": "Zhongyi Xie",
        "date": "2019-05-01T20:40:00-07:00",
        "message": "set snappy compression only when supported (#4325)\n\nSummary:\nRight now `OptimizeLevelStyleCompaction` may set compression type to Snappy even when Snappy is not supported, this may cause errors like \"no snappy compression support\"\nFixes https://github.com/facebook/rocksdb/issues/4283\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4325\n\nDifferential Revision: D15125542\n\nPulled By: miasantreble\n\nfbshipit-source-id: 70890b73ababe16752721555dbd290633c2aafac",
        "modified_files_count": 1,
        "modified_files": [
            "options/options.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d51eb0b583fe28ede2b4a6d778de4489433f1bbf",
        "contains_optimization_keyword": true
    },
    {
        "hash": "72c8533f2c137170478270cb74b218095ced8904",
        "author": "Siying Dong",
        "date": "2019-04-23T10:55:01-07:00",
        "message": "DBIter to use IteratorWrapper for inner iterator (#5214)\n\nSummary:\nIt's hard to get DBIter to directly use InternalIterator::NextAndGetResult() because the code change would be complicated. Instead, use IteratorWrapper, where Next() is already using NextAndGetResult(). Performance number is hard to measure because it is small and ther is variation. I run readseq many times, and there seems to be 1% gain.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5214\n\nDifferential Revision: D15003635\n\nPulled By: siying\n\nfbshipit-source-id: 17af1965c409c2fe90cd85037fbd2c5a1364f82a",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_iter.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/72c8533f2c137170478270cb74b218095ced8904",
        "contains_optimization_keyword": true
    },
    {
        "hash": "01cfea66373d6ac7ee65c323fe7c2755d5d5dd7b",
        "author": "Siying Dong",
        "date": "2019-04-18T12:18:11-07:00",
        "message": "Some small code changes to improve Next() (#5200)\n\nSummary:\nSeveral small changes for Next():\n1. Reducing branching by always update local_stats_.next_count_++ even if statistics is null. This should be faster than a branching.\n2. Replacing ResetInternalKeysSkippedCounter() in Next() because the valid_ check is not needed in this case.\n3. iter_->Valid() should always be true for non merge case. Remove this check.\n4. Adding an inline annotation. It ends up with not picked up by my compiler, but it shouldn't hurt.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5200\n\nDifferential Revision: D15000391\n\nPulled By: siying\n\nfbshipit-source-id: be97f61c708968234fb8e5cf272b5c2ac07dc4dd",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_iter.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/01cfea66373d6ac7ee65c323fe7c2755d5d5dd7b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "106a94af1552ba6eb89cfe08fd4f92c2078af463",
        "author": "Siying Dong",
        "date": "2019-03-28T13:16:02-07:00",
        "message": "Improve obsolete_files_test (#5125)\n\nSummary:\nWe see a failure of obsolete_files_test but aren't able to identify\nthe issue. Improve the test in following way and hope we can debug\nbetter next time:\n1. Place sync point before automatic compaction runs so race condition\n   will always trigger.\n2. Disable sync point before test finishes.\n3. ASSERT_OK() instead of ASSERT_TRUE(status.ok())\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5125\n\nDifferential Revision: D14669456\n\nPulled By: siying\n\nfbshipit-source-id: dccb7648e334501ad651eb212880096eef1f4ab2",
        "modified_files_count": 1,
        "modified_files": [
            "db/obsolete_files_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/106a94af1552ba6eb89cfe08fd4f92c2078af463",
        "contains_optimization_keyword": true
    },
    {
        "hash": "18d2e4beb77abcacd5d89ee45be8c51b4851bd1f",
        "author": "Andrew Kryczka",
        "date": "2019-03-01T11:19:03-08:00",
        "message": "Run db_bench on database generated externally (#5017)\n\nSummary:\nAdded an option, `-use_existing_keys`, which can be set to run\nbenchmarks against an arbitrary existing database. Now users can\nbenchmark against their actual database rather than synthetic data.\n\nBefore the run begins, it loads all the keys into memory, then uses that\nset of keys rather than synthesizing new ones in `GenerateKeyFromInt`.\nThis is mainly intended for small-scale DBs where the memory consumption\nis not a concern.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5017\n\nDifferential Revision: D14270303\n\nPulled By: riversand963\n\nfbshipit-source-id: 6328df9dffb5e19170270dd00a69f4bbe424e5ed",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/18d2e4beb77abcacd5d89ee45be8c51b4851bd1f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f83eecff99889883349ecec237b1a78edb1ad09f",
        "author": "Levi Tamasi",
        "date": "2019-02-28T11:54:24-08:00",
        "message": "Introduce an enum for flag types in LRUHandle (#5024)\n\nSummary:\nReplace the integers used for setting and querying the various\nflags in LRUHandle with enum values to improve readability.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/5024\n\nDifferential Revision: D14263429\n\nPulled By: ltamasi\n\nfbshipit-source-id: b1b9ba95635265f122c2b40da73850eaac18227a",
        "modified_files_count": 1,
        "modified_files": [
            "cache/lru_cache.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f83eecff99889883349ecec237b1a78edb1ad09f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d9c9f3c8093f703316e7f93931580c297d074457",
        "author": "Siying Dong",
        "date": "2019-02-05T17:20:02-08:00",
        "message": "db_bench: fix \"micros/op\" reporting (#4949)\n\nSummary:\nhttps://github.com/facebook/rocksdb/commit/4985a9f73b9fb8a0323fbbb06222ae1f758a6b1d#diff-e5276985b26a0551957144f4420a594bR511\nchanges the meaning of latency reporting from running time per query, to elapse_time / #ops, without providing a reason why.\nConsidering that this is a counter-intuitive reporting, Reverting the change.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4949\n\nDifferential Revision: D13964684\n\nPulled By: siying\n\nfbshipit-source-id: d6304d3d4b5a802daa292302623c7dbca9a680bc",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d9c9f3c8093f703316e7f93931580c297d074457",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7d13f307ff189c25a6e51f1782b3addd83859989",
        "author": "Siying Dong",
        "date": "2019-01-15T16:46:04-08:00",
        "message": "Improve Error Message When wal_dir doesn't exist (#4874)\n\nSummary:\nRight now the error mesage when options.wal_dir doesn't exist is not helpful to users. Be more specific\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4874\n\nDifferential Revision: D13642425\n\nPulled By: siying\n\nfbshipit-source-id: 9a3172ed0f799af233b0f3b2e5e35bc7ce04c7b5",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl_open.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7d13f307ff189c25a6e51f1782b3addd83859989",
        "contains_optimization_keyword": true
    },
    {
        "hash": "de651035535b6d66276edc97c121cb507e9138cb",
        "author": "Yanqin Jin",
        "date": "2018-11-13T20:03:59-08:00",
        "message": "Improve result report of scan (#4648)\n\nSummary:\nWhen iterator becomes invalid, there are two possibilities.\nFirst, all data in the column family have been scanned and there is nothing\nmore to scan.\nSecond, an underlying error has occurred, causing `status()` to be !ok.\nTherefore, we need to check for both cases when `!iter->Valid()`.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4648\n\nDifferential Revision: D12959601\n\nPulled By: riversand963\n\nfbshipit-source-id: 49c9382c9ea9e78f2e2b6f3708f0670b822ca8dd",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/de651035535b6d66276edc97c121cb507e9138cb",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d8df169b8498609eda28c0d6c2d91588b0aa925b",
        "author": "Zhongyi Xie",
        "date": "2018-11-13T17:08:34-08:00",
        "message": "release db mutex when calling ApproximateSize (#4630)\n\nSummary:\n`GenSubcompactionBoundaries` calls `VersionSet::ApproximateSize` which gets BlockBasedTableReader for every file and seeks in its index block to find `key`'s offset. If the table or index block aren't in memory already, this involves I/O. This can be improved by releasing DB mutex when calling ApproximateSize.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4630\n\nDifferential Revision: D13052653\n\nPulled By: miasantreble\n\nfbshipit-source-id: cae31d46d10d0860fa8a26b8d5154b2d17d1685f",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction_job.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d8df169b8498609eda28c0d6c2d91588b0aa925b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a2de8e52bb6c13baf5f2323eba0ca356f1294f88",
        "author": "Simon Liu",
        "date": "2018-11-13T14:39:03-08:00",
        "message": "optimized the performance of autovector::emplace_back. (#4606)\n\nSummary:\nIt called the autovector::push_back simply in autovector::emplace_back.\nThis was not efficient, and then optimazed this function through the\nperfect forwarding.\n\nThis was the src and result of the benchmark(using the google'benchmark library, the type of elem in\nautovector was std::string, and call emplace_back with the \"char *\" type):\n\nhttps://gist.github.com/monadbobo/93448b89a42737b08cbada81de75c5cd\n\nPS: The benchmark's result of  previous PR was not accurate, and so I update the test case and result.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4606\n\nDifferential Revision: D13046813\n\nPulled By: sagar0\n\nfbshipit-source-id: 19cde1bcadafe899aa454b703acb35737a1cc02d",
        "modified_files_count": 1,
        "modified_files": [
            "util/autovector.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a2de8e52bb6c13baf5f2323eba0ca356f1294f88",
        "contains_optimization_keyword": true
    },
    {
        "hash": "92b440156686ec0cac5adb9833b9f6f4447ee479",
        "author": "Yanqin Jin",
        "date": "2018-10-29T09:45:32-07:00",
        "message": "Avoid memtable cut when active memtable is empty (#4595)\n\nSummary:\nFor flush triggered by RocksDB due to memory usage approaching certain\nthreshold (WriteBufferManager or Memtable full), we should cut the memtable\nonly when the current active memtable is not empty, i.e. contains data. This is\nwhat we do for non-atomic flush. If we always cut memtable even when the active\nmemtable is empty, we will generate extra, empty immutable memtable.\nThis is not ideal since it may cause write stall. It also causes some\nDBAtomicFlushTest to fail because cfd->imm()->NumNotFlushed() is different from\nexpectation.\n\nTest plan\n```\n$make clean && make J=1 -j32 all check\n$make clean && OPT=\"-DROCKSDB_LITE -g\" make J=1 -j32 all check\n$make clean && TEST_TMPDIR=/dev/shm/rocksdb OPT=-g make J=1 -j32 valgrind_test\n```\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4595\n\nDifferential Revision: D12818520\n\nPulled By: riversand963\n\nfbshipit-source-id: d867bdbeacf4199fdd642debb085f94703c41a18",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl_write.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/92b440156686ec0cac5adb9833b9f6f4447ee479",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b41b2d431e0757b5651c4ce1133de9284d9635de",
        "author": "Yanqin Jin",
        "date": "2018-10-04T14:53:36-07:00",
        "message": "Improve error message when opening file for truncation (#4454)\n\nSummary:\nThe old error message was misleading because it led people to believe the truncation operation failed.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4454\n\nDifferential Revision: D10203575\n\nPulled By: riversand963\n\nfbshipit-source-id: c76482a132566635cb55d4c73d45c461f295ec43",
        "modified_files_count": 1,
        "modified_files": [
            "util/fault_injection_test_env.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b41b2d431e0757b5651c4ce1133de9284d9635de",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a1f6142f38fc2130a522cf78e69ebc502f69bf92",
        "author": "JiYou",
        "date": "2018-10-03T18:40:59-07:00",
        "message": "VersionSet: GetOverlappingInputs() fix overflow and optimize. (#4385)\n\nSummary:\nThis fix is for `level == 0` in `GetOverlappingInputs()`:\n- In `GetOverlappingInputs()`, if `level == 0`, it has potential\nrisk of overflow if `i == 0`.\n- Optmize process when `expand = true`, the expected complexity\ncan be reduced to O(n).\n\nSigned-off-by: JiYou <jiyou09@gmail.com>\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4385\n\nDifferential Revision: D10181001\n\nPulled By: riversand963\n\nfbshipit-source-id: 46eef8a1d1605c9329c164e6471cd5c5b6de16b5",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a1f6142f38fc2130a522cf78e69ebc502f69bf92",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3c350a7cf0f4c19d0b420b730df46e19699014b3",
        "author": "Abhishek Madan",
        "date": "2018-09-21T16:13:08-07:00",
        "message": "Improve RangeDelAggregator benchmarks (#4395)\n\nSummary:\nImprove time measurements for AddTombstones to only include the\ncall and not the VectorIterator setup. Also add a new\nadd_tombstones_per_run flag to call AddTombstones multiple times per\naggregator, which will help simulate more realistic workloads.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4395\n\nDifferential Revision: D9996811\n\nPulled By: abhimadan\n\nfbshipit-source-id: 5865a95c323fbd9b3606493013664b4890fe5a02",
        "modified_files_count": 1,
        "modified_files": [
            "db/range_del_aggregator_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3c350a7cf0f4c19d0b420b730df46e19699014b3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "82e8e9e26bb16d1af07a26741bcf63d8342e4336",
        "author": "JiYou",
        "date": "2018-09-14T19:43:04-07:00",
        "message": "VersionBuilder: optmize SaveTo() to linear time. (#4366)\n\nSummary:\nBecause `base_files` and `added_files` both are sorted, using a merge\noperation to these two sorted arrays is more effective. The complexity\nis reduced to linear time.\n\n    - optmize the merge complexity.\n    - move the `NDEBUG` of sorted `added_files` out of merge process.\n\nSigned-off-by: JiYou <jiyou09@gmail.com>\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4366\n\nDifferential Revision: D9833592\n\nPulled By: ajkr\n\nfbshipit-source-id: dd32b67ebdca4c20e5e9546ab8082cecefe99fd0",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/82e8e9e26bb16d1af07a26741bcf63d8342e4336",
        "contains_optimization_keyword": true
    },
    {
        "hash": "82057b0d8f94770acc82ea8d68fa16a23ab346a9",
        "author": "Yanqin Jin",
        "date": "2018-09-14T11:12:52-07:00",
        "message": "Improve type conversion (#4367)\n\nSummary:\nUse `static_cast<type>(var)` instead of `(type)var`.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4367\n\nDifferential Revision: D9833391\n\nPulled By: riversand963\n\nfbshipit-source-id: 3d33fc2c290e7e0f3d1d45b256a881d1bc5a7df2",
        "modified_files_count": 1,
        "modified_files": [
            "db/log_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/82057b0d8f94770acc82ea8d68fa16a23ab346a9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3e801e5ed1c06619fed4bd0acabda1b9fab3a01b",
        "author": "Yi Wu",
        "date": "2018-08-30T11:57:46-07:00",
        "message": "BlobDB: Improve info log (#4324)\n\nSummary:\nImprove BlobDB info logs.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4324\n\nDifferential Revision: D9545074\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 678ab8820a78758fee451be3b123b0680c1081df",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/blob_db/blob_db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3e801e5ed1c06619fed4bd0acabda1b9fab3a01b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d00e5de7fc85a3696642d14ba57568f4c490b810",
        "author": "Wez Furlong",
        "date": "2018-08-29T20:27:43-07:00",
        "message": "use atomic O_CLOEXEC when available (#4328)\n\nSummary:\nIn our application we spawn helper child processes concurrently with\nopening rocksdb.  In one situation I observed that the child process had inherited\nthe rocksdb lock file as well as directory handles to the rocksdb storage location.\n\nThe code in env_posix takes care to set CLOEXEC but doesn't use `O_CLOEXEC` at the\ntime that the files are opened which means that there is a window of opportunity\nto leak the descriptors across a fork/exec boundary.\n\nThis diff introduces a helper that can conditionally set the `O_CLOEXEC` bit for\nthe open call using the same logic as that in the existing helper for setting\nthat flag post-open.\n\nI've preserved the post-open logic for systems that don't have `O_CLOEXEC`.\n\nI've introduced setting `O_CLOEXEC` for what appears to be a number of temporary\nor transient files and directory handles; I suspect that none of the files\nopened by Rocks are intended to be inherited by a forked child process.\n\nIn one case, `fopen` is used to open a file.  I've added the use of the glibc-specific `e`\nmode to turn on `O_CLOEXEC` for this case.  While this doesn't cover all posix systems,\nit is an improvement for our common deployment system.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4328\n\nReviewed By: ajkr\n\nDifferential Revision: D9553046\n\nPulled By: wez\n\nfbshipit-source-id: acdb89f7a85ca649b22fe3c3bd76f82142bec2bf",
        "modified_files_count": 1,
        "modified_files": [
            "env/env_posix.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d00e5de7fc85a3696642d14ba57568f4c490b810",
        "contains_optimization_keyword": true
    },
    {
        "hash": "da40d45267338ac0e91e5d065acab8df2c69cef3",
        "author": "Fenggang Wu",
        "date": "2018-08-23T10:12:58-07:00",
        "message": "DataBlockHashIndex: avoiding expensive iiter->Next when handling hash kNoEntry (#4296)\n\nSummary:\nWhen returning `kNoEntry` from HashIndex lookup, previously we invalidate the\n`biter` by set `current_=restarts_`, so that the search can continue to the next\nblock in case the search result may reside in the next block.\n\nThere is one problem: when we are searching for a missing key, if the search\nfinds a `kNoEntry` and continue the search to the next block, there is also a\nnon-trivial possibility that the HashIndex return `kNoEntry` too, and the\nexpensive index iterator `Next()` will happen several times for nothing.\n\nThe solution is that if the hash table returns `kNoEntry`, `SeekForGetImpl()` just search the last restart interval for the key. It will stop at the first key that is large than the seek_key, or to the end of the block, and each case will be handled correctly.\n\nMicrobenchmark script:\n```\nTEST_TMPDIR=/dev/shm ./db_bench --benchmarks=fillseq,readtocache,readmissing \\\n          --cache_size=20000000000  --use_data_block_hash_index={true|false}\n```\n\n`readmissing` performance (lower is better):\n```\nbinary:                      3.6098 micros/op\nhash (before applying diff): 4.1048 micros/op\nhash (after  applying diff): 3.3502 micros/op\n```\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4296\n\nDifferential Revision: D9419159\n\nPulled By: fgwu\n\nfbshipit-source-id: 21e3eedcccbc47a249aa8eb4bf405c9def0b8a05",
        "modified_files_count": 1,
        "modified_files": [
            "table/block.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/da40d45267338ac0e91e5d065acab8df2c69cef3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "640cfa7c3316741a0d04ef5163810f710dc44df8",
        "author": "Fenggang Wu",
        "date": "2018-08-21T17:12:45-07:00",
        "message": "DataBlockHashIndex: fix comment in NumRestarts() (#4286)\n\nSummary:\nImprove the description of the backward compatibility check in NumRestarts()\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4286\n\nDifferential Revision: D9412490\n\nPulled By: fgwu\n\nfbshipit-source-id: ea7dd5c61d8ff8eacef623b729d4e4fd53cca066",
        "modified_files_count": 1,
        "modified_files": [
            "table/block.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/640cfa7c3316741a0d04ef5163810f710dc44df8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fa4de6e30ffaf9188a48f5e30d2da1ac0e454917",
        "author": "Andrey Zagrebin",
        "date": "2018-08-17T10:57:25-07:00",
        "message": "#3865 followup for fix performance degression introduced by switching order of operands (#4284)\n\nSummary:\nFollowup for #4266. There is one more place in **get_context.cc** where **MergeOperator::ShouldMerge** should be called with reversed list of operands.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4284\n\nDifferential Revision: D9380008\n\nPulled By: sagar0\n\nfbshipit-source-id: 70ec26e607e5b88465e1acbdcd6c6171bd76b9f2",
        "modified_files_count": 1,
        "modified_files": [
            "table/get_context.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fa4de6e30ffaf9188a48f5e30d2da1ac0e454917",
        "contains_optimization_keyword": true
    },
    {
        "hash": "eb8885a08a688e08131c2a7acb94bca746de6789",
        "author": "Maysam Yabandeh",
        "date": "2018-08-08T17:43:00-07:00",
        "message": "Return correct usable_size for BlockContents (#4246)\n\nSummary:\nIf jemalloc is disabled or the API is incorrectly referenced (jemalloc api on windows have a prefix je_) memory usage is incorrectly reported for all block sizes. This is because sizeof(char) is always 1. sizeof() is calculated at compile time and *(char*) is char. The patch uses the size of the slice to fix that.\nFixes #4245\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4246\n\nDifferential Revision: D9233958\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 9646933b24504e2814c7379f06a31148829c6b4e",
        "modified_files_count": 1,
        "modified_files": [
            "table/format.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/eb8885a08a688e08131c2a7acb94bca746de6789",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4bb1e239b5d348400b682519ee74a7ee198fd38f",
        "author": "Siying Dong",
        "date": "2018-07-18T10:43:54-07:00",
        "message": "Cap concurrent arena's shard block size to 128KB (#4147)\n\nSummary:\nUsers sometime see their memtable size far smaller than expected. They probably have hit a fragementation of shard blocks. Cap their size anyway to reduce the impact of problem. 128KB is conservative so I don't imagine it can cause any performance problem.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4147\n\nDifferential Revision: D8886706\n\nPulled By: siying\n\nfbshipit-source-id: 8528a2a4196aa4457274522e2565fd3ff28f621e",
        "modified_files_count": 1,
        "modified_files": [
            "util/concurrent_arena.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4bb1e239b5d348400b682519ee74a7ee198fd38f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "21171615c10ee1a636ea28f2303a93a4bc39dbde",
        "author": "Yanqin Jin",
        "date": "2018-07-13T17:27:39-07:00",
        "message": "Reduce execution time of IngestFileWithGlobalSeqnoRandomized (#4131)\n\nSummary:\nMake `ExternalSSTFileTest.IngestFileWithGlobalSeqnoRandomized` run faster.\n\n`make format`\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4131\n\nDifferential Revision: D8839952\n\nPulled By: riversand963\n\nfbshipit-source-id: 4a7e842fde1cde4dc902e928a1cf511322578521",
        "modified_files_count": 1,
        "modified_files": [
            "db/external_sst_file_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/21171615c10ee1a636ea28f2303a93a4bc39dbde",
        "contains_optimization_keyword": true
    },
    {
        "hash": "dbeaa0d397fd2d26e105817242782024d1e607b7",
        "author": "Yanqin Jin",
        "date": "2018-07-12T14:42:39-07:00",
        "message": "Reduce #iterations to shorten execution time. (#4123)\n\nSummary:\nReduce #iterations from 5000 to 1000 so that\n`ExternalSSTFileTest.CompactDuringAddFileRandom` can finish faster.\nOn the one hand, 5000 iterations does not seem to improve the quality of unit\ntest in comparison with 1000. On the other hand, long running tests should belong to stress tests.\nPull Request resolved: https://github.com/facebook/rocksdb/pull/4123\n\nDifferential Revision: D8822514\n\nPulled By: riversand963\n\nfbshipit-source-id: 0f439b8d5ccd9a4aed84638f8bac16382de17245",
        "modified_files_count": 1,
        "modified_files": [
            "db/external_sst_file_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/dbeaa0d397fd2d26e105817242782024d1e607b7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "39218a72a4fb728bc2892af684b9a9019cb15dd0",
        "author": "Yanqin Jin",
        "date": "2018-07-05T11:45:11-07:00",
        "message": "Increase the size of LRU cache. (#4090)\n\nSummary:\nIncrease the size of each shard so that the number of cache hit/miss match\nexpectation. Otherwise FilterBlockInBlockCache test will fail.\nCloses https://github.com/facebook/rocksdb/pull/4090\n\nDifferential Revision: D8736158\n\nPulled By: riversand963\n\nfbshipit-source-id: 5cdbc06b02390389fd5b72a6d251d88949ad3d91",
        "modified_files_count": 1,
        "modified_files": [
            "table/table_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/39218a72a4fb728bc2892af684b9a9019cb15dd0",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2462763b2e77067936b97e7ef661356f3fd649ce",
        "author": "Maysam Yabandeh",
        "date": "2018-06-29T09:28:12-07:00",
        "message": "Fix mis-spoken assert on prefetch_filter and prefetch_index (#4077)\n\nSummary:\nWe can have prefetch_index without prefetch_filter but not the other way around. The assert statement is fixed.\nCloses https://github.com/facebook/rocksdb/pull/4077\n\nDifferential Revision: D8694472\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: ccd2804d9d9cdafb1c3e65062c7bc38603e69004",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2462763b2e77067936b97e7ef661356f3fd649ce",
        "contains_optimization_keyword": true
    },
    {
        "hash": "25403c2265cb700462d59fa3cb9dbec85d25d48f",
        "author": "Andrew Kryczka",
        "date": "2018-06-28T13:20:29-07:00",
        "message": "Prefetch cache lines for filter lookup (#4068)\n\nSummary:\nSince the filter data is unaligned, even though we ensure all probes are within a span of `cache_line_size` bytes, those bytes can span two cache lines. In that case I doubt hardware prefetching does a great job considering we don't necessarily access those two cache lines in order. This guess seems correct since adding explicit prefetch instructions reduced filter lookup overhead by 19.4%.\nCloses https://github.com/facebook/rocksdb/pull/4068\n\nDifferential Revision: D8674189\n\nPulled By: ajkr\n\nfbshipit-source-id: 747427d9a17900151c17820488e3f7efe06b1871",
        "modified_files_count": 1,
        "modified_files": [
            "util/bloom.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/25403c2265cb700462d59fa3cb9dbec85d25d48f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fbe3b9e2b61c952782821d5dd5e898346d5dee53",
        "author": "Fenggang Wu",
        "date": "2018-06-15T10:42:21-07:00",
        "message": "Udpate db_universal_compaction_test according to PR #3970 (#3995)\n\nSummary:\nThe SST file sizes changed slightly after the improvement of PR #3970\nwhich reduces the size of the properties block. Before PR #3970 a size\nratio compaction included all of the first four flushed files but it\nonly includes two files after. We increase the size_ratio universal\ncompaction option to make that compaction include all four files again.\nCloses https://github.com/facebook/rocksdb/pull/3995\n\nDifferential Revision: D8426925\n\nPulled By: fgwu\n\nfbshipit-source-id: 1429c38672e9f4fb4d4881fd4b06db45c4861d62",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_universal_compaction_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fbe3b9e2b61c952782821d5dd5e898346d5dee53",
        "contains_optimization_keyword": true
    },
    {
        "hash": "dd216dd76a35c2e512e1501cd16607f4ec61de2e",
        "author": "Andrew Kryczka",
        "date": "2018-06-13T13:43:23-07:00",
        "message": "Choose unique keys faster in db_stress (#3990)\n\nSummary:\ndb_stress initialization randomly chooses a set of keys to not overwrite. It was doing it separately for each column family. That caused 30+ second initialization times for the non-simple crash tests, which have 10 CFs. This PR:\n\n- reuses the same set of randomly chosen no-overwrite keys across all CFs\n- logs a couple more timestamps so we can more easily see initialization time\nCloses https://github.com/facebook/rocksdb/pull/3990\n\nDifferential Revision: D8393821\n\nPulled By: ajkr\n\nfbshipit-source-id: d0b263a298df607285ffdd8b0983ff6575cc6c34",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/dd216dd76a35c2e512e1501cd16607f4ec61de2e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "812c7371d3ac3840b1f88c59e7f514407ae45dc4",
        "author": "Mike Kolupaev",
        "date": "2018-06-05T11:43:16-07:00",
        "message": "Fix performance regression in Get() for block-based tables (#3953)\n\nSummary:\nThis fixes a regression in one of myrocks regression tests (readwhilewriting), introduced in https://github.com/facebook/rocksdb/commit/8bf555f487d1de84a4fb19cb97b9ae1a8dbebc60\n\nThis PR changes two lines of code: one of them actually fixes the observed regression, the other is a mostly unrelated small fix that I'm piggy-backing here. EDIT: Nevermind, it fixes one line. More details in inline comments.\nCloses https://github.com/facebook/rocksdb/pull/3953\n\nDifferential Revision: D8270664\n\nPulled By: al13n321\n\nfbshipit-source-id: a7d91e196807d1e816551591257c700f70e4ccac",
        "modified_files_count": 1,
        "modified_files": [
            "table/block.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/812c7371d3ac3840b1f88c59e7f514407ae45dc4",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2a0dfaa0446c546a474959fe3f75a13a4fcc3e23",
        "author": "Zhongyi Xie",
        "date": "2018-05-31T21:42:50-07:00",
        "message": "fix PrefixExtractorChanged: pass raw pointer instead shared_ptr\n\nSummary:\nThis should resolve the performance regression caused by the unnecessary copying of the shared_ptr.\nCloses https://github.com/facebook/rocksdb/pull/3937\n\nDifferential Revision: D8232330\n\nPulled By: miasantreble\n\nfbshipit-source-id: 7885bf7cd190b6f87164c52d6edd328298c13f97",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2a0dfaa0446c546a474959fe3f75a13a4fcc3e23",
        "contains_optimization_keyword": true
    },
    {
        "hash": "aed7abbcca7574ed438c3b22e27db9bf249e8de3",
        "author": "Xin Tong",
        "date": "2018-05-17T17:57:48-07:00",
        "message": "Reorder field based on esan data\n\nSummary:\nRunning. TEST_TMPDIR=/dev/shm ./buck-out/gen/rocks/tools/rocks_db_bench --benchmarks=readwhilewriting --num=5000000 -benchmark_write_rate_limit=2000000 --threads=32\n\nCollected esan data and reorder field. Accesses to 4th and 6th fields take majority of the access.  Group them. Overall, this struct takes 10%+ of the total accesses in the program. (637773011/6107964986)\n\n==2433831==  class rocksdb::InlineSkipList\n==2433831==   size = 48, count = 637773011, ratio = 112412, array access = 0\n==2433831==   # 0: offset = 0,   size = 2,       count = 455137, type = i16\n==2433831==   # 1: offset = 2,   size = 2,       count = 6,      type = i16\n==2433831==   # 2: offset = 4,   size = 4,       count = 182303, type = i32\n==2433831==   # 3: offset = 8,   size = 8,       count = 263953900, type = %\"class.rocksdb::MemTableRep::KeyComparator\"*\n==2433831==   # 4: offset = 16,  size = 8,       count = 136409, type = %\"class.rocksdb::Allocator\"*\n==2433831==   # 5: offset = 24,  size = 8,       count = 366628820, type = %\"struct.rocksdb::InlineSkipList<const rocksdb::MemTableRep::KeyComparator &>::Node\"*\n==2433831==   # 6: offset = 32,  size = 4,       count = 6280031, type = %\"struct.std::atomic\" = type { %\"struct.std::__atomic_base\" }\n==2433831==   # 7: offset = 40,  size = 8,       count = 136405, type = %\"struct.rocksdb::InlineSkipList<const rocksdb::MemTableRep::KeyComparator &>::Splice\"*\n==2433831==EfficiencySanitizer: total struct field access count = 6107964986\n\nBefore re-ordering\n[trentxintong@devbig460.frc2 ~/fbsource/fbcode]$ fgrep readwhilewriting\nwithout-ro.log\nreadwhilewriting :       0.036 micros/op 27545605 ops/sec;   26.8 MB/s\n(45954 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 28024240 ops/sec;   27.2 MB/s\n(43158 of 5000000 found)\nreadwhilewriting :       0.037 micros/op 27345145 ops/sec;   27.1 MB/s\n(46725 of 5000000 found)\nreadwhilewriting :       0.037 micros/op 27072588 ops/sec;   27.3 MB/s\n(42605 of 5000000 found)\nreadwhilewriting :       0.034 micros/op 29578781 ops/sec;   28.3 MB/s\n(44294 of 5000000 found)\nreadwhilewriting :       0.035 micros/op 28528304 ops/sec;   27.7 MB/s\n(44176 of 5000000 found)\nreadwhilewriting :       0.037 micros/op 27075497 ops/sec;   26.5 MB/s\n(43763 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 28024117 ops/sec;   27.1 MB/s\n(40622 of 5000000 found)\nreadwhilewriting :       0.037 micros/op 27078709 ops/sec;   27.6 MB/s\n(47774 of 5000000 found)\nreadwhilewriting :       0.034 micros/op 29020689 ops/sec;   28.1 MB/s\n(45066 of 5000000 found)\nAVERAGE()=27.37 MB/s\n\nAfter re-ordering\n[trentxintong@devbig460.frc2 ~/fbsource/fbcode]$ fgrep readwhilewriting\nro.log\nreadwhilewriting :       0.036 micros/op 27542409 ops/sec;   27.7 MB/s\n(46163 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 28021148 ops/sec;   28.2 MB/s\n(46155 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 28021035 ops/sec;   27.3 MB/s\n(44039 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 27538659 ops/sec;   27.5 MB/s\n(46781 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 28028604 ops/sec;   27.6 MB/s\n(44689 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 27541452 ops/sec;   27.3 MB/s\n(43156 of 5000000 found)\nreadwhilewriting :       0.034 micros/op 29041338 ops/sec;   28.8 MB/s\n(44895 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 27784974 ops/sec;   26.3 MB/s\n(39963 of 5000000 found)\nreadwhilewriting :       0.036 micros/op 27538892 ops/sec;   28.1 MB/s\n(46570 of 5000000 found)\nreadwhilewriting :       0.038 micros/op 26622473 ops/sec;   27.0 MB/s\n(43236 of 5000000 found)\nAVERAGE()=27.58 MB/s\nCloses https://github.com/facebook/rocksdb/pull/3855\n\nReviewed By: siying\n\nDifferential Revision: D8048781\n\nPulled By: trentxintong\n\nfbshipit-source-id: bc9807a9845e2a92cb171ce1ecb5a2c8a51f1481",
        "modified_files_count": 1,
        "modified_files": [
            "memtable/inlineskiplist.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/aed7abbcca7574ed438c3b22e27db9bf249e8de3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fc522bdb3ebbcc9354407717733e1f886504a2b8",
        "author": "Maysam Yabandeh",
        "date": "2018-05-04T15:28:06-07:00",
        "message": "Evenly split HarnessTest.Randomized\n\nSummary:\nCurrently HarnessTest.Randomized is already split but some of the splits are faster than the others. The reason is that each split takes a continuous range of the generated args and the test with later args takes longer to finish. The patch evenly split the args among splits in a round robin fashion.\nBefore:\n```\n[       OK ] HarnessTest.Randomized1n2 (2278 ms)\n[       OK ] HarnessTest.Randomized3n4 (1095 ms)\n[       OK ] HarnessTest.Randomized5 (658 ms)\n[       OK ] HarnessTest.Randomized6 (1258 ms)\n[       OK ] HarnessTest.Randomized7 (6476 ms)\n[       OK ] HarnessTest.Randomized8 (8182 ms)\n```\nAfter\n```\n[       OK ] HarnessTest.Randomized1 (2649 ms)\n[       OK ] HarnessTest.Randomized2 (2645 ms)\n[       OK ] HarnessTest.Randomized3 (2577 ms)\n[       OK ] HarnessTest.Randomized4 (2490 ms)\n[       OK ] HarnessTest.Randomized5 (2553 ms)\n[       OK ] HarnessTest.Randomized6 (2560 ms)\n[       OK ] HarnessTest.Randomized7 (2501 ms)\n[       OK ] HarnessTest.Randomized8 (2574 ms)\n```\nCloses https://github.com/facebook/rocksdb/pull/3808\n\nDifferential Revision: D7882663\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 09b749a9684b6d7d65466aa4b00c5334a49e833e",
        "modified_files_count": 1,
        "modified_files": [
            "table/table_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fc522bdb3ebbcc9354407717733e1f886504a2b8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a8d77ca3819b098a2e0e7d69b15cef576e6b21ff",
        "author": "Maysam Yabandeh",
        "date": "2018-05-03T16:13:09-07:00",
        "message": "Speedup ManualCompactionTest.Test\n\nSummary:\nManualCompactionTest.Test occasionally times out in tsan flavor of our test infra. The patch reduces the number of keys to make the test run faster. The change does not seem to negatively impact the coverage of the test.\nCloses https://github.com/facebook/rocksdb/pull/3802\n\nDifferential Revision: D7865596\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: b4f60e32c3ae1677e25506f71c766e33fa985785",
        "modified_files_count": 1,
        "modified_files": [
            "db/manual_compaction_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a8d77ca3819b098a2e0e7d69b15cef576e6b21ff",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b4f333922ac4c73c1c3cc5222048e4f1cb436e64",
        "author": "Harry Wong",
        "date": "2018-04-16T16:58:20-07:00",
        "message": "Improve the comment on TableFactory::NewTableReader()\n\nSummary:\n`DBImpl::AddFile()` has been replaced by `DBImpl::IngestExternalFile()`.\nCloses https://github.com/facebook/rocksdb/pull/3726\n\nDifferential Revision: D7646875\n\nPulled By: ajkr\n\nfbshipit-source-id: 241eb7a8d88527fdc5c26b0c3f6faec3296451f8",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/table.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b4f333922ac4c73c1c3cc5222048e4f1cb436e64",
        "contains_optimization_keyword": true
    },
    {
        "hash": "28087acd796b2d14f35e4e49224a91ebea4c57d6",
        "author": "Amy Tai",
        "date": "2018-04-13T22:13:13-07:00",
        "message": "Implemented Knuth shuffle to construct permutation for selecting no_o\u2026\n\nSummary:\n\u2026verwrite_keys. Also changed each no_overwrite_key set to an unordered set, otherwise Knuth shuffle only gets you 2x time improvement, because insertion (and subsequent internal sorting) into an ordered set is the bottleneck.\n\nWith this change, each iteration of permutation construction and prefix selection takes around 40 secs, as opposed to 360 secs previously. However, this still means that with the default 10 CF per blackbox test case, the test is going to time out given the default interval of 200 secs.\n\nAlso, there is currently an assertion error affecting all blackbox tests in db_crashtest.py; this assertion error will be fixed in a future PR.\nCloses https://github.com/facebook/rocksdb/pull/3699\n\nDifferential Revision: D7624616\n\nPulled By: amytai\n\nfbshipit-source-id: ea64fbe83407ff96c1c0ecabbc6c830576939393",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/28087acd796b2d14f35e4e49224a91ebea4c57d6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "0a0fad447bc46bb7169f4ca86d5942f2a445a5ed",
        "author": "Andrew Kryczka",
        "date": "2018-02-12T14:57:13-08:00",
        "message": "db_bench separate options for partition index and filters\n\nSummary:\nSome workloads (like my current benchmarking) may want partitioned indexes without partitioned filters. Particularly, when `-optimize_filters_for_hits=true`, the total index size may be larger than the total filter size, so it can make sense to hold all filters in-memory but not all indexes.\nCloses https://github.com/facebook/rocksdb/pull/3492\n\nDifferential Revision: D6970092\n\nPulled By: ajkr\n\nfbshipit-source-id: b7fa1828e1d13829339aefb90fd56eb7c5337f61",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/0a0fad447bc46bb7169f4ca86d5942f2a445a5ed",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b78ed0460b7f16013e62f72a0253eb98220a0965",
        "author": "Andrew Kryczka",
        "date": "2018-02-01T09:42:09-08:00",
        "message": "fix ReadaheadRandomAccessFile/iterator prefetch bug\n\nSummary:\n`ReadaheadRandomAccessFile` is used by iterators for file reads in several cases, like in compaction when `compaction_readahead_size > 0` or `use_direct_io_for_flush_and_compaction == true`, or in user iterator when `ReadOptions::readahead_size > 0`. `ReadaheadRandomAccessFile` maintains an internal buffer for readahead data. It assumes that, if the buffer's length is less than `ReadaheadRandomAccessFile::readahead_size_`, which is fixed in the constructor, then EOF has been reached so it doesn't try reading further.\n\nRecently, d938226af405681c592f25310f41c0c933bcdb19 started calling `RandomAccessFile::Prefetch` with various lengths: 8KB, 16KB, etc. When the `RandomAccessFile` is a `ReadaheadRandomAccessFile`, it triggers the above condition and incorrectly determines EOF. If a block is partially in the readahead buffer and EOF is incorrectly decided, the result is a truncated data block.\n\nThe problem is reproducible:\n\n```\nTEST_TMPDIR=/data/compaction_bench ./db_bench -benchmarks=fillrandom -write_buffer_size=1048576 -target_file_size_base=1048576 -block_size=18384 -use_direct_io_for_flush_and_compaction=true\n...\nput error: Corruption: truncated block read from /data/compaction_bench/dbbench/000014.sst offset 20245, expected 10143 bytes, got 8427\n```\nCloses https://github.com/facebook/rocksdb/pull/3454\n\nDifferential Revision: D6869405\n\nPulled By: ajkr\n\nfbshipit-source-id: 87001c299e7600a37c0dcccbd0368e0954c929cf",
        "modified_files_count": 1,
        "modified_files": [
            "util/file_reader_writer.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b78ed0460b7f16013e62f72a0253eb98220a0965",
        "contains_optimization_keyword": true
    },
    {
        "hash": "30a017fecae60aa7b87c4a1e283b6ac027724a92",
        "author": "Yi Wu",
        "date": "2018-01-05T16:41:58-08:00",
        "message": "Blob DB: avoid having a separate read of checksum\n\nSummary:\nPreviously on a blob db read, we are making a read of the blob value, and then make another read to get CRC checksum. I'm combining the two read into one.\n\nreadrandom db_bench with 1G database with base db size of 13M, value size 1k:\n`./db_bench --db=/home/yiwu/tmp/db_bench --use_blob_db --value_size=1024 --num=1000000 --benchmarks=readrandom --use_existing_db --cache_size=32000000`\nmaster: throughput 234MB/s, get micros p50 5.984 p95 9.998 p99 20.817 p100 787\nthis PR: throughput 261MB/s, get micros p50 5.157 p95 9.928 p99 20.724 p100 190\nCloses https://github.com/facebook/rocksdb/pull/3301\n\nDifferential Revision: D6615950\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 052410c6d8539ec0cc305d53793bbc8f3616baa3",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/blob_db/blob_db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/30a017fecae60aa7b87c4a1e283b6ac027724a92",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4634c735a8bb4f83b8099928fb12b50ad8df7b88",
        "author": "Alex Robinson",
        "date": "2017-12-04T01:56:15-08:00",
        "message": "Update DBOptions::IncreaseParallelism to use newer background settings\n\nSummary:\nThe Options header file recommends using max_background_jobs rather than\ndirectly setting max_background_compactions or max_background_flushes.\n\nI've personally seen a performance problem where stalls were happening\nbecause the one background flushing thread was blocked that was fixed\nby this change -\nhttps://github.com/cockroachdb/cockroach/issues/19699#issuecomment-347672485\nCloses https://github.com/facebook/rocksdb/pull/3208\n\nDifferential Revision: D6473178\n\nPulled By: ajkr\n\nfbshipit-source-id: 67c892ceb7b1909d251492640cb15a0f2262b7ed",
        "modified_files_count": 1,
        "modified_files": [
            "options/options.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4634c735a8bb4f83b8099928fb12b50ad8df7b88",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ed3af9ef9957866300c144cd9e60a60e2a490583",
        "author": "Andrew Kryczka",
        "date": "2017-11-28T17:28:58-08:00",
        "message": "improve ldb CLI option support\n\nSummary:\n- Made CLI arguments take precedence over options file when both are provided. Note some of the CLI args are not settable via options file, like `--compression_max_dict_bytes`, so it's necessary to allow both ways of providing options simultaneously.\n- Changed `PrepareOptionsForOpenDB` to update the proper `ColumnFamilyOptions` if one exists for the user's `--column_family_name` argument. I supported this only in the base class, `LDBCommand`, so it works for the general arguments. Will defer adding support for subcommand-specific arguments.\n- Made the command fail if `--try_load_options` is provided and loading options file returns NotFound. I found the previous behavior of silently continuing confusing.\nCloses https://github.com/facebook/rocksdb/pull/3144\n\nDifferential Revision: D6270544\n\nPulled By: ajkr\n\nfbshipit-source-id: 7c2eac9f9b38720523d74466fb9e78db53561367",
        "modified_files_count": 1,
        "modified_files": [
            "tools/ldb_cmd.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ed3af9ef9957866300c144cd9e60a60e2a490583",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f8b5bb2fd82a39ab4c0b27d21bd492bc61177742",
        "author": "Andrew Kryczka",
        "date": "2017-10-20T14:11:52-07:00",
        "message": "remove unused code\n\nSummary:\nfixup 6a541afcc4d1e5b6e6d78e288b9bee3bb2a933b5. This code didn't do anything because (1) `bytes_per_sync` is assigned in `EnvOptions`'s constructor; and (2) `OptimizeForCompactionTableWrite`'s return value was ignored, even though its only purpose is to return something.\nCloses https://github.com/facebook/rocksdb/pull/3055\n\nDifferential Revision: D6114132\n\nPulled By: ajkr\n\nfbshipit-source-id: ea4831770930e9cf83518e13eb2e1934d1f5487c",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f8b5bb2fd82a39ab4c0b27d21bd492bc61177742",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b8cea7cc279fe609de85b7ce4f50d4ff4f90047f",
        "author": "Changli Gao",
        "date": "2017-10-17T10:12:37-07:00",
        "message": "VersionBuilder: Erase with iterators for better performance\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3007\n\nDifferential Revision: D6077701\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: a6fd5b8a23f4feb1660b9ce027f651a7e90352b3",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b8cea7cc279fe609de85b7ce4f50d4ff4f90047f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1a61ba179e9d8b55cdc27df5b03e184355901864",
        "author": "Jay Patel",
        "date": "2017-10-09T22:59:02-07:00",
        "message": "compaction picker to use max_bytes_for_level_multiplier_additional\n\nSummary:\nHi,\nAs part of some optimization, we're using multiple DB locations (tmpfs and spindle) to store data and configured max_bytes_for_level_multiplier_additional. But, max_bytes_for_level_multiplier_additional is not used to compute the actual size for the level while picking the DB location. So, even if DB location does not have space, RocksDB mistakenly puts the level at that location.\n\nCan someone pls. verify the fix? Let me know any other changes required.\n\nThanks,\nJay\nCloses https://github.com/facebook/rocksdb/pull/2704\n\nDifferential Revision: D5992515\n\nPulled By: ajkr\n\nfbshipit-source-id: cbbc6c0e0a7dbdca91c72e0f37b218c4cec57e28",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction_picker.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1a61ba179e9d8b55cdc27df5b03e184355901864",
        "contains_optimization_keyword": true
    },
    {
        "hash": "75f7f42d41d8659b43a839bd31570a81e562eb7e",
        "author": "Adam Kupczyk",
        "date": "2017-10-04T18:12:52-07:00",
        "message": "Added CPU prefetch for skiplist\n\nSummary:\nThis change causes following changes result of test:\n./db_bench --writes 10000000 --benchmarks=\"fillrandom\" --compression_type none\nfrom\nfillrandom   :       3.177 micros/op 314804 ops/sec;   34.8 MB/s\nto\nfillrandom   :       2.777 micros/op 360087 ops/sec;   39.8 MB/s\nCloses https://github.com/facebook/rocksdb/pull/2961\n\nDifferential Revision: D5977822\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 1ea77707bffa978b1592b0c5d0fe76bfa1930f8d",
        "modified_files_count": 1,
        "modified_files": [
            "memtable/inlineskiplist.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/75f7f42d41d8659b43a839bd31570a81e562eb7e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "821887036e5235c827029d14decb185bea01ec4b",
        "author": "Andrew Kryczka",
        "date": "2017-10-03T16:27:28-07:00",
        "message": "pin L0 filters/indexes for compaction outputs\n\nSummary:\nWe need to tell the iterator the compaction output file's level so it can apply proper optimizations, like pinning filter and index blocks when user enables `pin_l0_filter_and_index_blocks_in_cache` and the output file's level is zero.\nCloses https://github.com/facebook/rocksdb/pull/2949\n\nDifferential Revision: D5945597\n\nPulled By: ajkr\n\nfbshipit-source-id: 2389decf9026ffaa32d45801a77d002529f64a62",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction_job.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/821887036e5235c827029d14decb185bea01ec4b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "025b85b4ac078110302c039556e4c12ba8e7a731",
        "author": "Andrew Kryczka",
        "date": "2017-09-12T11:26:47-07:00",
        "message": "speedup DBTest.EncodeDecompressedBlockSizeTest\n\nSummary:\nit sometimes takes more than 10 minutes (i.e., times out) on our internal CI. mainly because bzip is super slow. so I reduced the amount of  work it tries to do.\nCloses https://github.com/facebook/rocksdb/pull/2856\n\nDifferential Revision: D5795883\n\nPulled By: ajkr\n\nfbshipit-source-id: e69f986ae60b44ecc26b6b024abd0f13bdf3a3c5",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/025b85b4ac078110302c039556e4c12ba8e7a731",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2dd22e54495b145a12931add763b7fcc4521ae10",
        "author": "Siying Dong",
        "date": "2017-09-11T12:04:21-07:00",
        "message": "Make DBIter class final\n\nSummary:\nDBIter is referenced in ArenaWrappedDBIter, which is a simple wrapper. If DBIter is final, some virtual function call can be avoided. Some functions can even be inlined, like DBIter.value() to ArenaWrappedDBIter.value() and DBIter.key() to ArenaWrappedDBIter.key(). The performance gain is hard to measure. I just ran the memory-only benchmark for readseq and saw it didn't regress. There shouldn't be any harm doing it. Just give compiler more choices.\nCloses https://github.com/facebook/rocksdb/pull/2859\n\nDifferential Revision: D5799888\n\nPulled By: siying\n\nfbshipit-source-id: 829788f91310c40282dcfb7e412e6ef489931143",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_iter.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2dd22e54495b145a12931add763b7fcc4521ae10",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ba3c58cab6c691c53c7f98589651233695da1f62",
        "author": "Kefu Chai",
        "date": "2017-09-07T12:40:57-07:00",
        "message": "specify SSE42 'target' attribute for Fast_CRC32()\n\nSummary:\nif we enable SSE42 globally when compiling the tree for preparing a\nportable binary, which could be running on CPU w/o SSE42 instructions\neven the GCC on the building host is able to emit SSE42 code, this leads\nto illegal instruction errors on machines not supporting SSE42. to solve\nthis problem, crc32 detects the supported instruction at runtime, and\nselects the supported CRC32 implementation according to the result of\n`cpuid`. but intrinics like \"_mm_crc32_u64()\" will not be available\nunless the \"target\" machine is appropriately specified in the command\nline, like \"-msse42\", or using the \"target\" attribute.\n\nwe could pass \"-msse42\" only when compiling crc32c.cc, and allow the\ncompiler to generate the SSE42 instructions, but we are still at the\nrisk of executing illegal instructions on machines does not support\nSSE42 if the compiler emits code that is not guarded by our runtime\ndetection. and we need to do the change in both Makefile and CMakefile.\n\nor, we can use GCC's \"target\" attribute to enable the machine specific\ninstructions on certain function. in this way, we have finer grained\ncontrol of the used \"target\". and no need to change the makefiles. so\nwe don't need to duplicate the changes on both makefile and cmake as\nthe previous approach.\n\nthis problem surfaces when preparing a package for GNU/Linux distribution,\nand we only applies to optimization for SSE42, so using a feature\nonly available on GCC/Clang is not that formidable.\nCloses https://github.com/facebook/rocksdb/pull/2807\n\nDifferential Revision: D5786084\n\nPulled By: siying\n\nfbshipit-source-id: bca5c0f877b8d6fb55f58f8f122254a26422843d",
        "modified_files_count": 1,
        "modified_files": [
            "util/crc32c.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ba3c58cab6c691c53c7f98589651233695da1f62",
        "contains_optimization_keyword": true
    },
    {
        "hash": "23593171c42e88ea1c6d288dd1ab6f2b65bdbbe1",
        "author": "Andrew Kryczka",
        "date": "2017-08-16T19:13:01-07:00",
        "message": "minor improvements to db_stress\n\nSummary:\nfix some things that made this command hard to use from CLI:\n\n- use default values for `target_file_size_base` and `max_bytes_for_level_base`. previously we were using small values for these but default value of `write_buffer_size`, which led to enormous number of L1 files.\n- failure message for `value_size_mult` too big. previously there was just an assert, so in non-debug mode it'd overrun the value buffer and crash mysteriously.\n- only print verification success if there's no failure. before it'd print both in the failure case.\n- support `memtable_prefix_bloom_size_ratio`\n- support `num_bottom_pri_threads` (universal compaction)\nCloses https://github.com/facebook/rocksdb/pull/2741\n\nDifferential Revision: D5629495\n\nPulled By: ajkr\n\nfbshipit-source-id: ddad97d6d4ba0884e7c0f933b0a359712514fc1d",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/23593171c42e88ea1c6d288dd1ab6f2b65bdbbe1",
        "contains_optimization_keyword": true
    },
    {
        "hash": "20dc5e74f276bdcb26c44c13bced506a2d920d3f",
        "author": "Sagar Vemuri",
        "date": "2017-08-05T00:15:35-07:00",
        "message": "Optimize range-delete aggregator call in merge helper.\n\nSummary:\nIn the condition:\n```\nif (range_del_agg != nullptr &&\n    range_del_agg->ShouldDelete(\n        iter->key(),\n        RangeDelAggregator::RangePositioningMode::kForwardTraversal) &&\n    filter != CompactionFilter::Decision::kRemoveAndSkipUntil) {\n...\n}\n```\nit could be possible that all the work done in `range_del_agg->ShouldDelete` is wasted due to not having the right `filter` value later on.\nInstead, check `filter` value before even calling `range_del_agg->ShouldDelete`, which is a much more involved function.\nCloses https://github.com/facebook/rocksdb/pull/2690\n\nDifferential Revision: D5568931\n\nPulled By: sagar0\n\nfbshipit-source-id: 17512d52360425c7ae9de7675383f5d7bc3dad58",
        "modified_files_count": 1,
        "modified_files": [
            "db/merge_helper.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/20dc5e74f276bdcb26c44c13bced506a2d920d3f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "58410aee44e902735659b80364eecc0e075676e9",
        "author": "Maysam Yabandeh",
        "date": "2017-08-03T10:43:28-07:00",
        "message": "Fix the overflow bug in AwaitState\n\nSummary:\nhttps://github.com/facebook/rocksdb/issues/2559 reports an overflow in AwaitState. nbronson has debugged the issue and presented the fix, which is applied to this patch. Moreover this patch adds more comments to clarify the logic in AwaitState.\n\nI tried with both 16 and 64 threads on update benchmark. The fix lowers cpu usage by 1.6 but also lowers the throughput by 1.6 and 2% respectively. Apparently the bug had favored using the spinning more often.\n\nBenchmarks:\nTEST_TMPDIR=/dev/shm/tmpdb time ./db_bench --benchmarks=\"fillrandom\" --threads=16 --num=2000000\nTEST_TMPDIR=/dev/shm/tmpdb time ./db_bench --use_existing_db=1 --benchmarks=\"updaterandom[X3]\" --threads=16 --num=2000000\nTEST_TMPDIR=/dev/shm/tmpdb time ./db_bench --use_existing_db=1 --benchmarks=\"updaterandom[X3]\" --threads=64 --num=200000\n\nResults\n$ cat update-16t-bug.txt | tail -4\nupdaterandom [AVG    3 runs] : 234117 ops/sec;   51.8 MB/sec\nupdaterandom [MEDIAN 3 runs] : 233581 ops/sec;   51.7 MB/sec\n3896.42user 1539.12system 6:50.61elapsed 1323%CPU (0avgtext+0avgdata 331308maxresident)k\n0inputs+0outputs (0major+1281001minor)pagefaults 0swaps\n$ cat update-16t-fixed.txt | tail -4\nupdaterandom [AVG    3 runs] : 230364 ops/sec;   51.0 MB/sec\nupdaterandom [MEDIAN 3 runs] : 226169 ops/sec;   50.0 MB/sec\n3865.46user 1568.32system 6:57.63elapsed 1301%CPU (0avgtext+0avgdata 315012maxresident)k\n0inputs+0outputs (0major+1342568minor)pagefaults 0swaps\n\n$ cat update-64t-bug.txt | tail -4\nupdaterandom [AVG    3 runs] : 261878 ops/sec;   57.9 MB/sec\nupdaterandom [MEDIAN 3 runs] : 262859 ops/sec;   58.2 MB/sec\n926.27user 578.06system 2:27.46elapsed 1020%CPU (0avgtext+0avgdata 475480maxresident)k\n0inputs+0outputs (0major+1058728minor)pagefaults 0swaps\n$ cat update-64t-fixed.txt | tail -4\nupdaterandom [AVG    3 runs] : 256699 ops/sec;   56.8 MB/sec\nupdaterandom [MEDIAN 3 runs] : 256380 ops/sec;   56.7 MB/sec\n933.47user 575.37system 2:30.41elapsed 1003%CPU (0avgtext+0avgdata 482340maxresident)k\n0inputs+0outputs (0major+1078557minor)pagefaults 0swaps\nCloses https://github.com/facebook/rocksdb/pull/2679\n\nDifferential Revision: D5553732\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 98b72dc3a8e0f22ea29d4f7c7790af10c369c5bb",
        "modified_files_count": 1,
        "modified_files": [
            "db/write_thread.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/58410aee44e902735659b80364eecc0e075676e9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3e5ea29a83270e8a0a93afe471022fc41adc8180",
        "author": "Islam AbdelRahman",
        "date": "2017-07-20T11:29:01-07:00",
        "message": "Fix Flaky DeleteSchedulerTest::ImmediateDeleteOn25PercDBSize\n\nSummary:\nIn this test we are deleting 100 files, and we are expecting DeleteScheduler to delete 26 files in the background and 74 files immediately in the foreground\n\nThe main purpose of the test is to make sure that we delete files in foreground thread, which is verified in line 546\n\nBut sometimes we may end up with 26 files or 25 files in the trash directory because the background thread may be slow and not be able to delete the first file fast enough, so sometimes this test fail.\n\nRemove\n```\nASSERT_EQ(CountFilesInDir(trash_dir_), 25);\n```\nSince it does not have any benefit any way\nCloses https://github.com/facebook/rocksdb/pull/2618\n\nDifferential Revision: D5458674\n\nPulled By: IslamAbdelRahman\n\nfbshipit-source-id: 5556a9edfa049db71dce80b8e6ae0fdd25e1e74e",
        "modified_files_count": 1,
        "modified_files": [
            "util/delete_scheduler_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3e5ea29a83270e8a0a93afe471022fc41adc8180",
        "contains_optimization_keyword": true
    },
    {
        "hash": "36651d14eeefaa3b17740e70bceacb3e096fde1a",
        "author": "Maysam Yabandeh",
        "date": "2017-07-18T16:58:22-07:00",
        "message": "Moving static AdaptationContext to outside function\n\nSummary:\nMoving static AdaptationContext to outside function to bypass tsan's false report with static initializers.\n\nIt is because with optimization enabled std::atomic is simplified to as a simple read with no locks. The existing lock produced by static initializer is __cxa_guard_acquire which is apparently not understood by tsan as it is different from normal locks (__gthrw_pthread_mutex_lock).\n\nThis is a known problem with tsan:\nhttps://stackoverflow.com/questions/27464190/gccs-tsan-reports-a-data-race-with-a-thread-safe-static-local\nhttps://stackoverflow.com/questions/42062557/c-multithreading-is-initialization-of-a-local-static-lambda-thread-safe\n\nA workaround that I tried was to move the static variable outside the function. It is not a good coding practice since it gives global visibility to variable but it is a hackish workaround until g++ tsan is improved.\nCloses https://github.com/facebook/rocksdb/pull/2598\n\nDifferential Revision: D5445281\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 6142bd934eb5852d8fd7ce027af593ba697ed41d",
        "modified_files_count": 1,
        "modified_files": [
            "db/write_thread.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/36651d14eeefaa3b17740e70bceacb3e096fde1a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "521724ba8279df153ceb96eeba30a76904687853",
        "author": "jsteemann",
        "date": "2017-06-23T09:41:19-07:00",
        "message": "fixed wrong type for \"allow_compaction\" parameter\n\nSummary:\nshould be boolean, not uint64_t\nMSVC complains about it during compilation with error `include\\rocksdb\\advanced_options.h(77): warning C4800: 'uint64_t': forcing value to bool 'true' or 'false' (performance warning)`\nCloses https://github.com/facebook/rocksdb/pull/2487\n\nDifferential Revision: D5310685\n\nPulled By: siying\n\nfbshipit-source-id: 719a33b3dba4f711aa72e3f229013c188015dc86",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/advanced_options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/521724ba8279df153ceb96eeba30a76904687853",
        "contains_optimization_keyword": true
    },
    {
        "hash": "be723a8d8c4cd58a5eff351e68b1418a7b938d77",
        "author": "Dmitri Smirnov",
        "date": "2017-03-22T11:24:12-07:00",
        "message": "Optionally construct Post Processing Info map in MemTableInserter\n\nSummary:\nMemTableInserter default constructs Post processing info\n  std::map. However, on Windows with 2015 STL the default\n  constructed map still dynamically allocates one node\n  which shows up on a profiler and we loose ~40% throughput\n  on fillrandom benchmark.\n  Solution: declare a map as std::aligned storage and optionally\n  construct.\n\nThis addresses https://github.com/facebook/rocksdb/issues/1976\n\nBefore:\n-------------------------------------------------------------------\n  Initializing RocksDB Options from command-line flags\n  DB path: [k:\\data\\BulkLoadRandom_10M_fillonly]\n  fillrandom   :       2.775 micros/op 360334 ops/sec;  280.4 MB/s\n  Microseconds per write:\n  Count: 10000000 Average: 2.7749  StdDev: 39.92\n  Min: 1  Median: 2.0826  Max: 26051\n  Percentiles: P50: 2.08 P75: 2.55 P99: 3.55 P99.9: 9.58 P99.99: 51.5**6\n  ------------------------------------------------------\n\n  After:\n\n  Initializing RocksDB Options from command-line flags\n  DB path: [k:\\data\\BulkLoadRandom_10M_fillon\nCloses https://github.com/facebook/rocksdb/pull/2011\n\nDifferential Revision: D4740823\n\nPulled By: siying\n\nfbshipit-source-id: 1daaa2c",
        "modified_files_count": 1,
        "modified_files": [
            "db/write_batch.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/be723a8d8c4cd58a5eff351e68b1418a7b938d77",
        "contains_optimization_keyword": true
    },
    {
        "hash": "90d835507581324d0449f1ded4f56a8b16f20bf7",
        "author": "xiusir",
        "date": "2017-02-28T10:39:11-08:00",
        "message": "Fix the wrong address for PREFETCH in DynamicBloom::Prefetch\n\nSummary:\n- Change data_[b] to data_[b / 8] in DynamicBloom::Prefetch, as b means the b-th bit in data_ and data_[b / 8] is the proper byte in data_.\nCloses https://github.com/facebook/rocksdb/pull/1935\n\nDifferential Revision: D4628696\n\nPulled By: siying\n\nfbshipit-source-id: bc5a0c6",
        "modified_files_count": 1,
        "modified_files": [
            "util/dynamic_bloom.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/90d835507581324d0449f1ded4f56a8b16f20bf7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "18eeb7b90e45af4bbac0777021711d8547f41eca",
        "author": "Mike Kolupaev",
        "date": "2017-02-21T16:09:10-08:00",
        "message": "Fix interference between max_total_wal_size and db_write_buffer_size checks\n\nSummary:\nThis is a trivial fix for OOMs we've seen a few days ago in logdevice.\n\nRocksDB get into the following state:\n(1) Write throughput is too high for flushes to keep up. Compactions are out of the picture - automatic compactions are disabled, and for manual compactions we don't care that much if they fall behind. We write to many CFs, with only a few L0 sst files in each, so compactions are not needed most of the time.\n(2) total_log_size_ is consistently greater than GetMaxTotalWalSize(). It doesn't get smaller since flushes are falling ever further behind.\n(3) Total size of memtables is way above db_write_buffer_size and keeps growing. But the write_buffer_manager_->ShouldFlush() is not checked because (2) prevents it (for no good reason, afaict; this is what this commit fixes).\n(4) Every call to WriteImpl() hits the MaybeFlushColumnFamilies() path. This keeps flushing the memtables one by one in order of increasing log file number.\n(5) No write stalling trigger is hit. We rely on max_write_buffer_number\nCloses https://github.com/facebook/rocksdb/pull/1893\n\nDifferential Revision: D4593590\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: af79c5f",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/18eeb7b90e45af4bbac0777021711d8547f41eca",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d438e1ec174bdf1474edcdf9902fe3cb14b8a1e2",
        "author": "Andrew Kryczka",
        "date": "2017-01-24T13:24:14-08:00",
        "message": "Test range deletion block outlives table reader\n\nSummary:\nThis test ensures RangeDelAggregator can still access blocks even if it outlives the table readers that created them (detailed description in comments).\n\nI plan to optimize away the extra cache lookup we currently do in BlockBasedTable::NewRangeTombstoneIterator(), as it is ~5% CPU in my random read benchmark in a database with 1k tombstones. This test will help make sure nothing breaks in the process.\nCloses https://github.com/facebook/rocksdb/pull/1739\n\nDifferential Revision: D4375954\n\nPulled By: ajkr\n\nfbshipit-source-id: aef9357",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_range_del_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d438e1ec174bdf1474edcdf9902fe3cb14b8a1e2",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9f246298e2f0af3973918a0dac0c5f46bc0993c0",
        "author": "Changli Gao",
        "date": "2017-01-11T10:54:37-08:00",
        "message": "Performance: Iterate vector by reference\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/1763\n\nDifferential Revision: D4398796\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: b82636d",
        "modified_files_count": 1,
        "modified_files": [
            "db/event_helpers.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9f246298e2f0af3973918a0dac0c5f46bc0993c0",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3d692822f824b3822d856bed86d5c33ea248d340",
        "author": "Siying Dong",
        "date": "2016-12-21T13:39:16-08:00",
        "message": "persistent_cache: fix two timer\n\nSummary:\nIn persistent_cache/block_cache_tier.cc, timers are never restarted, so the latency measured is not correct.\nCloses https://github.com/facebook/rocksdb/pull/1707\n\nDifferential Revision: D4355828\n\nPulled By: siying\n\nfbshipit-source-id: cd5f9e1",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/persistent_cache/block_cache_tier.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3d692822f824b3822d856bed86d5c33ea248d340",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7bd725e962d41ac4476c37e93b885da08bdc206a",
        "author": "Siying Dong",
        "date": "2016-12-19T12:39:11-08:00",
        "message": "db_bench: introduce --benchmark_read_rate_limit\n\nSummary:\nAdd the parameter in db_bench to help users to measure latency histogram with constant read rate.\nCloses https://github.com/facebook/rocksdb/pull/1683\n\nDifferential Revision: D4341387\n\nPulled By: siying\n\nfbshipit-source-id: 1b4b276",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7bd725e962d41ac4476c37e93b885da08bdc206a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2c2ba68247da3dd58f56f3349a28fe6052274192",
        "author": "Andrew Kryczka",
        "date": "2016-12-07T13:09:24-08:00",
        "message": "db_stress support for range deletions\n\nSummary:\nmade db_stress capable of adding range deletions to its db and verifying their correctness. i'll make db_crashtest.py use this option later once the collapsing optimization (https://github.com/facebook/rocksdb/pull/1614) is committed because currently it slows down the test too much.\nCloses https://github.com/facebook/rocksdb/pull/1625\n\nDifferential Revision: D4293939\n\nPulled By: ajkr\n\nfbshipit-source-id: d3beb3a",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2c2ba68247da3dd58f56f3349a28fe6052274192",
        "contains_optimization_keyword": true
    },
    {
        "hash": "5b219eccb545992a486b2711e4446f0a9e5911c2",
        "author": "Andrew Kryczka",
        "date": "2016-11-29T12:24:13-08:00",
        "message": "deleterange end-to-end test improvements for lite/robustness\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/1591\n\nDifferential Revision: D4246019\n\nPulled By: ajkr\n\nfbshipit-source-id: 0c4aa37",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_range_del_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/5b219eccb545992a486b2711e4446f0a9e5911c2",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a2bf265a393721a30ddae9b04fde4aece6515c97",
        "author": "Islam AbdelRahman",
        "date": "2016-11-28T18:39:13-08:00",
        "message": "Avoid intentional overflow in GetL0ThresholdSpeedupCompaction\n\nSummary:\nhttps://github.com/facebook/rocksdb/commit/99c052a34f93d119b75eccdcd489ecd581d48ee9 fixes integer overflow in GetL0ThresholdSpeedupCompaction() by checking if int become -ve.\nUBSAN will complain about that since this is still an overflow, we can fix the issue by simply using int64_t\nCloses https://github.com/facebook/rocksdb/pull/1582\n\nDifferential Revision: D4241525\n\nPulled By: IslamAbdelRahman\n\nfbshipit-source-id: b3ae21f",
        "modified_files_count": 1,
        "modified_files": [
            "db/column_family.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a2bf265a393721a30ddae9b04fde4aece6515c97",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4444256ab79876bf33500ad4907c45b7b4d57137",
        "author": "Nick Terrell",
        "date": "2016-11-21T12:24:14-08:00",
        "message": "Remove use of deprecated LZ4 function\n\nSummary:\nLZ4 1.7.3 emits warnings when calling the deprecated function `LZ4_compress_limitedOutput_continue()`.  Starting in r129, LZ4 introduces `LZ4_compress_fast_continue()` as a replacement, and the two functions calls are [exactly equivalent](https://github.com/lz4/lz4/blob/dev/lib/lz4.c#L1408).\nCloses https://github.com/facebook/rocksdb/pull/1532\n\nDifferential Revision: D4199240\n\nPulled By: siying\n\nfbshipit-source-id: 138c2bc",
        "modified_files_count": 1,
        "modified_files": [
            "util/compression.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4444256ab79876bf33500ad4907c45b7b4d57137",
        "contains_optimization_keyword": true
    },
    {
        "hash": "da61f348d37e45a0cfd70893499d9f6625111b4e",
        "author": "Siying Dong",
        "date": "2016-10-31T16:09:13-07:00",
        "message": "Print compression and Fast CRC support info as Header level\n\nSummary:\nCurrently the compression suppport and fast CRC support information is printed as info level. They should be in the same level as options, which is header level.\n\nAlso add ZSTD to this printing.\nCloses https://github.com/facebook/rocksdb/pull/1448\n\nDifferential Revision: D4106608\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: cb9a076",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/da61f348d37e45a0cfd70893499d9f6625111b4e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2946cadc46e444601e30b3ced0d59d9c938df84b",
        "author": "Andrew Kryczka",
        "date": "2016-10-28T15:54:21-07:00",
        "message": "Improve RangeDelAggregator documentation\n\nSummary:\nas requested in D62259\nCloses https://github.com/facebook/rocksdb/pull/1434\n\nDifferential Revision: D4099047\n\nPulled By: ajkr\n\nfbshipit-source-id: a258cfb",
        "modified_files_count": 1,
        "modified_files": [
            "db/range_del_aggregator.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2946cadc46e444601e30b3ced0d59d9c938df84b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "48e4e842b79a0fe26ee9044830110f9a3484c20a",
        "author": "yiwu-arbug",
        "date": "2016-10-19T18:18:42-07:00",
        "message": "Disable auto compactions in memory_test and re-enable the test (#1408)\n\nSummary: Auto-compactions will change memory usage of DB but memory_test\r\ndidn't take it into account. This PR disable auto compactions in the\r\ntest and hopefully it fixes its flakyness.\r\n\r\nTest Plan:\r\nUBSAN build used to catch the flakyness. Run `make ubsan_check` and it\r\npasses.",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/memory/memory_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/48e4e842b79a0fe26ee9044830110f9a3484c20a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "cce702a6e44be094b7c63ca4ed9eb018ed10d58a",
        "author": "Islam AbdelRahman",
        "date": "2016-08-25T12:57:35-07:00",
        "message": "[db_bench] Support single benchmark arguments (Repeat for X times, Warm up for X times), Support CombinedStats (AVG / MEDIAN)\n\nSummary:\nThis diff allow us to run a single benchmark X times and warm it up for Y times. and see the AVG & MEDIAN throughput of these X runs\nfor example\n\n```\n$ ./db_bench --benchmarks=\"fillseq,readseq[X5-W2]\"\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nRocksDB:    version 4.12\nDate:       Wed Aug 24 10:45:26 2016\nCPU:        32 * Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz\nCPUCache:   20480 KB\nKeys:       16 bytes each\nValues:     100 bytes each (50 bytes after compression)\nEntries:    1000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    110.6 MB (estimated)\nFileSize:   62.9 MB (estimated)\nWrite rate: 0 bytes/second\nCompression: Snappy\nMemtablerep: skip_list\nPerf Level: 1\nWARNING: Assertions are enabled; benchmarks unnecessarily slow\n------------------------------------------------\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nDB path: [/tmp/rocksdbtest-8616/dbbench]\nfillseq      :       4.695 micros/op 212971 ops/sec;   23.6 MB/s\nDB path: [/tmp/rocksdbtest-8616/dbbench]\nWarming up benchmark by running 2 times\nreadseq      :       0.214 micros/op 4677005 ops/sec;  517.4 MB/s\nreadseq      :       0.212 micros/op 4706834 ops/sec;  520.7 MB/s\nRunning benchmark for 5 times\nreadseq      :       0.218 micros/op 4588187 ops/sec;  507.6 MB/s\nreadseq      :       0.208 micros/op 4816538 ops/sec;  532.8 MB/s\nreadseq      :       0.213 micros/op 4685376 ops/sec;  518.3 MB/s\nreadseq      :       0.214 micros/op 4676787 ops/sec;  517.4 MB/s\nreadseq      :       0.217 micros/op 4618532 ops/sec;  510.9 MB/s\nreadseq [AVG    5 runs] : 4677084 ops/sec;  517.4 MB/sec\nreadseq [MEDIAN 5 runs] : 4676787 ops/sec;  517.4 MB/sec\n```\n\nTest Plan: run db_bench\n\nReviewers: sdong, andrewkr, yhchiang\n\nReviewed By: yhchiang\n\nSubscribers: andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D62235",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/cce702a6e44be094b7c63ca4ed9eb018ed10d58a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d1be59463696d64dcc20fee5129ca7bc23d54904",
        "author": "Dhruba Borthakur",
        "date": "2016-08-11T15:10:09-07:00",
        "message": "Improve documentation of SliceTransform.\n\nTest Plan: make check\n\nSubscribers: leveldb, andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D61875",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/slice_transform.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d1be59463696d64dcc20fee5129ca7bc23d54904",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b06ca5f8602a0aa4fc87fac84362fb7de918cc39",
        "author": "Peter (Stig) Edwards",
        "date": "2016-07-22T11:46:40-07:00",
        "message": "ldb load, prefer ifsteam(/dev/stdin) to std::cin (#1207)\n\ngetline on std::cin can be very inefficient when ldb is loading large values, with high CPU usage in libc _IO_(un)getc, this is because of the performance penalty that comes from synchronizing stdio and iostream buffers.\r\nSee the reproducers and tests in #1133 .\r\nIf an ifstream on /dev/stdin is used (when available) then using ldb to load large values can be much more efficient.\r\nI thought for ldb load, that this approach is preferable to using <cstdio> or std::ios_base::sync_with_stdio(false).\r\nI couldn't think of a use case where ldb load would need to support reading unbuffered input, an alternative approach would be to add support for passing --input_file=/dev/stdin.\r\nI have a CLA in place, thanks.\r\n\r\nThe CI tests were failing at the time of https://github.com/facebook/rocksdb/pull/1156, so this change and PR will supersede it.",
        "modified_files_count": 1,
        "modified_files": [
            "tools/ldb_cmd.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b06ca5f8602a0aa4fc87fac84362fb7de918cc39",
        "contains_optimization_keyword": true
    },
    {
        "hash": "21c55bdb6e97d3d86efab3770264be0d188c0a07",
        "author": "sdong",
        "date": "2016-07-15T16:10:09-07:00",
        "message": "DBTest.DynamicLevelCompressionPerLevel: Tune Threshold\n\nSummary: Each SST's file size increases after we add more table properties. Threshold in DBTest.DynamicLevelCompressionPerLevel need to adjust accordingly to avoid occasional failures.\n\nTest Plan: Run the test\n\nReviewers: andrewkr, yiwu\n\nSubscribers: leveldb, andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D60819",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/21c55bdb6e97d3d86efab3770264be0d188c0a07",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f5177c761ffb256ecba3ddb9c1af3bc52a105cca",
        "author": "Islam AbdelRahman",
        "date": "2016-06-13T16:22:14-07:00",
        "message": "Remove wasteful instrumentation in FullMerge (stacked on D59577)\n\nSummary:\n[ This diff is stacked on top of D59577 ]\n\nWe keep calling timer.ElapsedNanos() on every call to MergeOperator::FullMerge even when statistics are disabled, this is wasteful.\n\nI run the readseq benchmark on a DB containing 100K merge operands for 100K keys (1 operand per key) with 1GB block cache\nI see slight performance improvment\n\nOriginal results\n\n```\n$ ./db_bench --benchmarks=\"readseq,readseq,readseq,readseq,readseq,readseq,readseq,readseq,readseq\" --merge_operator=\"max\" --merge_keys=100000 --num=100000 --db=\"/dev/shm/100K_merge_compacted/\" --cache_size=1073741824 --use_existing_db --disable_auto_compactions\n------------------------------------------------\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.498 micros/op 2006597 ops/sec;  222.0 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.295 micros/op 3393627 ops/sec;  375.4 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.285 micros/op 3511155 ops/sec;  388.4 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.286 micros/op 3500470 ops/sec;  387.2 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.283 micros/op 3530751 ops/sec;  390.6 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.289 micros/op 3464811 ops/sec;  383.3 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.277 micros/op 3612814 ops/sec;  399.7 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.283 micros/op 3539640 ops/sec;  391.6 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.285 micros/op 3503766 ops/sec;  387.6 MB/s\n```\n\nAfter patch\n\n```\n$ ./db_bench --benchmarks=\"readseq,readseq,readseq,readseq,readseq,readseq,readseq,readseq,readseq\" --merge_operator=\"max\" --merge_keys=100000 --num=100000 --db=\"/dev/shm/100K_merge_compacted/\" --cache_size=1073741824 --use_existing_db --disable_auto_compactions\n------------------------------------------------\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.476 micros/op 2100119 ops/sec;  232.3 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.278 micros/op 3600887 ops/sec;  398.4 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.275 micros/op 3636698 ops/sec;  402.3 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.271 micros/op 3691661 ops/sec;  408.4 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.273 micros/op 3661534 ops/sec;  405.1 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.276 micros/op 3627106 ops/sec;  401.3 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.272 micros/op 3682635 ops/sec;  407.4 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.266 micros/op 3758331 ops/sec;  415.8 MB/s\nDB path: [/dev/shm/100K_merge_compacted/]\nreadseq      :       0.266 micros/op 3761907 ops/sec;  416.2 MB/s\n```\n\nTest Plan: make check -j64\n\nReviewers: yhchiang, sdong\n\nReviewed By: sdong\n\nSubscribers: andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D59583",
        "modified_files_count": 1,
        "modified_files": [
            "db/merge_helper.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f5177c761ffb256ecba3ddb9c1af3bc52a105cca",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9790b94c924453cea4d230a0f40edf02015f71e8",
        "author": "Mark Callaghan",
        "date": "2016-05-05T07:32:10-07:00",
        "message": "Add optimize_filters_for_hits option to db_bench\n\nSummary:\nAdd optimize_filters_for_hits option to db_bench\n\nTask ID: #\n\nBlame Rev:\n\nTest Plan:\nrun db_bench\n\nRevert Plan:\n\nDatabase Impact:\n\nMemcache Impact:\n\nOther Notes:\n\nEImportant:\n\n- begin *PUBLIC* platform impact section -\nBugzilla: #\n- end platform impact -\n\nReviewers: sdong\n\nReviewed By: sdong\n\nSubscribers: andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D57621",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_bench_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9790b94c924453cea4d230a0f40edf02015f71e8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "052299035803827efa6f1c2cb28d3dc3c6e8b1f7",
        "author": "Islam AbdelRahman",
        "date": "2016-04-08T12:05:02-07:00",
        "message": "Improve sst_dump help message\n\nSummary:\nCurrent Message\n\n```\nsst_dump [--command=check|scan|none|raw] [--verify_checksum] --file=data_dir_OR_sst_file [--output_hex] [--input_key_hex] [--from=<user_key>] [--to=<user_key>] [--read_num=NUM] [--show_properties] [--show_compression_sizes] [--show_compression_sizes [--set_block_size=<block_size>]]\n```\nNew message\n\n```\nsst_dump --file=<data_dir_OR_sst_file> [--command=check|scan|raw]\n    --file=<data_dir_OR_sst_file>\n      Path to SST file or directory containing SST files\n\n    --command=check|scan|raw\n        check: Iterate over entries in files but dont print anything except if an error is encounterd (default command)\n        scan: Iterate over entries in files and print them to screen\n        raw: Dump all the table contents to <file_name>_dump.txt\n\n    --output_hex\n      Can be combined with scan command to print the keys and values in Hex\n\n    --from=<user_key>\n      Key to start reading from when executing check|scan\n\n    --to=<user_key>\n      Key to stop reading at when executing check|scan\n\n    --read_num=<num>\n      Maximum number of entries to read when executing check|scan\n\n    --verify_checksum\n      Verify file checksum when executing check|scan\n\n    --input_key_hex\n      Can be combined with --from and --to to indicate that these values are encoded in Hex\n\n    --show_properties\n      Print table properties after iterating over the file\n\n    --show_compression_sizes\n      Independent command that will recreate the SST file using 16K block size with different\n      compressions and report the size of the file using such compression\n\n    --set_block_size=<block_size>\n      Can be combined with --show_compression_sizes to set the block size that will be used\n      when trying different compression algorithms\n```\n\nTest Plan: none\n\nReviewers: yhchiang, andrewkr, kradhakrishnan, yiwu, sdong\n\nReviewed By: sdong\n\nSubscribers: andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D56325",
        "modified_files_count": 1,
        "modified_files": [
            "tools/sst_dump_tool.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/052299035803827efa6f1c2cb28d3dc3c6e8b1f7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e7c64fb115d255a03cf17f8d54b3f088f29464ca",
        "author": "Islam AbdelRahman",
        "date": "2016-03-29T13:20:26-07:00",
        "message": "Imporve sst_file_manager comment\n\nSummary: Improve the comment for sst_file_manager\n\nTest Plan: none\n\nReviewers: yhchiang, sdong\n\nReviewed By: sdong\n\nSubscribers: andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D56001",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e7c64fb115d255a03cf17f8d54b3f088f29464ca",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9cad56861a7b4ad87998239d43a738f35870e926",
        "author": "Siying Dong",
        "date": "2016-03-16T10:30:01-07:00",
        "message": "Merge pull request #1039 from bureau14/master\n\nImprove documentation of the allow_os_buffer parameter.",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9cad56861a7b4ad87998239d43a738f35870e926",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3d29f914665ade3c5e657006e2506ec27d71327d",
        "author": "Edouard Alligand",
        "date": "2016-03-16T15:37:55+01:00",
        "message": "Improve documentation of the allow_os_buffer parameter.",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3d29f914665ade3c5e657006e2506ec27d71327d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1a2cc27e013b561c9d3c8b81384d14443822057f",
        "author": "Dhruba Borthakur",
        "date": "2016-03-14T16:21:54-07:00",
        "message": "ColumnFamilyOptions SanitizeOptions is buggy on 32-bit platforms.\n\nSummary:\nThe pre-existing code is trying to clamp between 65,536 and 0,\nresulting in clamping to 65,536, resulting in very small buffers,\nresulting in ShouldFlushNow() being true quite easily,\nresulting in assertion failing and database performance\nbeing \"not what it should be\".\n\nhttps://github.com/facebook/rocksdb/issues/1018\n\nTest Plan: make check\n\nReviewers: sdong, andrewkr, IslamAbdelRahman, yhchiang, igor\n\nReviewed By: igor\n\nSubscribers: leveldb, andrewkr, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D55455",
        "modified_files_count": 1,
        "modified_files": [
            "db/column_family.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1a2cc27e013b561c9d3c8b81384d14443822057f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "730a422c3a41df569e0c51cc7b7e0d8938d33ee8",
        "author": "Yueh-Hsuan Chiang",
        "date": "2016-02-16T14:55:24-08:00",
        "message": "Improve the documentation of LoadLatestOptions\n\nSummary: Improve the documentation of LoadLatestOptions\n\nTest Plan: No code change\n\nReviewers: anthony, IslamAbdelRahman, kradhakrishnan, sdong\n\nReviewed By: sdong\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D54087",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/utilities/options_util.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/730a422c3a41df569e0c51cc7b7e0d8938d33ee8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a7b6f0748a1bcdead0168df19901cfc15c9dc881",
        "author": "reid horuff",
        "date": "2016-02-16T14:04:14-08:00",
        "message": "Improve write_with_callback_test to sync WAL\n\nSummary: Currently write_with_callback_test does not test with WAL syncing enabled. This addresses that.\n\nTest Plan: write_with_callback_test\n\nReviewers: anthony\n\nReviewed By: anthony\n\nSubscribers: leveldb, dhruba, hermanlee4\n\nDifferential Revision: https://reviews.facebook.net/D54255",
        "modified_files_count": 1,
        "modified_files": [
            "db/write_callback_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a7b6f0748a1bcdead0168df19901cfc15c9dc881",
        "contains_optimization_keyword": true
    },
    {
        "hash": "8019aa9b55c90ae2b89889d6260674f4c8fd19ff",
        "author": "agiardullo",
        "date": "2016-01-22T11:47:59-08:00",
        "message": "improve test for manifest write failure\n\nSummary: Improve testing per discussion in D52989\n\nTest Plan: ran test\n\nReviewers: sdong\n\nReviewed By: sdong\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D53211",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/8019aa9b55c90ae2b89889d6260674f4c8fd19ff",
        "contains_optimization_keyword": true
    },
    {
        "hash": "8c71eb5afce8917db9ad78bb4fac136427891b99",
        "author": "Islam AbdelRahman",
        "date": "2016-01-07T07:59:14-08:00",
        "message": "Optimize DBIter::Prev() by reducing stack overhead\n\nSummary:\nIt looks like we are spending significant amount of time creating std::deque<std::string> every time we do Iterator::Prev()\n\n{F921567}\n\nBy using merge_operands_ as a DBIter data member w create it once and reduce this overhead and see ~30% performance improvement when using Iterator::Prev() on hot data\n\nOrignal performance\n\n```\nDEBUG_LEVEL=0 make db_bench -j64 && ./db_bench --benchmarks=\"readreverse\" --db=\"/dev/shm/bench_prev_opt/\" --use_existing_db --disable_auto_compactions\nreadreverse  :       0.713 micros/op 1402219 ops/sec;  155.1 MB/s\nreadreverse  :       0.609 micros/op 1641386 ops/sec;  181.6 MB/s\nreadreverse  :       0.684 micros/op 1461150 ops/sec;  161.6 MB/s\nreadreverse  :       0.629 micros/op 1589842 ops/sec;  175.9 MB/s\nreadreverse  :       0.647 micros/op 1544530 ops/sec;  170.9 MB/s\n```\n\nAfter optimization\n\n```\nDEBUG_LEVEL=0 make db_bench -j64 && ./db_bench --benchmarks=\"readreverse\" --db=\"/dev/shm/bench_prev_opt/\" --use_existing_db --disable_auto_compactions\nreadreverse  :       0.488 micros/op 2051189 ops/sec;  226.9 MB/s\nreadreverse  :       0.505 micros/op 1980892 ops/sec;  219.1 MB/s\nreadreverse  :       0.541 micros/op 1846971 ops/sec;  204.3 MB/s\nreadreverse  :       0.497 micros/op 2013612 ops/sec;  222.8 MB/s\nreadreverse  :       0.480 micros/op 2082665 ops/sec;  230.4 MB/s\n```\n\nTest Plan: make check -j64\n\nReviewers: sdong, anthony, rven, igor, yhchiang\n\nReviewed By: yhchiang\n\nSubscribers: jkedgar, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D52563",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_iter.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/8c71eb5afce8917db9ad78bb4fac136427891b99",
        "contains_optimization_keyword": true
    },
    {
        "hash": "291088ae4e583ccdf4de0881a8762bb5412f7d27",
        "author": "sdong",
        "date": "2015-12-07T10:53:29-08:00",
        "message": "Fix undeterministic failure of ColumnFamilyTest.DifferentWriteBufferSizes\n\nSummary: After the skip list optimization, ColumnFamilyTest.DifferentWriteBufferSizes can occasionally fail with flush triggering of column family 3. Insert more data to it to make sure flush will trigger.\n\nTest Plan: Run it multiple times with both of jemaloc on and off and see it always passes. (Without thd commit the run with jemalloc fails with chance of about one in two)\n\nReviewers: rven, yhchiang, IslamAbdelRahman, anthony, kradhakrishnan, igor\n\nReviewed By: igor\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D51645",
        "modified_files_count": 1,
        "modified_files": [
            "db/column_family_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/291088ae4e583ccdf4de0881a8762bb5412f7d27",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e61d9c1484c32bd0028604ef65a724f7f55f2447",
        "author": "sdong",
        "date": "2015-10-09T09:47:56-07:00",
        "message": "Make DBTest.AggregatedTableProperties more deterministic\n\nSummary: Now based on environment, DBTest.AggregatedTableProperties has a possibility of issuing a L0->L1 compaction after reopening and the results are not what we expected. We tune the L0 compaction trigger to make it less likely to happen.\n\nTest Plan: I can't repro the failure but I think the change is better. Just run the test and make sure it passes.\n\nReviewers: kradhakrishnan, yhchiang, igor\n\nReviewed By: igor\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D48423",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e61d9c1484c32bd0028604ef65a724f7f55f2447",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4704833357a8609e7c42df4f337f938a8e870c08",
        "author": "jsteemann",
        "date": "2015-09-18T20:20:32+02:00",
        "message": "pass input string to WriteBatch() by const reference\n\nthis may lead to copying less data (in case compilers don't\noptimize away copying the string by themselves)",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/write_batch.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4704833357a8609e7c42df4f337f938a8e870c08",
        "contains_optimization_keyword": true
    },
    {
        "hash": "df22e2fb71be07d25448a01e1eef77f65506b429",
        "author": "Andres Noetzli",
        "date": "2015-09-15T10:52:00-07:00",
        "message": "Relax memory order for faster tickers\n\nSummary:\nThe default behavior for atomic operations is sequentially consistent ordering\nwhich is not needed for simple counters (see:\nhttp://en.cppreference.com/w/cpp/atomic/memory_order). Change the memory order\nto std::memory_order_relaxed for better performance.\n\nTest Plan: make clean all check\n\nReviewers: rven, anthony, yhchiang, aekmekji, sdong, MarkCallaghan, igor\n\nReviewed By: igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D46953",
        "modified_files_count": 1,
        "modified_files": [
            "util/statistics.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/df22e2fb71be07d25448a01e1eef77f65506b429",
        "contains_optimization_keyword": true
    },
    {
        "hash": "abc7f5fdb266745493d89b09719d9dd091e2b3b8",
        "author": "sdong",
        "date": "2015-09-10T11:32:19-07:00",
        "message": "Make DBTest.ReadLatencyHistogramByLevel more robust\n\nSummary: DBTest.ReadLatencyHistogramByLevel was not written as expected. After writes, reads aren't guaranteed to hit data written. It was not expected. Fix it.\n\nTest Plan: Run the test multiple times\n\nReviewers: IslamAbdelRahman, rven, anthony, kradhakrishnan, yhchiang, igor\n\nReviewed By: igor\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D46587",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/abc7f5fdb266745493d89b09719d9dd091e2b3b8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "342ba8089528ac1ed4636f2c320b4508151b1f0b",
        "author": "sdong",
        "date": "2015-09-08T19:31:34-07:00",
        "message": "Make DBTest.OptimizeFiltersForHits more deterministic\n\nSummary:\nThis commit makes DBTest.OptimizeFiltersForHits more deterministic by:\n(1) make key inserts more random\n(2) make sure L0 has one file\n(3) make file size smaller compared to level target so L1 will cover more range.\n\nTest Plan: Run the test many times.\n\nReviewers: rven, IslamAbdelRahman, kradhakrishnan, igor, anthony\n\nReviewed By: anthony\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D46461",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/342ba8089528ac1ed4636f2c320b4508151b1f0b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "18db1e46959f4472a1b4564471de3ffe91491505",
        "author": "agiardullo",
        "date": "2015-08-31T15:56:07-07:00",
        "message": "better db_bench options for transactions\n\nSummary:\nPessimistic Transaction expiration time checking currently causes a performace regression,  Lets disable it in db_bench by default.\n\nAlso, in order to be able to better tune how much contention we're simulating, added new optinos to set lock timeout and snapshot.\n\nTest Plan: run db_bench randomtranansaction\n\nReviewers: sdong, igor, yhchiang, MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D45831",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/18db1e46959f4472a1b4564471de3ffe91491505",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e853191c17eb85f527d0e495a612fd0679b241ea",
        "author": "Andres Noetzli",
        "date": "2015-08-27T16:17:08-07:00",
        "message": "Fix DBTest.ApproximateMemoryUsage\n\nSummary:\nThis patch fixes two issues in DBTest.ApproximateMemoryUsage:\n- It was possible that a flush happened between getting the two properties in\n  Phase 1, resulting in different numbers for the properties and failing the\n  assertion. This is fixed by waiting for the flush to finish before getting\n  the properties.\n- There was a similar issue in Phase 2 and additionally there was an issue that\n  rocksdb.size-all-mem-tables was not monotonically increasing because it was\n  possible that a flush happened just after getting the properties and then\n  another flush just before getting the properties in the next round. In this\n  situation, the reported memory usage decreased. This is fixed by forcing a\n  flush before getting the properties.\n\nNote: during testing, I found that kFlushesPerRound does not seem very\naccurate. I added a TODO for this and it would be great to get some input on\nwhat to do there.\n\nTest Plan:\nThe first issue can be made more likely to trigger by inserting a\n`usleep(10000);` between the calls to GetIntProperty() in Phase 1.\nThe second issue can be made more likely to trigger by inserting a\n`if (r != 0) usleep(10000);` before the calls to GetIntProperty() and a\n`usleep(10000);` after the calls.\nThen execute make db_test && ./db_test --gtest_filter=DBTest.ApproximateMemoryUsage\n\nReviewers: rven, yhchiang, igor, sdong, anthony\n\nReviewed By: anthony\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D45675",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e853191c17eb85f527d0e495a612fd0679b241ea",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1ae27113c7d4f760fa846fedf3761c3b76481e5f",
        "author": "Nathan Bronson",
        "date": "2015-08-11T11:25:22-07:00",
        "message": "reduce comparisons by skiplist\n\nSummary:\nKey comparison is the single largest CPU user for CPU-bound\nworkloads. This diff reduces the number of comparisons in two ways.\n\nThe first is that it moves predecessor array gathering from\nFindGreaterOrEqual to FindLessThan, so that FindGreaterOrEqual can\nreturn immediately if compare_ returns 0.  As part of this change I\nmoved the sequential insertion optimization into Insert, to remove the\nundocumented (and smelly) requirement that prev must be equal to prev_\nif it is non-null.\n\nThe second optimization is that all of the search functions skip calling\ncompare_ when moving to a lower level that has the same Next pointer.\nWith a branching factor of 4 we would expect this to happen 1/4 of\nthe time.\n\nOn a single-threaded CPU-bound workload (-benchmarks=fillrandom -threads=1\n-batch_size=1 -memtablerep=skip_list -value_size=0 --num=1600000\n-level0_slowdown_writes_trigger=9999 -level0_stop_writes_trigger=9999\n-disable_auto_compactions --max_write_buffer_number=8\n-max_background_flushes=8 --disable_wal --write_buffer_size=160000000)\non my dev server this is good for a 7% perf win.\n\nTest Plan: unit tests\n\nReviewers: rven, ljin, yhchiang, sdong, igor\n\nReviewed By: igor\n\nSubscribers: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D43233",
        "modified_files_count": 1,
        "modified_files": [
            "db/skiplist.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1ae27113c7d4f760fa846fedf3761c3b76481e5f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "48da7a9cad76f1a736e54a495525c7721f4a9967",
        "author": "Yueh-Hsuan Chiang",
        "date": "2015-06-24T15:00:51-07:00",
        "message": "Improve the comment for BYTES_READ in statistics.\n\nSummary:\nBYTES_READ only count the number of logical bytes read from\nthe DB::Get() function.  It neither includes all logical bytes read\nnor indicates IO read bytes.\n\nThis patch improves the comment for BYTES_READ.\n\nTest Plan: Only change comment.\n\nReviewers: sdong, rven, anthony, kradhakrishnan, IslamAbdelRahman, igor\n\nReviewed By: igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D40599",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/statistics.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/48da7a9cad76f1a736e54a495525c7721f4a9967",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2dc3910b5e56afdaeea7133d6fe17a89e3f7e75a",
        "author": "Igor Canadi",
        "date": "2015-06-17T16:44:52-07:00",
        "message": "Add --benchmark_write_rate_limit option to db_bench\n\nSummary:\nSo far, we benchmarked RocksDB by writing as fast as possible. With this change, we're able to limit our write throughput, which should help us better understand how RocksDB performes under varying write workloads.\n\nSpecifically, I'm currently interested in the shape of the graph that has write throughput on one axis and write rate on another. This should help us with designing our stall system, as we have started to do with D36351.\n\nTest Plan:\n    $ ./db_bench --benchmarks=fillrandom --benchmark_write_rate_limit=1000000\n    fillrandom   :     118.523 micros/op 8437 ops/sec;    0.9 MB/s\n    $ ./db_bench --benchmarks=fillrandom --benchmark_write_rate_limit=2000000\n    fillrandom   :      59.136 micros/op 16910 ops/sec;    1.9 MB/s\n\nReviewers: MarkCallaghan, sdong\n\nReviewed By: sdong\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D39759",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2dc3910b5e56afdaeea7133d6fe17a89e3f7e75a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3ab8ffd4dd406d25a4164ea0d7e1aa5b30bca5a8",
        "author": "Yueh-Hsuan Chiang",
        "date": "2015-05-26T14:05:38-07:00",
        "message": "Compaction now conditionally boosts the size of deletion entries.\n\nSummary:\nCompaction now boosts the size of deletion entries of a file only when\nthe number of deletion entries is greater than the number of non-deletion\nentries in the file.  The motivation here is that in a stable workload,\nthe number of deletion entries should be roughly equal to the number of\nnon-deletion entries.  If we compensate the size of deletion entries in a\nstable workload, the deletion compensation logic might introduce unwanted\neffet which changes the shape of LSM tree.\n\nTest Plan: db_test --gtest_filter=\"*Deletion*\"\n\nReviewers: sdong, igor\n\nReviewed By: igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D38703",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3ab8ffd4dd406d25a4164ea0d7e1aa5b30bca5a8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fb5bdbf9875bb2c9b583b82eb6d881ac2c39ddc7",
        "author": "sdong",
        "date": "2015-05-18T11:49:45-07:00",
        "message": "DBTest.DynamicLevelMaxBytesCompactRange: make sure L0 is not empty before running compact range\n\nSummary: DBTest.DynamicLevelMaxBytesCompactRange needs to make sure L0 is not empty to properly cover the code paths we want to cover. However, current codes have a bug that might leave the condition not held. Improve the test to ensure it.\n\nTest Plan: Run the test in an environment that is used to fail. Also run it many times.\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D38631",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fb5bdbf9875bb2c9b583b82eb6d881ac2c39ddc7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "79c21ec0c4af76e69a728a1e27aa3de558d27c8f",
        "author": "Jim Meyering",
        "date": "2015-04-17T20:39:02-07:00",
        "message": "skip ioctl-using tests when not supported\n\nSummary:\n[NB: this is a prerequisite for the /tmp-abuse-fixing patch]\nThis avoids spurious test failure on Linux systems\nlike Fedora for which /tmp is a tmpfs file system.\n\nOn a devtmpfs file\nsystem, ioctl(fd, FS_IOC_GETVERSION, &version) returns -1 with\nerrno == ENOTTTY, indicating that that ioctl is not supported\non such a file system.  Do not let this cause test failures, e.g.,\nwhere env_test would assert that file->GetUniqueId(...) > 0.\n\nBefore this change, ./env_test would fail these three tests\non a fedora rawhide system:\n\n  [  FAILED  ] 3 tests, listed below:\n  [  FAILED  ] EnvPosixTest.RandomAccessUniqueID\n  [  FAILED  ] EnvPosixTest.RandomAccessUniqueIDConcurrent\n  [  FAILED  ] EnvPosixTest.RandomAccessUniqueIDDeletes\n   3 FAILED TESTS\n\nThe fix:\n  When support for that ioctl is lacking, skip each affected test.\n  Could be improved by noting which sub-tests are being skipped.\n\nTest Plan:\nrun these on F21 and note that they now pass.\n\n  TEST_TMPDIR=/dev/shm/rdb ./env_test\n  ./env_test\n\nReviewers: ljin, rven, igor.sugak, yhchiang, sdong, igor\n\nReviewed By: igor\n\nSubscribers: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D37323",
        "modified_files_count": 1,
        "modified_files": [
            "util/env_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/79c21ec0c4af76e69a728a1e27aa3de558d27c8f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "48b0a045da0ef7c07b59c529fc9a5c5f682853b6",
        "author": "Igor Canadi",
        "date": "2015-04-16T19:31:34-07:00",
        "message": "Speed up reduce_levels_test\n\nSummary: For some reason reduce_levels is opening the databse with 65.000 levels. This makes ComputeCompactionScore() function terribly slow and the tests is also very slow (20seconds).\n\nTest Plan: mr reduce_levels_test now takes 20ms\n\nReviewers: sdong, rven, kradhakrishnan, yhchiang\n\nReviewed By: yhchiang\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D37059",
        "modified_files_count": 1,
        "modified_files": [
            "util/ldb_cmd.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/48b0a045da0ef7c07b59c529fc9a5c5f682853b6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "281db8bb62d0f42c89abd2ed0812961843e32ad5",
        "author": "Igor Canadi",
        "date": "2015-04-13T19:30:40-07:00",
        "message": "Temporarily disable test CompactFilesOnLevelCompaction\n\nSummary: https://reviews.facebook.net/D36963 made the debug build much faster and that triggered failures of CompactFilesOnLevelCompaction test. 3 out of 4 last tests on Jenkins failed. I'm disabling this test temporarily, since we likely know the reason why it's failing and there's already work in progress to address it -- https://reviews.facebook.net/D36225\n\nTest Plan: none\n\nReviewers: sdong, rven, yhchiang, meyering\n\nReviewed By: meyering\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D36993",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/281db8bb62d0f42c89abd2ed0812961843e32ad5",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d41a565a4a48ac7818bca8bf0a311bc25462c3c2",
        "author": "Igor Canadi",
        "date": "2015-04-13T16:11:47-07:00",
        "message": "Don't do O(N^2) operations in debug mode for vector memtable\n\nSummary: As title. For every operation we're asserting Valid(), which sorts the data. That's pretty terrible. We have to be careful to have decent performance even with DEBUG builds.\n\nTest Plan: make check\n\nReviewers: sdong, rven, yhchiang, MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D36969",
        "modified_files_count": 1,
        "modified_files": [
            "util/vectorrep.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d41a565a4a48ac7818bca8bf0a311bc25462c3c2",
        "contains_optimization_keyword": true
    },
    {
        "hash": "08be1803eecb5ae464440812ea06e79b21289053",
        "author": "Igor Canadi",
        "date": "2015-04-13T15:58:45-07:00",
        "message": "Fix bad performance in debug mode\n\nSummary:\nSee github issue 574: https://github.com/facebook/rocksdb/issues/574\n\nBasically when we're running in DEBUG mode we're calling `usleep(0)` on\nevery mutex lock. I bisected the issue to\nhttps://reviews.facebook.net/D36963. Instead of calling sleep(0), this\ndiff just avoids calling SleepForMicroseconds() when delay is not set.\n\nTest Plan:\n    bpl=10485760;overlap=10;mcz=2;del=300000000;levels=2;ctrig=10000000; delay=10000000; stop=10000000; wbn=30; mbc=20; mb=1073741824;wbs=268435456; dds=1; sync=0; r=100000; t=1; vs=800; bs=65536; cs=1048576; of=500000; si=1000000; ./db_bench --benchmarks=fillrandom --disable_seek_compaction=1 --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t --value_size=$vs --block_size=$bs --cache_size=$cs --bloom_bits=10 --cache_numshardbits=4 --open_files=$of --verify_checksum=1 --db=/tmp/rdb10test --sync=$sync --disable_wal=1 --compression_type=snappy --stats_interval=$si --compression_ratio=0.5 --disable_data_sync=$dds --write_buffer_size=$wbs --target_file_size_base=$mb --max_write_buffer_number=$wbn --max_background_compactions=$mbc --level0_file_num_compaction_trigger=$ctrig --level0_slowdown_writes_trigger=$delay --level0_stop_writes_trigger=$stop --num_levels=$levels --delete_obsolete_files_period_micros=$del --min_level_to_compress=$mcz --max_grandparent_overlap_factor=$overlap --stats_per_interval=1 --max_bytes_for_level_base=$bpl --memtablerep=vector --use_existing_db=0 --disable_auto_compactions=1 --source_compaction_factor=10000000 | grep ops\n\nBefore:\nfillrandom   :     117.525 micros/op 8508 ops/sec;    6.6 MB/s\nAfter:\nfillrandom   :       1.283 micros/op 779502 ops/sec;  606.6 MB/s\n\nReviewers: rven, yhchiang, sdong\n\nReviewed By: sdong\n\nSubscribers: meyering, dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D36963",
        "modified_files_count": 1,
        "modified_files": [
            "util/thread_status_util_debug.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/08be1803eecb5ae464440812ea06e79b21289053",
        "contains_optimization_keyword": true
    },
    {
        "hash": "697380f3d7a0e7c054278f8cbb171ac2d9b8170a",
        "author": "krad",
        "date": "2015-04-10T12:35:28-07:00",
        "message": "Repairer documentation improvement.\n\nSummary: Adding verbosity to existing comments.\n\nTest Plan: None\n\nReviewers: sdong\n\nCC: leveldb\n\nTask ID: #6718960\n\nBlame Rev:",
        "modified_files_count": 1,
        "modified_files": [
            "db/repair.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/697380f3d7a0e7c054278f8cbb171ac2d9b8170a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b1bbdd79191d8fc605e3fd41e801b23022d59b5b",
        "author": "sdong",
        "date": "2015-04-08T14:40:42-07:00",
        "message": "Create EnvOptions using sanitized DB Options\n\nSummary: Now EnvOptions uses unsanitized DB options. bytes_per_sync is tuned off when rate_limiter is used, but this change doesn't take effort.\n\nTest Plan: See different I/O pattern in db_bench running fillseq.\n\nReviewers: yhchiang, kradhakrishnan, rven, anthony, igor\n\nReviewed By: igor\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D36723",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b1bbdd79191d8fc605e3fd41e801b23022d59b5b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "f69071265203edb0084f136b03bd4fcb42f16911",
        "author": "Igor Canadi",
        "date": "2015-03-13T14:45:15-07:00",
        "message": "Speed up db_bench shutdown\n\nSummary: See t6489044\n\nTest Plan: compiles\n\nReviewers: MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D34977",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/f69071265203edb0084f136b03bd4fcb42f16911",
        "contains_optimization_keyword": true
    },
    {
        "hash": "c1b3cde18abf2749ca6772cb8747510778c410b3",
        "author": "Yueh-Hsuan Chiang",
        "date": "2015-03-13T13:16:53-07:00",
        "message": "Improve the robustness of ThreadStatusSingleCompaction\n\nSummary:\nImprove the robustness of ThreadStatusSingleCompaction\nby ensuring the number of files flushed in the test.\n\nTest Plan:\nexport ROCKSDB_TESTS=ThreadStatus\n./db_test\n\nReviewers: sdong, igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D35019",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/c1b3cde18abf2749ca6772cb8747510778c410b3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2ddf53b2cad65283538e65436580f9589060a221",
        "author": "Igor Canadi",
        "date": "2015-03-10T17:53:22-07:00",
        "message": "Get OptimizeFilterForHits work on Mac\n\nSummary: Got it working by some voodoo programming\n\nTest Plan: works!\n\nReviewers: sdong\n\nReviewed By: sdong\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D34611",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2ddf53b2cad65283538e65436580f9589060a221",
        "contains_optimization_keyword": true
    },
    {
        "hash": "37921b499760f68ea8c3e2d8e44e452e8da3ddc3",
        "author": "sdong",
        "date": "2015-03-09T11:34:52-07:00",
        "message": "db_bench: Add Option -read_random_exp_range to allow read skewness.\n\nSummary: Introduce parameter -read_random_exp_range in db_bench to provide some key skewness in readrandom and multireadrandom benchmarks. It will helpful to cover block cache better.\n\nTest Plan:\nRun benchmarks with this new parameter. I can clearly see block cache hit rate change while I increase this value (DB size is about 66MB):\n\n./db_bench --benchmarks=readrandom -statistics -use_existing_db -cache_size=5000000 --read_random_exp_range=0.0\nrocksdb.block.cache.data.miss COUNT : 958418\nrocksdb.block.cache.data.hit COUNT : 41582\n\n./db_bench --benchmarks=readrandom -statistics -use_existing_db -cache_size=5000000 --read_random_exp_range=5.0\nrocksdb.block.cache.data.miss COUNT : 819518\nrocksdb.block.cache.data.hit COUNT : 180482\n\n./db_bench --benchmarks=readrandom -statistics -use_existing_db -cache_size=5000000 --read_random_exp_range=10.0\nrocksdb.block.cache.data.miss COUNT : 450479\nrocksdb.block.cache.data.hit COUNT : 549521\n\n./db_bench --benchmarks=readrandom -statistics -use_existing_db -cache_size=5000000 --read_random_exp_range=20.0\nrocksdb.block.cache.data.miss COUNT : 223192\nrocksdb.block.cache.data.hit COUNT : 776808\n\nReviewers: MarkCallaghan, kradhakrishnan, yhchiang, rven, igor\n\nReviewed By: igor\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D34629",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/37921b499760f68ea8c3e2d8e44e452e8da3ddc3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b8d23cdcb8a126f3ab38ed92e153d6483fe08f42",
        "author": "Igor Canadi",
        "date": "2015-03-03T11:29:31-08:00",
        "message": "Revert chrono use\n\nSummary:\nFor some reason, libstdc++ implements steady_clock::now() using syscall instead of VDSO optimized clock_gettime() when using glibc 2.16 and earlier. This leads to significant performance degradation for users with older glibcs. See bug reported here: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=59177\n\nWe observed this behavior when testing mongo on AWS hosts. Facebook hosts are unaffected since we use glibc2.17 and 2.20.\n\nRevert \"Fix timing\"\nThis reverts commit 965d9d50b8cbb413de5e834b5b83ddbb682d0f1d.\n\nRevert \"Use chrono for timing\"\nThis reverts commit 001ce64dc7659c65569ffb1c440e26cd23db3c94.\n\nTest Plan: make check\n\nReviewers: MarkCallaghan, yhchiang, rven, meyering, sdong\n\nReviewed By: sdong\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D34371",
        "modified_files_count": 1,
        "modified_files": [
            "util/env_posix.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b8d23cdcb8a126f3ab38ed92e153d6483fe08f42",
        "contains_optimization_keyword": true
    },
    {
        "hash": "829363b449fc6f0f9c973f530222f5767c625704",
        "author": "sdong",
        "date": "2015-02-03T09:46:04-08:00",
        "message": "Options::PrepareForBulkLoad() to increase parallelism of flushes\n\nSummary: Increasing parallelism of flushes will help bulk load throughput.\n\nTest Plan: Compile it.\n\nReviewers: MarkCallaghan, yhchiang, rven, igor\n\nReviewed By: igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D32685",
        "modified_files_count": 1,
        "modified_files": [
            "util/options.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/829363b449fc6f0f9c973f530222f5767c625704",
        "contains_optimization_keyword": true
    },
    {
        "hash": "5257c9c42c489828863f7da5d20efc4a78f3d1f4",
        "author": "Igor Canadi",
        "date": "2015-01-28T10:53:46-08:00",
        "message": "Merge pull request #452 from robertabcd/backupable-mem\n\nReduce memory footprint in backupable db.",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/backupable/backupable_db.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/5257c9c42c489828863f7da5d20efc4a78f3d1f4",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d2a2b058f089d9b0b9e70aa3b365a04008cdd504",
        "author": "sdong",
        "date": "2015-01-27T17:06:47-08:00",
        "message": "fault_injection_test: to support file closed after being deleted\n\nSummary: fault_injection_test occasionally fails because file closing can happen after deletion. Improve the test to support it.\n\nTest Plan: I have a new test case I'm working on, where the issue appears almost every time. With the patch, the problem goes away.\n\nReviewers: rven, yhchiang, igor\n\nReviewed By: igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D32373",
        "modified_files_count": 1,
        "modified_files": [
            "db/fault_injection_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d2a2b058f089d9b0b9e70aa3b365a04008cdd504",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b4c13a868a57cda00c12b7d8a202fa0499c21578",
        "author": "sdong",
        "date": "2015-01-23T15:50:15-08:00",
        "message": "fault_injection_test: improvements and add new tests\n\nSummary:\nWrapper classes in fault_injection_test doesn't simulate RocksDB Env behavior close enough. Improve it by:\n(1) when fsync, don't sync parent\n(2) support directory fsync\n(3) support multiple directories\n\nAdd test cases of\n(1) persisting by WAL fsync, not just compact range\n(2) different WAL dir\n(3) combination of (1) and (2)\n(4) data directory is not the same as db name.\n\nTest Plan: Run the test and make sure it passes.\n\nReviewers: rven, yhchiang, igor\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D32031",
        "modified_files_count": 1,
        "modified_files": [
            "db/fault_injection_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b4c13a868a57cda00c12b7d8a202fa0499c21578",
        "contains_optimization_keyword": true
    },
    {
        "hash": "bf9aa4dfcd27e7dc4f55d63d0adf5abaf86b06bd",
        "author": "Yueh-Hsuan Chiang",
        "date": "2015-01-13T00:38:09-08:00",
        "message": "Improve GetThreadStatus to avoid false alarm in some case.",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/bf9aa4dfcd27e7dc4f55d63d0adf5abaf86b06bd",
        "contains_optimization_keyword": true
    },
    {
        "hash": "628a67b0071f77ce45334601d12269a49db94d95",
        "author": "Robert",
        "date": "2015-01-09T16:58:13+08:00",
        "message": "Reduce memory footprint in backupable db.\n\n* Use emplace when possible.\n* Make FileInfo shared among all BackupMeta, instead of storing filenames.\n* Make checksum_value in FileInfo constant.\n* Reserve space beforehand if container size is known.\n* Make FileInfo and BackupMeta non-copyable and non-assignable to prevent future logic errors.\n  It is very dangerous to copy BackupMeta without careful handling refcounts of FileInfo.\n* Remove a copy of BackupMeta when detected corrupt backup.",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/backupable/backupable_db.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/628a67b0071f77ce45334601d12269a49db94d95",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9619081d9bf14b74a203bf308af6f92fdb9967b1",
        "author": "Igor Canadi",
        "date": "2015-01-05T09:59:50-08:00",
        "message": "Merge pull request #449 from robertabcd/improve-backupable\n\nImprove backupable db performance on loading BackupMeta",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/backupable/backupable_db.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9619081d9bf14b74a203bf308af6f92fdb9967b1",
        "contains_optimization_keyword": true
    },
    {
        "hash": "caa1fd0e0e26b520ff4fd2da25224a3372935a7d",
        "author": "Robert",
        "date": "2015-01-04T12:19:32+08:00",
        "message": "Improve performance when loading BackupMeta.\n\n* Use strtoul() and strtoull() instead of sscanf().\n  glibc's sscanf() will do a implicit strlen().\n\n* Move implicit construction of Slice(\"crc32 \") out of loop.",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/backupable/backupable_db.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/caa1fd0e0e26b520ff4fd2da25224a3372935a7d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "0ab0242f373608c304aead5a291335a784ae0176",
        "author": "sdong",
        "date": "2014-12-10T18:53:30-08:00",
        "message": "VersionBuilder to use unordered set and map to store added and deleted files\n\nSummary: Set operations in VerisonBuilder is shown as a performance bottleneck of restarting DB when there are lots of files. Make both of added_files and deleted_files use unordered set or map. Only when adding the files, sort the added files.\n\nTest Plan: make all check\n\nReviewers: yhchiang, rven, igor\n\nReviewed By: igor\n\nSubscribers: hermanlee4, leveldb, dhruba, ljin\n\nDifferential Revision: https://reviews.facebook.net/D30051",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/0ab0242f373608c304aead5a291335a784ae0176",
        "contains_optimization_keyword": true
    },
    {
        "hash": "c0dee851c32f7afebb202856a8da36aeb87ee694",
        "author": "Mark Callaghan",
        "date": "2014-12-04T10:34:06-08:00",
        "message": "Improve formatting, add missing newlines\n\nSummary:\nImprove formatting\n\nTask ID: #\n\nBlame Rev:\n\nTest Plan:\nmake\n\nRevert Plan:\n\nDatabase Impact:\n\nMemcache Impact:\n\nOther Notes:\n\nEImportant:\n\n- begin *PUBLIC* platform impact section -\nBugzilla: #\n- end platform impact -\n\nReviewers: igor\n\nReviewed By: igor\n\nSubscribers: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D29829",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/c0dee851c32f7afebb202856a8da36aeb87ee694",
        "contains_optimization_keyword": true
    },
    {
        "hash": "90ee85f8e11d52a04dedc663f20c8128ee0bde8d",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-11-24T18:28:06-08:00",
        "message": "Improve listener_test to avoid possible false alarm\n\nSummary:\nImprove listener_test to avoid possible false alarm\n\nTest Plan:\n./listener_test",
        "modified_files_count": 1,
        "modified_files": [
            "db/listener_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/90ee85f8e11d52a04dedc663f20c8128ee0bde8d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "aa31fc506827b11b585d150b63b42f103a74f07a",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-11-21T10:22:05-08:00",
        "message": "Improve listener_test by ensuring flushes are completed before assert.\n\nSummary: Improve listener_test by ensuring flushes are completed before assert.\n\nTest Plan: listener_test\n\nReviewers: ljin, sdong, igor\n\nReviewed By: igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D29319",
        "modified_files_count": 1,
        "modified_files": [
            "db/listener_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/aa31fc506827b11b585d150b63b42f103a74f07a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "367a3f9cb4ff1b7d5c23f2fc4b37e8ef55b4971d",
        "author": "sdong",
        "date": "2014-11-06T10:48:06-08:00",
        "message": "Improve DBTest.GroupCommitTest: artificially slowdown log writing to trigger group commit\n\nSummary: In order to avoid random failure of DBTest.GroupCommitTest, artificially sleep 100 microseconds in each log writing.\n\nTest Plan: Run the test in a machine where valgrind version of the test always fails multiple times and see it always succeed.\n\nReviewers: igor, yhchiang, rven, ljin\n\nReviewed By: ljin\n\nSubscribers: leveldb, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D28401",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/367a3f9cb4ff1b7d5c23f2fc4b37e8ef55b4971d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "fb3f8ffe5e7c0bdecc40f89cf95cb1b86746b729",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-10-28T15:35:10-07:00",
        "message": "Improve the robustness of PartialCompactionFailure test again.\n\nSummary:\nImprove the robustness of PartialCompactionFailure test again.\n\nTest Plan:\n./db_test",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/fb3f8ffe5e7c0bdecc40f89cf95cb1b86746b729",
        "contains_optimization_keyword": true
    },
    {
        "hash": "60fa7d1365323f32b29b3441b3678dd3c319dafc",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-10-28T15:17:50-07:00",
        "message": "Improve the robustnesss of PartialCompactionFailure test.\n\nSummary:\nImprove the robustness of PartialCompactionFailure test.\n\nTest Plan:\n./db_test",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/60fa7d1365323f32b29b3441b3678dd3c319dafc",
        "contains_optimization_keyword": true
    },
    {
        "hash": "724fba2b397396978bbe9533c5be81564ffe090e",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-10-23T15:43:51-07:00",
        "message": "Improve the log in Universal Compaction to include more debug information.\n\nSummary:\nPreviously, the log for Universal Compaction does not include the current\nnumber of files in case the compaction is triggered by the number of files.\nThis diff includes the number of files in the log.\n\nTest Plan:\nmake",
        "modified_files_count": 1,
        "modified_files": [
            "db/compaction_picker.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/724fba2b397396978bbe9533c5be81564ffe090e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e11a5e776fa44611f34ca8676480fd6a9c21f4be",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-10-21T17:28:31-07:00",
        "message": "Improve the comment of util/thread_local.h\n\nSummary: Improve the comment of util/thread_local.h\n\nTest Plan: n/a\n\nReviewers: ljin\n\nReviewed By: ljin\n\nDifferential Revision: https://reviews.facebook.net/D25449",
        "modified_files_count": 1,
        "modified_files": [
            "util/thread_local.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e11a5e776fa44611f34ca8676480fd6a9c21f4be",
        "contains_optimization_keyword": true
    },
    {
        "hash": "55652043c83c463ce57b7748e01c6d12bb5bf9fe",
        "author": "Danny Al-Gaaf",
        "date": "2014-10-01T10:49:08+02:00",
        "message": "table/cuckoo_table_reader.cc: pass func parameter by reference\n\nFix for:\n\n[table/cuckoo_table_reader.cc:196]: (performance) Function\n parameter 'target' should be passed by reference.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "table/cuckoo_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/55652043c83c463ce57b7748e01c6d12bb5bf9fe",
        "contains_optimization_keyword": true
    },
    {
        "hash": "43c789c8f246a2a35864e3fca9585b55c40c2095",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:32+02:00",
        "message": "spatialdb/spatial_db.cc: use !empty() instead of 'size() > 0'\n\nUse empty() since it should be prefered as it has, following\nthe standard, a constant time complexity regardless of the\ncontainter type. The same is not guaranteed for size().\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/spatialdb/spatial_db.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/43c789c8f246a2a35864e3fca9585b55c40c2095",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4cc8643baf5b4e4a25fb20a77b3257100747d483",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:32+02:00",
        "message": "util/ldb_cmd.cc: prefer prefix ++operator for non-primitive types\n\nPrefer prefix ++operator for non-primitive types like iterators for\nperformance reasons. Prefix ++/-- operators avoid creating a temporary\ncopy.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "util/ldb_cmd.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4cc8643baf5b4e4a25fb20a77b3257100747d483",
        "contains_optimization_keyword": true
    },
    {
        "hash": "873f1356a1781e8d638973ea320b722d3240fc5a",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:32+02:00",
        "message": "db_ttl_impl.h: pass func parameter by reference\n\nFix for:\n\n[utilities/ttl/db_ttl_impl.h:209]: (performance) Function parameter\n 'merge_op' should be passed by reference.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/ttl/db_ttl_impl.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/873f1356a1781e8d638973ea320b722d3240fc5a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "8558457143bfa76d61e0d2f715e40ec2ddb6ffc2",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:32+02:00",
        "message": "ldb_cmd_execute_result.h: perform init in initialization list\n\nFix for:\n\n[util/ldb_cmd_execute_result.h:18]: (performance) Variable 'message_'\n is assigned in constructor body. Consider performing initialization\n in initialization list.\n[util/ldb_cmd_execute_result.h:23]: (performance) Variable 'message_'\n is assigned in constructor body. Consider performing initialization\n in initialization list.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "util/ldb_cmd_execute_result.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/8558457143bfa76d61e0d2f715e40ec2ddb6ffc2",
        "contains_optimization_keyword": true
    },
    {
        "hash": "063471bf7613544496a4d4b5a1e1ba4a7aa605cf",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:32+02:00",
        "message": "table/table_test.cc: pass func parameter by reference\n\nFix for:\n\n[table/table_test.cc:1218]: (performance) Function parameter\n 'prefix' should be passed by reference.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "table/table_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/063471bf7613544496a4d4b5a1e1ba4a7aa605cf",
        "contains_optimization_keyword": true
    },
    {
        "hash": "93548ce8f451a701ad0967ba705f04fef80aa11a",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:32+02:00",
        "message": "table/cuckoo_table_reader.cc: pass func parameter by ref\n\nFix for:\n\n[table/cuckoo_table_reader.cc:198]: (performance) Function\n parameter 'file_data' should be passed by reference.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "table/cuckoo_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/93548ce8f451a701ad0967ba705f04fef80aa11a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b8b7117e97e649fc65d0a4dd397caf9a39fb71b1",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:31+02:00",
        "message": "db/version_set.cc: use !empty() instead of 'size() > 0'\n\nUse empty() since it should be prefered as it has, following\nthe standard, a constant time complexity regardless of the\ncontainter type. The same is not guaranteed for size().\n\nFix for:\n[db/version_set.cc:2250]: (performance) Possible inefficient\n checking for 'column_families_not_found' emptiness.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b8b7117e97e649fc65d0a4dd397caf9a39fb71b1",
        "contains_optimization_keyword": true
    },
    {
        "hash": "53910ddb152fbcba95a3e04b058a997c40f654ae",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:31+02:00",
        "message": "db_test.cc: pass parameter by reference\n\nFix for:\n\n[db/db_test.cc:6141]: (performance) Function parameter\n 'key' should be passed by reference.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/53910ddb152fbcba95a3e04b058a997c40f654ae",
        "contains_optimization_keyword": true
    },
    {
        "hash": "68ca534169a4f9e1930f6511109e973b43cf5998",
        "author": "Danny Al-Gaaf",
        "date": "2014-09-30T23:30:31+02:00",
        "message": "corruption_test.cc: pass parameter by reference\n\nFix for:\n\n[db/corruption_test.cc:134]: (performance) Function parameter\n 'fname' should be passed by reference.\n\nSigned-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>",
        "modified_files_count": 1,
        "modified_files": [
            "db/corruption_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/68ca534169a4f9e1930f6511109e973b43cf5998",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1f963305a8f3384da3215c37ed7a264c5c99417c",
        "author": "Mark Callaghan",
        "date": "2014-09-29T17:51:40-07:00",
        "message": "Print MB per second compaction throughput separately for reads and writes\n\nSummary:\nFrom this line there used to be one column (MB/sec) that includes reads and writes. This change splits it and for real workloads the rd and wr rates might not match when keys are dropped.\n2014/09/29-17:31:01.213162 7f929fbff700 (Original Log Time 2014/09/29-17:31:01.180025) [default] compacted to: files[2 5 0 0 0 0 0], MB/sec: 14.0 rd, 14.0 wr, level 1, files in(4, 0) out(5) MB in(8.5, 0.0) out(8.5), read-write-amplify(2.0) write-amplify(1.0) OK\n\nTest Plan:\nmake check, grepped LOG\n\n- begin *PUBLIC* platform impact section -\nBugzilla: #\n- end platform impact -\n\nReviewers: igor\n\nDifferential Revision: https://reviews.facebook.net/D24237",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1f963305a8f3384da3215c37ed7a264c5c99417c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9b976e34f54bc3f58224231e716acc63906293ca",
        "author": "Igor Canadi",
        "date": "2014-09-03T08:42:25-07:00",
        "message": "Merge pull request #259 from wankai/master\n\ntypo improvement",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based_table_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9b976e34f54bc3f58224231e716acc63906293ca",
        "contains_optimization_keyword": true
    },
    {
        "hash": "dff2b1a8f89211b97eea45cf6ce776b06388bc9d",
        "author": "Wankai Zhang",
        "date": "2014-09-02T22:57:03+08:00",
        "message": "typo improvement",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based_table_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/dff2b1a8f89211b97eea45cf6ce776b06388bc9d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "0c26e76b28323a6ab10b0aae8832e6d76339cf24",
        "author": "Igor Canadi",
        "date": "2014-08-28T20:40:10-04:00",
        "message": "Merge pull request #237 from tdfischer/tdfischer/faster-timeout-test\n\ntest: db: fix test to have a smaller timeout for when it runs on faster ...",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/0c26e76b28323a6ab10b0aae8832e6d76339cf24",
        "contains_optimization_keyword": true
    },
    {
        "hash": "6b46f784346070b5ed9df2e2464b73b0bb8236d8",
        "author": "Igor Canadi",
        "date": "2014-08-28T09:42:31-04:00",
        "message": "Merge pull request #248 from wankai/master\n\nBlockBuilder typo improvement",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_builder.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/6b46f784346070b5ed9df2e2464b73b0bb8236d8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "536e9973e30d70fd510e5ab6e423ef75248ed582",
        "author": "Igor Canadi",
        "date": "2014-08-27T11:05:41-07:00",
        "message": "Remove assert in vector rep\n\nSummary: This assert makes Insert O(n^2) instead of O(n) in debug mode. Memtable insert is in the critical path. No need to assert uniqunnes of the key here, since we're adding a sequence number to it anyway.\n\nTest Plan: none\n\nReviewers: sdong, ljin\n\nReviewed By: ljin\n\nSubscribers: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D22443",
        "modified_files_count": 1,
        "modified_files": [
            "util/vectorrep.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/536e9973e30d70fd510e5ab6e423ef75248ed582",
        "contains_optimization_keyword": true
    },
    {
        "hash": "01cbdd2aae8f998e3e532dec06f0f373a6cff719",
        "author": "Igor Canadi",
        "date": "2014-08-20T11:14:01-07:00",
        "message": "Optimize storage parameters for spatialDB\n\nSummary: We need to start compression at level 1, while OptimizeForLevelComapaction() only sets up rocksdb to start compressing at level 2. I also adjusted some other things.\n\nTest Plan: compiles\n\nReviewers: yinwang\n\nReviewed By: yinwang\n\nDifferential Revision: https://reviews.facebook.net/D22203",
        "modified_files_count": 1,
        "modified_files": [
            "utilities/spatialdb/spatial_db.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/01cbdd2aae8f998e3e532dec06f0f373a6cff719",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7c5173d27f4432fe7799f5fc7a85f857b61a3d6b",
        "author": "Torrie Fischer",
        "date": "2014-08-19T13:45:12-07:00",
        "message": "test: db: fix test to have a smaller timeout for when it runs on faster hardware",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7c5173d27f4432fe7799f5fc7a85f857b61a3d6b",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4a8f0c957ce8e0c3a7928256d1149c0fc3e67262",
        "author": "sdong",
        "date": "2014-07-25T17:27:57-07:00",
        "message": "Block::Iter::PrefixSeek() to have an extra check to filter out some false matches\n\nSummary:\nIn block based table's hash index checking, when looking for a key that doesn't exist, there is a high chance that a false block is returned because of hash bucket conflicts. In this revision, another check is done to filter out some of those cases: comparing previous key of the block boundary to see whether the target block is what we are looking for.\n\nIn a favored test setting (bloom filter disabled, 8 L0 files), I saw about 80% improvements. In a non-favored test setting (bloom filter enabled, files are all in L1, files are all cached), I see the performance penalty is less than 3%.\n\nTest Plan: make all check\n\nReviewers: haobo, ljin\n\nReviewed By: ljin\n\nSubscribers: wuj, leveldb, zagfox, yhchiang\n\nDifferential Revision: https://reviews.facebook.net/D20595",
        "modified_files_count": 1,
        "modified_files": [
            "table/block.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4a8f0c957ce8e0c3a7928256d1149c0fc3e67262",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9c0d84d240ceab1cfc27a72a32e5fa632d93b0b9",
        "author": "Lei Jin",
        "date": "2014-07-21T11:03:16-07:00",
        "message": "improve comments for CrateRateLimiter()\n\nSummary:\nSuggested by @dhruba from the other diff, here is the improved\ncomments for parameters of the function\n\nTest Plan: none\n\nReviewers: dhruba, sdong, igor\n\nReviewed By: igor\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D19623",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9c0d84d240ceab1cfc27a72a32e5fa632d93b0b9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "dd6c444822f34239d3e80887b659112aa5b15307",
        "author": "sdong",
        "date": "2014-07-14T14:43:59-07:00",
        "message": "Improve Put()'s comment to indicate that the key is overwritten if existing\n\nSummary: As title\n\nTest Plan: Not needed for comment only.\n\nReviewers: yhchiang, ljin, MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nSubscribers: xjin, dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D19887",
        "modified_files_count": 1,
        "modified_files": [
            "include/rocksdb/db.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/dd6c444822f34239d3e80887b659112aa5b15307",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b278ae8e50466e8073a1754a506145df5bb27c72",
        "author": "Lei Jin",
        "date": "2014-07-08T11:40:42-07:00",
        "message": "Apply fractional cascading in ForwardIterator::Seek()\n\nSummary:\nUse search hint to reduce FindFile range thus avoid comparison\nFor a small DB with 50M keys, perf_context counter shows it reduces\ncomparison from 2B to 1.3B for a 15-minute run. No perf change was\nobserved for 1 seek thread, but quite good improvement was seen for 32\nseek threads, when CPU was busy.\nwill post detail results when ready\n\nTest Plan: db_bench and db_test\n\nReviewers: haobo, sdong, dhruba, igor\n\nReviewed By: igor\n\nSubscribers: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D18879",
        "modified_files_count": 1,
        "modified_files": [
            "db/forward_iterator.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b278ae8e50466e8073a1754a506145df5bb27c72",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7b85c1e900a1e0f47c78ff1ddfa8ddd924715eaf",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-07-04T00:02:12-07:00",
        "message": "Improve SimpleWriteTimeoutTest to avoid false alarm.\n\nSummary:\nSimpleWriteTimeoutTest has two parts: 1) insert two large key/values\nto make memtable full and expect both of them are successful; 2) insert\nanother key / value and expect it to be timed-out.  Previously we also\nset a timeout in the first step, but this might sometimes cause\nfalse alarm.\n\nThis diff makes the first two writes run without timeout setting.\n\nTest Plan:\nexport ROCKSDB_TESTS=Time\nmake db_test\n\nReviewers: sdong, ljin\n\nReviewed By: ljin\n\nSubscribers: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D19461",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7b85c1e900a1e0f47c78ff1ddfa8ddd924715eaf",
        "contains_optimization_keyword": true
    },
    {
        "hash": "8898a0a0d123132fdcad196c06d352a5edf3b47f",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-06-24T19:22:11-06:00",
        "message": "Reorder the member variables of FileMetaData to improve cache locality.\n\nSummary:\nMove stats related member variables of FileMetaData to the bottom to\nimprove cache locality of normal DB operations.\n\nTest Plan: make\n\nReviewers: haobo, ljin, sdong\n\nReviewed By: sdong\n\nSubscribers: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D19287",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_edit.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/8898a0a0d123132fdcad196c06d352a5edf3b47f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "faa8d21922b09988c86c75887cd7a49895120e25",
        "author": "Yueh-Hsuan Chiang",
        "date": "2014-06-24T15:29:28-06:00",
        "message": "Improve an assertion in RandomGenerator::Generate() in db_bench.\n\nSummary:\nRandomGenerator::Generate() currently has an assertion len < data_.size().\nHowever, it is actually fine to have len == data_.size().\nThis diff change the assertion to len <= data_.size().\n\nTest Plan:\nmake db_bench\n./db_bench\n\nReviewers: haobo, sdong, ljin\n\nReviewed By: ljin\n\nSubscribers: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D19269",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/faa8d21922b09988c86c75887cd7a49895120e25",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3b0dc76699c895a4698602547fb474eeffff512e",
        "author": "Lei Jin",
        "date": "2014-06-23T13:23:02-07:00",
        "message": "db_bench: measure the real latency of write/delete\n\nSummary: as title\n\nTest Plan: make release\n\nReviewers: haobo, sdong, yhchiang\n\nReviewed By: yhchiang\n\nSubscribers: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D19227",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3b0dc76699c895a4698602547fb474eeffff512e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2413a06c7bf2a1f3e86680ceb041e75c729f44f1",
        "author": "Igor Canadi",
        "date": "2014-04-24T09:22:58-04:00",
        "message": "Improve stability of db_stress\n\nSummary:\nCurrently, whenever DB Verification fails we bail out by calling `exit(1)`. This is kind of bad since it causes unclean shutdown and spew of error log messages like:\n\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n    05:03:27 pthread lock: Invalid argument\n\nThis diff adds a new parameter that is set to true when verification fails. It can then use the parameter to bail out safely.\n\nTest Plan: Casued artificail failure. Verified that exit was clean.\n\nReviewers: dhruba, haobo, ljin\n\nReviewed By: haobo\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D18243",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2413a06c7bf2a1f3e86680ceb041e75c729f44f1",
        "contains_optimization_keyword": true
    },
    {
        "hash": "d5e087b6df8ba564eb67c370980dbf17dd417e54",
        "author": "sdong",
        "date": "2014-04-11T16:59:08-07:00",
        "message": "db_bench: add a mode to operate multiple DBs\n\nSummary: This patch introduces a new parameter num_multi_db in db_bench. When this parameter is larger than 1, multiple DBs will be created. In all benchmarks, any operation applies to a random DB among them. This is to benchmark the performance of similar applications.\n\nTest Plan: run db_bench on both of num_multi_db=0 and more.\n\nReviewers: haobo, ljin, igor\n\nReviewed By: igor\n\nCC: igor, yhchiang, dhruba, nkg-, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D17769",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/d5e087b6df8ba564eb67c370980dbf17dd417e54",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e23e73e67c85af70274f0d799050f37cc92a1cad",
        "author": "Kai Liu",
        "date": "2014-04-10T22:45:25-07:00",
        "message": "Use shorten index key for hash-index\n\nSummary:\nI was wrong about the \"index builder\", right now since we create index\nby scanning both whole table and index, there is not need to preserve\nthe whole key as the index key.\n\nI switch back to original way index which is both space efficient and\nable to supprot in-fly construction of hash index.\n\nIN this patch, I made minimal change since I'm not sure if we still need\nthe \"pluggable index builder\", under current circumstance it is of no use\nand kind of over-engineered. But I'm not sure if we can still exploit its\nusefulness in the future; otherwise I think I can just burn them with great\nvengeance.\n\nTest Plan: unit tests\n\nReviewers: sdong, haobo\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D17745",
        "modified_files_count": 1,
        "modified_files": [
            "table/block_based_table_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e23e73e67c85af70274f0d799050f37cc92a1cad",
        "contains_optimization_keyword": true
    },
    {
        "hash": "dc55903293790492bef4a26cb58bacfca2a6ebe8",
        "author": "Igor Canadi",
        "date": "2014-04-09T11:43:14-07:00",
        "message": "Improved CompressedCache\n\nSummary:\nThis is testing behavior that was reported in https://github.com/facebook/rocksdb/issues/111\n\nNo issue was found, but it still good to commit this and make CompressedCache more robust.\n\nTest Plan: this is a plan\n\nReviewers: ljin, dhruba\n\nReviewed By: dhruba\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D17625",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/dc55903293790492bef4a26cb58bacfca2a6ebe8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "beeee9dccc338ae7129016f2f2e17d2a40ecc5df",
        "author": "Igor Canadi",
        "date": "2014-04-08T11:06:39-07:00",
        "message": "Small speedup of CompactionFilterV2\n\nSummary: ToString() is expensive. Profiling shows that most compaction threads are stuck in jemalloc, allocating a new string. This will help out a litte.\n\nTest Plan: make check\n\nReviewers: haobo, danguo\n\nReviewed By: danguo\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D17583",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/beeee9dccc338ae7129016f2f2e17d2a40ecc5df",
        "contains_optimization_keyword": true
    },
    {
        "hash": "c9622aab776c3deff8cbeea41f26b132318a5045",
        "author": "Igor Canadi",
        "date": "2014-04-02T16:00:22-07:00",
        "message": "Merge pull request #107 from alberts/fastah\n\ncrc32: build a whole special Extend function for SSE 4.2.",
        "modified_files_count": 1,
        "modified_files": [
            "util/crc32c.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/c9622aab776c3deff8cbeea41f26b132318a5045",
        "contains_optimization_keyword": true
    },
    {
        "hash": "2d3468c2939b86eaa8b68c9c5c163504dccc0de0",
        "author": "sdong",
        "date": "2014-03-28T16:46:25-07:00",
        "message": "MemTableIterator not to reference Memtable\n\nSummary: In one of the perf, I shows 10%-25% CPU costs of MemTableIterator.Seek(), when doing dynamic prefix seek, are spent on checking whether we need to do bloom filter check or finding out the prefix extractor. Seems that  more level of pointer checking makes CPU cache miss more likely. This patch makes things slightly simpler by copying pointer of bloom of prefix extractor into the iterator.\n\nTest Plan: make all check\n\nReviewers: haobo, ljin\n\nReviewed By: ljin\n\nCC: igor, dhruba, yhchiang, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D17247",
        "modified_files_count": 1,
        "modified_files": [
            "db/memtable.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/2d3468c2939b86eaa8b68c9c5c163504dccc0de0",
        "contains_optimization_keyword": true
    },
    {
        "hash": "64138b5d9cae2990aa46157da97cdf1656c1a046",
        "author": "Lei Jin",
        "date": "2014-03-05T10:28:53-08:00",
        "message": "fix db_bench to use HashSkipList for real\n\nSummary:\nFor HashSkipList case, DBImpl has sanity check to see if prefix_extractor in\noptions is the same as the one in memtable factory. If not, it falls\nback to SkipList. As result, I was experimenting with SkipList\nperformance. No wonder it is much worse than LinkedList\n\nTest Plan: ran benchmark\n\nReviewers: haobo, sdong, igor\n\nReviewed By: igor\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D16569",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/64138b5d9cae2990aa46157da97cdf1656c1a046",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a04dbf6e49d7fe25ab99cbf2d4d7357d618b4545",
        "author": "sdong",
        "date": "2014-02-26T15:12:44-08:00",
        "message": "PlainTable::Next() should pass the error message from ReadKey()\n\nSummary:\nPlainTable::Next() should pass the error message from ReadKey(). Now it would return a wrong error message.\nAlso improve the messages of status when failing to read\n\nTest Plan: make all check\n\nReviewers: ljin, kailiu, haobo\n\nReviewed By: kailiu\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D16365",
        "modified_files_count": 1,
        "modified_files": [
            "table/plain_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a04dbf6e49d7fe25ab99cbf2d4d7357d618b4545",
        "contains_optimization_keyword": true
    },
    {
        "hash": "6aef661230fbf7c28ccc77d6e19782911fba41fc",
        "author": "Igor Canadi",
        "date": "2014-02-14T17:47:53-08:00",
        "message": "some improvements to CompressedCache test",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/6aef661230fbf7c28ccc77d6e19782911fba41fc",
        "contains_optimization_keyword": true
    },
    {
        "hash": "59cffe02c404b3e5773166e4e00c2def09de8335",
        "author": "Kai Liu",
        "date": "2014-02-13T13:57:36-08:00",
        "message": "Benchmark table reader wiht nanoseconds\n\nSummary: nanosecnods gave us better view of the performance, especially when some operations are fast so that micro seconds may only reveal less informative results.\n\nTest Plan:\nsample output:\n\n    ./table_reader_bench --plain_table --time_unit=nanosecond\n    =======================================================================================================\n    InMemoryTableSimpleBenchmark:           PlainTable   num_key1:   4096   num_key2:   512   non_empty\n    =======================================================================================================\n    Histogram (unit: nanosecond):\n    Count: 6291456  Average: 475.3867  StdDev: 556.05\n    Min: 135.0000  Median: 400.1817  Max: 33370.0000\n    Percentiles: P50: 400.18 P75: 530.02 P99: 887.73 P99.9: 8843.26 P99.99: 9941.21\n    ------------------------------------------------------\n    [     120,     140 )        2   0.000%   0.000%\n    [     140,     160 )      452   0.007%   0.007%\n    [     160,     180 )    13683   0.217%   0.225%\n    [     180,     200 )    54353   0.864%   1.089%\n    [     200,     250 )   101004   1.605%   2.694%\n    [     250,     300 )   729791  11.600%  14.294% ##\n    [     300,     350 )   616070   9.792%  24.086% ##\n    [     350,     400 )  1628021  25.877%  49.963% #####\n    [     400,     450 )   647220  10.287%  60.250% ##\n    [     450,     500 )   577206   9.174%  69.424% ##\n    [     500,     600 )  1168585  18.574%  87.999% ####\n    [     600,     700 )   506875   8.057%  96.055% ##\n    [     700,     800 )   147878   2.350%  98.406%\n    [     800,     900 )    42633   0.678%  99.083%\n    [     900,    1000 )    16304   0.259%  99.342%\n    [    1000,    1200 )     7811   0.124%  99.466%\n    [    1200,    1400 )     1453   0.023%  99.490%\n    [    1400,    1600 )      307   0.005%  99.494%\n    [    1600,    1800 )       81   0.001%  99.496%\n    [    1800,    2000 )       18   0.000%  99.496%\n    [    2000,    2500 )        8   0.000%  99.496%\n    [    2500,    3000 )        6   0.000%  99.496%\n    [    3500,    4000 )        3   0.000%  99.496%\n    [    4000,    4500 )      116   0.002%  99.498%\n    [    4500,    5000 )     1144   0.018%  99.516%\n    [    5000,    6000 )     1087   0.017%  99.534%\n    [    6000,    7000 )     2403   0.038%  99.572%\n    [    7000,    8000 )     9840   0.156%  99.728%\n    [    8000,    9000 )    12820   0.204%  99.932%\n    [    9000,   10000 )     3881   0.062%  99.994%\n    [   10000,   12000 )      135   0.002%  99.996%\n    [   12000,   14000 )      159   0.003%  99.998%\n    [   14000,   16000 )       58   0.001%  99.999%\n    [   16000,   18000 )       30   0.000% 100.000%\n    [   18000,   20000 )       14   0.000% 100.000%\n    [   20000,   25000 )        2   0.000% 100.000%\n    [   25000,   30000 )        2   0.000% 100.000%\n    [   30000,   35000 )        1   0.000% 100.000%\n\nReviewers: haobo, dhruba, sdong\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D16113",
        "modified_files_count": 1,
        "modified_files": [
            "table/table_reader_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/59cffe02c404b3e5773166e4e00c2def09de8335",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7dea558e6d202c9dbf34e15077d5c9c8db594bf9",
        "author": "Siying Dong",
        "date": "2014-01-21T12:44:43-08:00",
        "message": "[Performance Branch] Fix a bug when merging from master\n\nSummary: Commit \"1304d8c8cefe66be1a3caa5e93413211ba2486f2\" (Merge branch 'master' into performance) removes a line in performance branch by mistake. This patch fixes it.\n\nTest Plan: make all check\n\nReviewers: haobo, kailiu, igor\n\nReviewed By: haobo\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D15297",
        "modified_files_count": 1,
        "modified_files": [
            "db/memtable.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7dea558e6d202c9dbf34e15077d5c9c8db594bf9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4e8321bfeae8541fb5d827dfcb089e39078841bc",
        "author": "Mark Callaghan",
        "date": "2014-01-17T21:32:23-08:00",
        "message": "Boost access before mutex is unlocked\n\nSummary:\nThis moves the use of versions_ to before the mutex is unlocked\nto avoid a possible race.\n\nTask ID: #\n\nBlame Rev:\n\nTest Plan:\nmake check\n\nRevert Plan:\n\nDatabase Impact:\n\nMemcache Impact:\n\nOther Notes:\n\nEImportant:\n\n- begin *PUBLIC* platform impact section -\nBugzilla: #\n- end platform impact -\n\nReviewers: haobo, dhruba\n\nReviewed By: dhruba\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D15279",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4e8321bfeae8541fb5d827dfcb089e39078841bc",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9b51af5a17f3cfd754575894e090dd867fb47740",
        "author": "Siying Dong",
        "date": "2014-01-14T17:41:44-08:00",
        "message": "[RocksDB Performance Branch] DBImpl.NewInternalIterator() to reduce works inside mutex\n\nSummary: To reduce mutex contention caused by DBImpl.NewInternalIterator(), in this function, move all the iteration creation works out of mutex, only leaving object ref and get.\n\nTest Plan:\nmake all check\nwill run db_stress for a while too to make sure no problem.\n\nReviewers: haobo, dhruba, kailiu\n\nReviewed By: haobo\n\nCC: igor, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14589\n\nConflicts:\n\tdb/db_impl.cc",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9b51af5a17f3cfd754575894e090dd867fb47740",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9281a826f1f80b0e6d423f296ef5639aa48a331a",
        "author": "kailiu",
        "date": "2014-01-02T10:30:42-08:00",
        "message": "Hotfix the bug in table cache's GetSliceForFileNumber\n\nForgot to fix this problem in master branch. Already fixed it in performance branch.",
        "modified_files_count": 1,
        "modified_files": [
            "db/table_cache.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9281a826f1f80b0e6d423f296ef5639aa48a331a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "bf4a48ccb356cf5ed205a30201e751218da7cfb0",
        "author": "Haobo Xu",
        "date": "2013-12-20T18:20:06-08:00",
        "message": "[RocksDB] [Performance Branch] Revert previous patch.\n\nSummary: The previous patch is wrong. rep_.resize(kHeader) just resets the header portion to zero, and should not cause a re-allocation if g++ does it right. I will go ahead and revert it.\n\nTest Plan: make check\n\nReviewers: dhruba, sdong\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14793",
        "modified_files_count": 1,
        "modified_files": [
            "db/write_batch.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/bf4a48ccb356cf5ed205a30201e751218da7cfb0",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e94eea4527f2d7de82a6bf3303177977011e5dd9",
        "author": "Haobo Xu",
        "date": "2013-12-20T16:29:05-08:00",
        "message": "[RocksDB] [Performance Branch] Minor fix, Remove string resize from WriteBatch::Clear\n\nSummary: tmp_batch_ will get re-allocated for every merged write batch because of the existing resize in WriteBatch::Clear. Note that in DBImpl::BuildBatchGroup, we have a hard coded upper limit of batch size 1<<20 = 1MB already.\n\nTest Plan: make check\n\nReviewers: dhruba, sdong\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14787",
        "modified_files_count": 1,
        "modified_files": [
            "db/write_batch.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e94eea4527f2d7de82a6bf3303177977011e5dd9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "14995a8ff3110bbcd19c34cd92a449ca3e435f5d",
        "author": "Siying Dong",
        "date": "2013-12-17T18:06:58-08:00",
        "message": "Move level0 sorting logic from Version::SaveTo() to Version::Finalize()\n\nSummary: I realized that \"D14409 Avoid sorting in Version::Get() by presorting them in VersionSet::Builder::SaveTo()\" is not done in an optimized place. SaveTo() is usually inside mutex. Move it to Finalize(), which is called out of mutex.\n\nTest Plan: make all check\n\nReviewers: dhruba, haobo, kailiu\n\nReviewed By: dhruba\n\nCC: igor, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14607",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/14995a8ff3110bbcd19c34cd92a449ca3e435f5d",
        "contains_optimization_keyword": true
    },
    {
        "hash": "5090316f0ded4bf9af04d78cbacb031824d31c3f",
        "author": "Haobo Xu",
        "date": "2013-12-13T14:21:59-08:00",
        "message": "[RocksDB] [Performance Branch] Trivia build fix\n\nSummary: make release complains signed unsigned comparison.\n\nTest Plan: make release\n\nReviewers: kailiu\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14661",
        "modified_files_count": 1,
        "modified_files": [
            "util/cache_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/5090316f0ded4bf9af04d78cbacb031824d31c3f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "9718c790ec286fe2dad70dea491b54c34e5547a7",
        "author": "Siying Dong",
        "date": "2013-12-12T22:22:35-08:00",
        "message": "[Performance Branch] Fix a bug of PlainTable when building indexes\n\nSummary:\nPlainTable now has a bug of the ordering of indexes for the prefixes in the same bucket. I thought std::map guaranteed key order but it didn't, probably because I didn't use it properly. But seems to me that we don't need to make extra sorting as input prefixes are already sorted. Found by problem by running leaf4 against plain table. Replace the map with a vector. It should performs better too.\n\nAfter the fix, leaf4 unit tests are passing.\n\nTest Plan:\nrun plain_table_db_test\nAlso going to run db_test with plain table in the uncommitted branch.\n\nReviewers: haobo, kailiu\n\nReviewed By: haobo\n\nCC: nkg-, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14649",
        "modified_files_count": 1,
        "modified_files": [
            "table/plain_table_reader.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/9718c790ec286fe2dad70dea491b54c34e5547a7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e8ab1934d9cb3ffebd61097d67bb23439554b265",
        "author": "Siying Dong",
        "date": "2013-12-12T11:30:00-08:00",
        "message": "[RocksDB Performance Branch] DBImpl.NewInternalIterator() to reduce works inside mutex\n\nSummary: To reduce mutex contention caused by DBImpl.NewInternalIterator(), in this function, move all the iteration creation works out of mutex, only leaving object ref and get.\n\nTest Plan:\nmake all check\nwill run db_stress for a while too to make sure no problem.\n\nReviewers: haobo, dhruba, kailiu\n\nReviewed By: haobo\n\nCC: igor, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14589",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e8ab1934d9cb3ffebd61097d67bb23439554b265",
        "contains_optimization_keyword": true
    },
    {
        "hash": "bc5dd19b141b1faaba28ac8b122dc5d3d6fa1f56",
        "author": "Siying Dong",
        "date": "2013-12-11T10:50:09-08:00",
        "message": "[RocksDB Performance Branch] Avoid sorting in Version::Get() by presorting them in VersionSet::Builder::SaveTo()\n\nSummary: Pre-sort files in VersionSet::Builder::SaveTo() so that when getting the value, no need to sort them. It can avoid the costs of vector operations and sorting in Version::Get().\n\nTest Plan: make all check\n\nReviewers: haobo, kailiu, dhruba\n\nReviewed By: dhruba\n\nCC: nkg-, igor, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14409",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/bc5dd19b141b1faaba28ac8b122dc5d3d6fa1f56",
        "contains_optimization_keyword": true
    },
    {
        "hash": "41349d9ef1fb1cdeb9c7b942678199c90db2ce22",
        "author": "Siying Dong",
        "date": "2013-12-11T10:49:49-08:00",
        "message": "[RocksDB Performance Branch] Avoid sorting in Version::Get() by presorting them in VersionSet::Builder::SaveTo()\n\nSummary: Pre-sort files in VersionSet::Builder::SaveTo() so that when getting the value, no need to sort them. It can avoid the costs of vector operations and sorting in Version::Get().\n\nTest Plan: make all check\n\nReviewers: haobo, kailiu, dhruba\n\nReviewed By: dhruba\n\nCC: nkg-, igor, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14409",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/41349d9ef1fb1cdeb9c7b942678199c90db2ce22",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e8d40c31b3cca0c3e1ae9abe9b9003b1288026a9",
        "author": "Igor Canadi",
        "date": "2013-12-11T08:33:29-08:00",
        "message": "[RocksDB perf] Cache speedup\n\nSummary:\nI have ran a get benchmark where all the data is in the cache and observed that most of the time is spent on waiting for lock in LRUCache.\n\nThis is an effort to optimize LRUCache.\n\nTest Plan:\nThe data was loaded with fillseq. Then, I ran a benchmark:\n\n    /db_bench --db=/tmp/rocksdb_stat_bench --num=1000000 --benchmarks=readrandom --statistics=1 --use_existing_db=1 --threads=16 --disable_seek_compaction=1 --cache_size=20000000000 --cache_numshardbits=8 --table_cache_numshardbits=8\n\nI ran the benchmark three times. Here are the results:\nAFTER THE PATCH: 798072, 803998, 811807\nBEFORE THE PATCH: 782008, 815593, 763017\n\nReviewers: dhruba, haobo, kailiu\n\nReviewed By: haobo\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14571",
        "modified_files_count": 1,
        "modified_files": [
            "util/cache.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e8d40c31b3cca0c3e1ae9abe9b9003b1288026a9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "4c81383628db46d35b674000a3668b5a9a2498a6",
        "author": "lovro",
        "date": "2013-11-27T11:28:06-08:00",
        "message": "Set background thread name with pthread_setname_np()\n\nSummary: Makes it easier to monitor performance with top\n\nTest Plan: ./manual_compaction_test with `top -H` running.  Previously was two `manual_compacti`, now one shows `rocksdb:bg0`.\n\nReviewers: igor, dhruba\n\nReviewed By: igor\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14367",
        "modified_files_count": 1,
        "modified_files": [
            "util/env_posix.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/4c81383628db46d35b674000a3668b5a9a2498a6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "8aac46d6864d56b1ff2baa4a7b01d2f2e72f28f9",
        "author": "Siying Dong",
        "date": "2013-11-26T14:05:37-08:00",
        "message": "[RocksDB Performance Branch] Fix a regression bug of munmap\n\nSummary:\nFix a stupid bug I just introduced in b59d4d5a5051263b4bfcef00913219ffe4654e42, which I didn't even mean to include.\nGCC might remove the munmap.\n\nTest Plan: Run it and make sure munmap succeeds\n\nReviewers: haobo, kailiu\n\nReviewed By: kailiu\n\nCC: dhruba, reconnect.grayhat, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14361",
        "modified_files_count": 1,
        "modified_files": [
            "util/env_posix.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/8aac46d6864d56b1ff2baa4a7b01d2f2e72f28f9",
        "contains_optimization_keyword": true
    },
    {
        "hash": "37b459f0aaaaae238d599d1e84f9003570683beb",
        "author": "Haobo Xu",
        "date": "2013-11-20T14:34:52-08:00",
        "message": "[RocksDB] Test diff on performance branch\n\nSummary: trivia comment change\n\nTest Plan: Go through the step ofs developing under the performance branch\n\nReviewers: dhruba, kailiu, sdong\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D14259",
        "modified_files_count": 1,
        "modified_files": [
            "db/table_cache.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/37b459f0aaaaae238d599d1e84f9003570683beb",
        "contains_optimization_keyword": true
    },
    {
        "hash": "1ca86f0391bd4e2d262afef0314a241940d445a8",
        "author": "Kai Liu",
        "date": "2013-10-28T10:51:34-07:00",
        "message": "Fix a bug that index block's restart_block_interval is not 1\n\nSummary:\n\nThis bug may affect the seek performance.\n\nTest Plan:\n\nmake\nmake check\n\nAlso gdb into some index block builder to make sure the restart_block_interval is `1`.",
        "modified_files_count": 1,
        "modified_files": [
            "table/table_builder.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/1ca86f0391bd4e2d262afef0314a241940d445a8",
        "contains_optimization_keyword": true
    },
    {
        "hash": "08740b15a4bd22d1433ce644e1ab324fc2ae1531",
        "author": "Haobo Xu",
        "date": "2013-09-26T15:17:03-07:00",
        "message": "[RocksDB] Fix skiplist sequential insertion optimization\n\nSummary: The original optimization missed updating links other than the lowest level.\n\nTest Plan: make check; perf_context_test\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: leveldb, adsharma\n\nDifferential Revision: https://reviews.facebook.net/D13119",
        "modified_files_count": 1,
        "modified_files": [
            "db/skiplist.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/08740b15a4bd22d1433ce644e1ab324fc2ae1531",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e0aa19a94e0d2091013668b67961d885c6d5b7bd",
        "author": "Haobo Xu",
        "date": "2013-09-25T22:49:18-07:00",
        "message": "[RocbsDB] Add an option to enable set based memtable for perf_context_test\n\nSummary:\nas title.\nSome result:\n\n-- Sequential insertion of 1M key/value with stock skip list (all in on memtable)\ntime ./perf_context_test  --total_keys=1000000  --use_set_based_memetable=0\nInserting 1000000 key/value pairs\n...\nPut uesr key comparison:\nCount: 1000000  Average: 8.0179  StdDev: 176.34\nMin: 0.0000  Median: 2.5555  Max: 88933.0000\nPercentiles: P50: 2.56 P75: 2.83 P99: 58.21 P99.9: 133.62 P99.99: 987.50\nGet uesr key comparison:\nCount: 1000000  Average: 43.4465  StdDev: 379.03\nMin: 2.0000  Median: 36.0195  Max: 88939.0000\nPercentiles: P50: 36.02 P75: 43.66 P99: 112.98 P99.9: 824.84 P99.99: 7615.38\nreal\t0m21.345s\nuser\t0m14.723s\nsys\t0m5.677s\n\n-- Sequential insertion of 1M key/value with set based memtable (all in on memtable)\ntime ./perf_context_test  --total_keys=1000000  --use_set_based_memetable=1\nInserting 1000000 key/value pairs\n...\nPut uesr key comparison:\nCount: 1000000  Average: 61.5022  StdDev: 6.49\nMin: 0.0000  Median: 62.4295  Max: 71.0000\nPercentiles: P50: 62.43 P75: 66.61 P99: 71.00 P99.9: 71.00 P99.99: 71.00\nGet uesr key comparison:\nCount: 1000000  Average: 29.3810  StdDev: 3.20\nMin: 1.0000  Median: 29.1801  Max: 34.0000\nPercentiles: P50: 29.18 P75: 32.06 P99: 34.00 P99.9: 34.00 P99.99: 34.00\nreal\t0m28.875s\nuser\t0m21.699s\nsys\t0m5.749s\n\nWorst case comparison for a Put is 88933 (skiplist) vs 71 (set based memetable)\n\nOf course, there's other in-efficiency in set based memtable implementation, which lead to the overall worst performance. However, P99 behavior advantage is very very obvious.\n\nTest Plan: ./perf_context_test and viewstate shadow testing\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D13095",
        "modified_files_count": 1,
        "modified_files": [
            "db/perf_context_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e0aa19a94e0d2091013668b67961d885c6d5b7bd",
        "contains_optimization_keyword": true
    },
    {
        "hash": "43eef52001cde469025a89b47c62af6bbe06c27a",
        "author": "Haobo Xu",
        "date": "2013-08-27T13:36:10-07:00",
        "message": "[RocksDB] move stats counting outside of mutex protected region for DB::Get()\n\nSummary:\nAs title. This is possible as tickers are atomic now.\ndb_bench on high qps in-memory muti-thread random get workload, showed ~5% throughput improvement.\n\nTest Plan: make check; db_bench; db_stress\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D12555",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/43eef52001cde469025a89b47c62af6bbe06c27a",
        "contains_optimization_keyword": true
    },
    {
        "hash": "b1074ac24f28afd0b5704ed520515b9ce00aae4c",
        "author": "Mayank Agarwal",
        "date": "2013-08-24T18:16:01-07:00",
        "message": "Use initializer list for VersionSet\n\nSummary: initialiszer list is fasteri/preferable because it can straightaway call the constructor for this object, otherwise it will be created first and then again initialized. Although gain may not be much in this case because files_ is just a pointer and not a complex object, this is recommended practice.\n\nTest Plan: make all check\n\nReviewers: dhruba, haobo\n\nReviewed By: dhruba\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D12519",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/b1074ac24f28afd0b5704ed520515b9ce00aae4c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "58a0ae06dca85529fcc8c0b04b96ef019b27a8d3",
        "author": "Haobo Xu",
        "date": "2013-08-08T15:31:12-07:00",
        "message": "[RocksDB] Improve sst_dump to take user key range\n\nSummary: The ability to dump internal keys associated with certain user keys, directly from sst files, is very useful for diagnosis. Will incorporate it directly into ldb later.\n\nTest Plan: run it\n\nReviewers: dhruba\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D12075",
        "modified_files_count": 1,
        "modified_files": [
            "tools/sst_dump.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/58a0ae06dca85529fcc8c0b04b96ef019b27a8d3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "821889e20764e88276e83df37e485a64191f1d75",
        "author": "Mayank Agarwal",
        "date": "2013-07-10T18:07:13-07:00",
        "message": "Print complete statistics in db_stress\n\nSummary: db_stress should alos print complete statistics like db_bench. Needed this when I wanted to measure number of delete-IOs dropped due to CheckKeyMayExist to be introduced to rocksdb codebase later- to make deltes in rocksdb faster\n\nTest Plan: make db_stress;./db_stress --max_key=100 --ops_per_thread=1000 --statistics=1\n\nReviewers: sheki, dhruba, vamsi, haobo\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D11655",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/821889e20764e88276e83df37e485a64191f1d75",
        "contains_optimization_keyword": true
    },
    {
        "hash": "836534debdacd99fa0fc428802ee1cbff9ceb44f",
        "author": "Dhruba Borthakur",
        "date": "2013-06-17T14:15:09-07:00",
        "message": "Enhance dbstress to allow specifying compaction trigger for L0.\n\nSummary:\nRocksdb allos specifying the number of files in L0 that triggers\ncompactions. Expose this api as a command line parameter for\nrunning db_stress.\n\nTest Plan: Run test\n\nReviewers: sheki, emayanke\n\nReviewed By: emayanke\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D11343",
        "modified_files_count": 1,
        "modified_files": [
            "tools/db_stress.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/836534debdacd99fa0fc428802ee1cbff9ceb44f",
        "contains_optimization_keyword": true
    },
    {
        "hash": "c3c13db346749c3dfe45e167db2129c645377e9e",
        "author": "Haobo Xu",
        "date": "2013-05-21T13:40:38-07:00",
        "message": "[RocksDB] [Performance Bug] MemTable::Get Slow\n\nSummary:\nThe merge operator diff introduced a performance problem in MemTable::Get.\nAn exit condition is missed when the current key does not match the user key.\nThis could lead to full memtable scan if the user key is not found.\n\nTest Plan: make check; db_bench\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D10851",
        "modified_files_count": 1,
        "modified_files": [
            "db/memtable.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/c3c13db346749c3dfe45e167db2129c645377e9e",
        "contains_optimization_keyword": true
    },
    {
        "hash": "ecd8db02004d73c4b65ebb772a6c882b5cb98223",
        "author": "Haobo Xu",
        "date": "2013-03-26T22:42:26-07:00",
        "message": "[RocksDB] Minimize Mutex protected code section in the critical path\n\nSummary: rocksdb uses a single global lock to protect in memory metadata. We should minimize the mutex protected code section to increase the effective parallelism of the program. See https://our.intern.facebook.com/intern/tasks/?t=2218928\n\nTest Plan:\nmake check\ndb_bench\n\nReviewers: dhruba, heyongqiang\n\nCC: zshao, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D9705",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/ecd8db02004d73c4b65ebb772a6c882b5cb98223",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a6f4275403e8b0be3404eff5ca0ee6917f368a1c",
        "author": "Mayank Agarwal",
        "date": "2013-03-20T11:19:12-07:00",
        "message": "Removing boost from ldb_cmd.cc\n\nSummary: Getting rid of boost in our github codebase which caused problems on third-party\n\nTest Plan: make ldb; python tools/ldb_test.py\n\nReviewers: sheki, dhruba\n\nReviewed By: sheki\n\nDifferential Revision: https://reviews.facebook.net/D9543",
        "modified_files_count": 1,
        "modified_files": [
            "util/ldb_cmd.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a6f4275403e8b0be3404eff5ca0ee6917f368a1c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a78fb5e8bc7ef4485a512c9c1a57fa01dbe3cd50",
        "author": "Mayank Agarwal",
        "date": "2013-03-14T18:16:46-07:00",
        "message": "Doing away with boost in ldb_cmd.h\n\nSummary: boost functions cause complications while deploying to third-party\n\nTest Plan: make\n\nReviewers: sheki, dhruba\n\nReviewed By: sheki\n\nDifferential Revision: https://reviews.facebook.net/D9441",
        "modified_files_count": 1,
        "modified_files": [
            "util/ldb_cmd.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a78fb5e8bc7ef4485a512c9c1a57fa01dbe3cd50",
        "contains_optimization_keyword": true
    },
    {
        "hash": "7c6f5278a27d297cd3b08ad6eb97e11c5af02329",
        "author": "Dhruba Borthakur",
        "date": "2012-11-26T12:01:55-08:00",
        "message": "Merge branch 'performance'",
        "modified_files_count": 1,
        "modified_files": [
            "db/c_test.c"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/7c6f5278a27d297cd3b08ad6eb97e11c5af02329",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e0cd6bf0e91e9b3892ded3b4e4a10a1e0c47ecf6",
        "author": "Dhruba Borthakur",
        "date": "2012-11-26T11:59:51-08:00",
        "message": "The c_test was sometimes failing with an assertion.\n\nSummary:\nOn fast filesystems (e.g. /dev/shm and ext4), the flushing\nof memstore to disk was fast and quick, and the background compaction\nthread was not getting scheduled fast enough to delete obsolete\nfiles before the db was closed. This caused the repair method\nto pick up those files that were not part of the db and the unit\ntest was failing.\n\nThe fix is to enhance the unti test to run a compaction before\nclosing the database so that all files that are not part of the\ndatabase are truly deleted from the filesystem.\n\nTest Plan: make c_test; ./c_test\n\nReviewers: chip, emayanke, sheki\n\nReviewed By: chip\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D6915",
        "modified_files_count": 1,
        "modified_files": [
            "db/c_test.c"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e0cd6bf0e91e9b3892ded3b4e4a10a1e0c47ecf6",
        "contains_optimization_keyword": true
    },
    {
        "hash": "a4b79b6e28489e1508e9e8ec4d201b6fd1b6a238",
        "author": "Dhruba Borthakur",
        "date": "2012-11-19T13:20:25-08:00",
        "message": "Merge branch 'master' into performance",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_test.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/a4b79b6e28489e1508e9e8ec4d201b6fd1b6a238",
        "contains_optimization_keyword": true
    },
    {
        "hash": "e988c11f5884120a6b8bbbd27a0eaea7f2f2504c",
        "author": "Dhruba Borthakur",
        "date": "2012-11-14T16:20:13-08:00",
        "message": "Enhance db_bench to be able to specify a grandparent_overlap_factor.\n\nSummary:\nThe value specified in max_grandparent_overlap_factor is used to\nlimit the file size in a compaction run. This patch makes it\nconfigurable when using db_bench.\n\nTest Plan: make clean db_bench\n\nReviewers: MarkCallaghan, heyongqiang\n\nReviewed By: heyongqiang\n\nCC: leveldb\n\nDifferential Revision: https://reviews.facebook.net/D6729",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/e988c11f5884120a6b8bbbd27a0eaea7f2f2504c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "43d9a8225a998276aa338812e939209ca0a1693c",
        "author": "Dhruba Borthakur",
        "date": "2012-11-13T10:40:52-08:00",
        "message": "Fix asserts so that \"make check OPT=-g\" works on performance branch\n\nSummary:\nCompilation used to fail with the error:\ndb/version_set.cc:1773: error: \u2018number_of_files_to_sort_\u2019 is not a member of \u2018leveldb::VersionSet\u2019\n\nI created a new method called CheckConsistencyForDeletes() so that\nall the high cost checking is done only when OPT=-g is specified.\n\nI also fixed a bug in PickCompactionBySize that was triggered when\nOPT=-g was switched on. The base_index in the compaction record\nwas not set correctly.\n\nTest Plan: make check OPT=-g\n\nDifferential Revision: https://reviews.facebook.net/D6687",
        "modified_files_count": 1,
        "modified_files": [
            "db/version_set.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/43d9a8225a998276aa338812e939209ca0a1693c",
        "contains_optimization_keyword": true
    },
    {
        "hash": "65855dd8d4d8756eec6c43b6faa3abbfb6ea8a86",
        "author": "Mark Callaghan",
        "date": "2012-10-26T14:19:13-07:00",
        "message": "Normalize compaction stats by time in compaction\n\nSummary:\nI used server uptime to compute per-level IO throughput rates. I\nintended to use time spent doing compaction at that level. This fixes that.\n\nTask ID: #\n\nBlame Rev:\n\nTest Plan:\nrun db_bench, look at results\n\nRevert Plan:\n\nDatabase Impact:\n\nMemcache Impact:\n\nOther Notes:\n\nEImportant:\n\n- begin *PUBLIC* platform impact section -\nBugzilla: #\n- end platform impact -\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6237",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/65855dd8d4d8756eec6c43b6faa3abbfb6ea8a86",
        "contains_optimization_keyword": true
    },
    {
        "hash": "47bce26acadf031563d79eb9c16753081b4cd778",
        "author": "Dhruba Borthakur",
        "date": "2012-10-23T22:32:54-07:00",
        "message": "Merge branch 'master' into performance",
        "modified_files_count": 1,
        "modified_files": [
            "include/leveldb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/47bce26acadf031563d79eb9c16753081b4cd778",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3489cd615c1e8d71b19cd0edd0b1fcc6200f3ef7",
        "author": "Dhruba Borthakur",
        "date": "2012-10-21T02:15:19-07:00",
        "message": "Merge branch 'master' into performance\n\nConflicts:\n\tdb/db_impl.cc\n\tdb/db_impl.h",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_impl.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3489cd615c1e8d71b19cd0edd0b1fcc6200f3ef7",
        "contains_optimization_keyword": true
    },
    {
        "hash": "50166999efd6c6f0404df9490b5c10a59b92e738",
        "author": "Dhruba Borthakur",
        "date": "2012-10-19T16:08:04-07:00",
        "message": "Merge branch 'master' into performance",
        "modified_files_count": 1,
        "modified_files": [
            "port/port_posix.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/50166999efd6c6f0404df9490b5c10a59b92e738",
        "contains_optimization_keyword": true
    },
    {
        "hash": "0230866791dff3de4da936b51026279922c309d3",
        "author": "Dhruba Borthakur",
        "date": "2012-10-15T10:18:49-07:00",
        "message": "Enhance db_bench to allow setting the number of levels in a database.\n\nSummary: Enhance db_bench to allow setting the number of levels in a database.\n\nTest Plan: run db_bench and look at LOG\n\nReviewers: heyongqiang, MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nCC: MarkCallaghan\n\nDifferential Revision: https://reviews.facebook.net/D6027",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/0230866791dff3de4da936b51026279922c309d3",
        "contains_optimization_keyword": true
    },
    {
        "hash": "3662c2976a74d140c589066480ee53171882f624",
        "author": "heyongqiang",
        "date": "2012-09-17T15:56:11-07:00",
        "message": "improve comments about target_file_size_base, target_file_size_multiplier, max_bytes_for_level_base, max_bytes_for_level_multiplier Summary:\n\nSummary: as subject\n\nTest Plan: compile\n\nReviewers: MarkCallaghan, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5499",
        "modified_files_count": 1,
        "modified_files": [
            "include/leveldb/options.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/3662c2976a74d140c589066480ee53171882f624",
        "contains_optimization_keyword": true
    },
    {
        "hash": "94208a7881452c96c3e7e2e9d713c5adf5d95dbd",
        "author": "Dhruba Borthakur",
        "date": "2012-09-04T12:06:26-07:00",
        "message": "Benchmark with both reads and writes at the same time.\n\nSummary:\nThis patch enables the db_bench benchmark to issue both random reads and random writes at the same time. This options can be trigged via\n./db_bench --benchmarks=readrandomwriterandom\n\nThe default percetage of reads is 90.\n\nOne can change the percentage of reads by specifying the --readwritepercent.\n./db_bench --benchmarks=readrandomwriterandom=50\n\nThis is a feature request from Jeffro asking for leveldb performance with a 90:10 read:write ratio.\n\nTest Plan: run on test machine.\n\nReviewers: heyongqiang\n\nReviewed By: heyongqiang\n\nDifferential Revision: https://reviews.facebook.net/D5067",
        "modified_files_count": 1,
        "modified_files": [
            "db/db_bench.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/94208a7881452c96c3e7e2e9d713c5adf5d95dbd",
        "contains_optimization_keyword": true
    },
    {
        "hash": "921a48428ed88d04a263c5269f87222112081523",
        "author": "Arun Sharma",
        "date": "2012-05-14T15:40:11-07:00",
        "message": "Optimize for lp64\n\nSummary:\nSome code reorganization in-preparation for replacing with a hardware\ninstruction.\n\n* Use u64 for some of the key types\n* Use an ALIGN macro so code is easier to read\n\nTest Plan: crc32c_test\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3135",
        "modified_files_count": 1,
        "modified_files": [
            "util/crc32c.cc"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/921a48428ed88d04a263c5269f87222112081523",
        "contains_optimization_keyword": true
    },
    {
        "hash": "90b2924fb25943adeea972f10a824d6e0d660935",
        "author": "Arun Sharma",
        "date": "2012-05-11T09:57:40-07:00",
        "message": "skiplist: optimize for sequential insert pattern\n\nSummary:\nskiplist doesn't cache the location of the last insert and becomes\nCPU bound when the input data has sequential keys.\n\nNotes on thread safety: ::Insert() already requires external\nsynchronization. So this change is not making it any worse.\n\nTest Plan: skiplist_test\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3129",
        "modified_files_count": 1,
        "modified_files": [
            "db/skiplist.h"
        ],
        "github_commit_url": "https://github.com/facebook/rocksdb/commit/90b2924fb25943adeea972f10a824d6e0d660935",
        "contains_optimization_keyword": true
    }
]