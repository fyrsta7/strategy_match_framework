size_t WriteThread::EnterAsBatchGroupLeader(Writer* leader,
                                            WriteGroup* write_group) {
  assert(leader->link_older == nullptr);
  assert(leader->batch != nullptr);
  assert(write_group != nullptr);

  size_t size = WriteBatchInternal::ByteSize(leader->batch);

  // Allow the group to grow up to a maximum size, but if the
  // original write is small, limit the growth so we do not slow
  // down the small write too much.
  size_t max_size = max_write_batch_group_size_bytes;
  const uint64_t min_batch_size_bytes = max_write_batch_group_size_bytes / 8;
  if (size <= min_batch_size_bytes) {
    max_size = size + min_batch_size_bytes;
  }

  leader->write_group = write_group;
  write_group->leader = leader;
  write_group->last_writer = leader;
  write_group->size = 1;
  Writer* newest_writer = newest_writer_.load(std::memory_order_acquire);

  // This is safe regardless of any db mutex status of the caller. Previous
  // calls to ExitAsGroupLeader either didn't call CreateMissingNewerLinks
  // (they emptied the list and then we added ourself as leader) or had to
  // explicitly wake us up (the list was non-empty when we added ourself,
  // so we have already received our MarkJoined).
  CreateMissingNewerLinks(newest_writer);

  // This comment illustrates how the rest of the function works using an
  // example. Notation:
  //
  // - Items are `Writer`s
  // - Items prefixed by "@" have been included in `write_group`
  // - Items prefixed by "*" have compatible options with `leader`, but have not
  //   been included in `write_group` yet
  // - Items after several spaces are in `r_list`. These have incompatible
  //   options with `leader` and are temporarily separated from the main list.
  //
  // Each line below depicts the state of the linked lists at the beginning of
  // an iteration of the while-loop.
  //
  // @leader, n1, *n2, n3, *newest_writer
  // @leader, *n2, n3, *newest_writer,    n1
  // @leader, @n2, n3, *newest_writer,    n1
  //
  // After the while-loop, the `r_list` is grafted back onto the main list.
  //
  // case A: no new `Writer`s arrived
  // @leader, @n2, @newest_writer,        n1, n3
  // @leader, @n2, @newest_writer, n1, n3
  //
  // case B: a new `Writer` (n4) arrived
  // @leader, @n2, @newest_writer, n4     n1, n3
  // @leader, @n2, @newest_writer, n1, n3, n4

  // Tricky. Iteration start (leader) is exclusive and finish
  // (newest_writer) is inclusive. Iteration goes from old to new.
  Writer* w = leader;
  // write_group end
  Writer* we = leader;
  // declare r_list
  Writer* rb = nullptr;
  Writer* re = nullptr;

  while (w != newest_writer) {
    assert(w->link_newer);
    w = w->link_newer;

    if ((w->sync && !leader->sync) ||
        // Do not include a sync write into a batch handled by a non-sync write.
        (w->no_slowdown != leader->no_slowdown) ||
        // Do not mix writes that are ok with delays with the ones that request
        // fail on delays.
        (w->disable_wal != leader->disable_wal) ||
        // Do not mix writes that enable WAL with the ones whose WAL disabled.
        (w->protection_bytes_per_key != leader->protection_bytes_per_key) ||
        // Do not mix writes with different levels of integrity protection.
        (w->rate_limiter_priority != leader->rate_limiter_priority) ||
        // Do not mix writes with different rate limiter priorities.
        (w->batch == nullptr) ||
        // Do not include those writes with nullptr batch. Those are not writes
        // those are something else. They want to be alone
        (w->callback != nullptr && !w->callback->AllowWriteBatching()) ||
        // dont batch writes that don't want to be batched
        (size + WriteBatchInternal::ByteSize(w->batch) > max_size)
        // Do not make batch too big
    ) {
      // remove from list
      w->link_older->link_newer = w->link_newer;
      if (w->link_newer != nullptr) {
        w->link_newer->link_older = w->link_older;
      }
      // insert into r_list
      if (re == nullptr) {
        rb = re = w;
        w->link_older = nullptr;
      } else {
        w->link_older = re;
        re->link_newer = w;
        re = w;
      }
    } else {
      // grow up
      we = w;
      w->write_group = write_group;
      size += WriteBatchInternal::ByteSize(w->batch);
      write_group->last_writer = w;
      write_group->size++;
    }
  }
  // append r_list after write_group end
  if (rb != nullptr) {
    rb->link_older = we;
    re->link_newer = nullptr;
    we->link_newer = rb;
    if (!newest_writer_.compare_exchange_weak(w, re)) {
      while (w->link_older != newest_writer) {
        w = w->link_older;
      }
      w->link_older = re;
    }
  }

  TEST_SYNC_POINT_CALLBACK("WriteThread::EnterAsBatchGroupLeader:End", w);
  return size;
}
