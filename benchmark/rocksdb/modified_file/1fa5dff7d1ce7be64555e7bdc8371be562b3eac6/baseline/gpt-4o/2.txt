size_t WriteThread::EnterAsBatchGroupLeader(Writer* leader,
                                            WriteGroup* write_group) {
  assert(leader->link_older == nullptr);
  assert(leader->batch != nullptr);
  assert(write_group != nullptr);

  size_t size = WriteBatchInternal::ByteSize(leader->batch);

  // Precompute min_batch_size_bytes to avoid division in every call
  static const uint64_t min_batch_size_bytes = max_write_batch_group_size_bytes / 8;

  // Allow the group to grow up to a maximum size, but if the
  // original write is small, limit the growth so we do not slow
  // down the small write too much.
  size_t max_size = (size <= min_batch_size_bytes)
                        ? size + min_batch_size_bytes
                        : max_write_batch_group_size_bytes;

  leader->write_group = write_group;
  write_group->leader = leader;
  write_group->last_writer = leader;
  write_group->size = 1;
  Writer* newest_writer = newest_writer_.load(std::memory_order_acquire);

  CreateMissingNewerLinks(newest_writer);

  Writer* w = leader;
  while (w != newest_writer) {
    assert(w->link_newer);
    w = w->link_newer;

    // Combine conditions to reduce branching and short-circuit early
    if (w->sync && !leader->sync) break;
    if (w->no_slowdown != leader->no_slowdown) break;
    if (w->disable_wal != leader->disable_wal) break;
    if (w->protection_bytes_per_key != leader->protection_bytes_per_key) break;
    if (w->rate_limiter_priority != leader->rate_limiter_priority) break;
    if (w->batch == nullptr) break;
    if (w->callback != nullptr && !w->callback->AllowWriteBatching()) break;

    // Cache batch size to avoid redundant computation
    auto batch_size = WriteBatchInternal::ByteSize(w->batch);
    if (size + batch_size > max_size) break;

    w->write_group = write_group;
    size += batch_size;
    write_group->last_writer = w;
    write_group->size++;
  }
  TEST_SYNC_POINT_CALLBACK("WriteThread::EnterAsBatchGroupLeader:End", w);
  return size;
}